{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (In Separate main module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.io import mmread\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data=mmread('data/koh_extract/koh.data.counts.mm').toarray().astype(float)\n",
    "with open('data/koh_extract/koh.data.col','r') as f: exp_data_col=[i.strip().strip('\"') for i in f.read().split()]\n",
    "with open('data/koh_extract/koh.data.row','r') as f: exp_data_row=[i.strip().strip('\"') for i in f.read().split()]\n",
    "assert exp_data.shape==(len(exp_data_row),len(exp_data_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(set(exp_data_row))==len(exp_data_row)\n",
    "assert len(set(exp_data_col))==len(exp_data_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.93134327e+04, 5.81785206e+04, 9.08678570e+03, ...,\n",
       "         7.58501657e+04, 8.94289538e+04, 1.01937411e+05],\n",
       "        [2.48547207e+04, 4.67361235e+04, 1.09620933e+04, ...,\n",
       "         1.85699498e+04, 1.85806217e+04, 2.49668288e+04],\n",
       "        [1.18570138e+04, 2.80836990e+04, 6.80287185e+03, ...,\n",
       "         3.53701816e+04, 5.75775170e+04, 5.59807979e+04],\n",
       "        ...,\n",
       "        [4.83851544e+01, 1.19916350e+01, 6.96361343e+00, ...,\n",
       "         4.88990717e+00, 8.49751588e+00, 7.80405857e+01],\n",
       "        [2.35818377e+01, 4.26839000e+01, 1.80639917e+01, ...,\n",
       "         7.54470439e+00, 2.84437127e+00, 8.88332407e+00],\n",
       "        [6.10992450e+01, 2.59814157e+01, 2.45930809e+01, ...,\n",
       "         4.85579286e-01, 0.00000000e+00, 0.00000000e+00]]), (4898, 446))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_data,exp_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ENSG00000198804',\n",
       "  'ENSG00000210082',\n",
       "  'ENSG00000198712',\n",
       "  'ENSG00000198938',\n",
       "  'ENSG00000198727'],\n",
       " ['SRR3952323', 'SRR3952325', 'SRR3952326', 'SRR3952327', 'SRR3952328'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_data_row[:5],exp_data_col[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marker info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustername_to_markers=pd.read_csv('data/koh_extract/koh.rho.tsv',sep='\\t').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000008311</th>\n",
       "      <th>ENSG00000018625</th>\n",
       "      <th>ENSG00000019549</th>\n",
       "      <th>ENSG00000026025</th>\n",
       "      <th>ENSG00000039068</th>\n",
       "      <th>ENSG00000042493</th>\n",
       "      <th>ENSG00000053438</th>\n",
       "      <th>ENSG00000075340</th>\n",
       "      <th>ENSG00000079102</th>\n",
       "      <th>ENSG00000092068</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000240563</th>\n",
       "      <th>ENSG00000241186</th>\n",
       "      <th>ENSG00000243004</th>\n",
       "      <th>ENSG00000249532</th>\n",
       "      <th>ENSG00000250305</th>\n",
       "      <th>ENSG00000254277</th>\n",
       "      <th>ENSG00000254339</th>\n",
       "      <th>ENSG00000260342</th>\n",
       "      <th>ENSG00000260834</th>\n",
       "      <th>ENSG00000280623</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hESC</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MPS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLL1pPXM</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESMT</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sclrtm</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D5CntrlDrmmtm</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2LtM</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ENSG00000008311  ENSG00000018625  ENSG00000019549  \\\n",
       "hESC                         1                1                0   \n",
       "APS                          0                0                1   \n",
       "MPS                          0                0                1   \n",
       "DLL1pPXM                     0                1                1   \n",
       "ESMT                         0                1                1   \n",
       "Sclrtm                       0                1                1   \n",
       "D5CntrlDrmmtm                0                1                1   \n",
       "D2LtM                        0                1                1   \n",
       "\n",
       "               ENSG00000026025  ENSG00000039068  ENSG00000042493  \\\n",
       "hESC                         0                1                1   \n",
       "APS                          0                1                1   \n",
       "MPS                          0                1                1   \n",
       "DLL1pPXM                     1                1                1   \n",
       "ESMT                         1                1                1   \n",
       "Sclrtm                       1                0                0   \n",
       "D5CntrlDrmmtm                1                0                1   \n",
       "D2LtM                        1                1                1   \n",
       "\n",
       "               ENSG00000053438  ENSG00000075340  ENSG00000079102  \\\n",
       "hESC                         0                1                1   \n",
       "APS                          0                1                1   \n",
       "MPS                          0                1                1   \n",
       "DLL1pPXM                     0                1                0   \n",
       "ESMT                         1                1                1   \n",
       "Sclrtm                       0                0                1   \n",
       "D5CntrlDrmmtm                0                1                1   \n",
       "D2LtM                        0                1                1   \n",
       "\n",
       "               ENSG00000092068  ...  ENSG00000240563  ENSG00000241186  \\\n",
       "hESC                         1  ...                1                1   \n",
       "APS                          1  ...                1                1   \n",
       "MPS                          1  ...                1                1   \n",
       "DLL1pPXM                     1  ...                0                0   \n",
       "ESMT                         1  ...                0                0   \n",
       "Sclrtm                       0  ...                0                0   \n",
       "D5CntrlDrmmtm                1  ...                0                0   \n",
       "D2LtM                        1  ...                0                0   \n",
       "\n",
       "               ENSG00000243004  ENSG00000249532  ENSG00000250305  \\\n",
       "hESC                         1                1                0   \n",
       "APS                          1                1                1   \n",
       "MPS                          1                1                1   \n",
       "DLL1pPXM                     1                0                1   \n",
       "ESMT                         1                0                1   \n",
       "Sclrtm                       0                0                1   \n",
       "D5CntrlDrmmtm                0                0                1   \n",
       "D2LtM                        1                0                1   \n",
       "\n",
       "               ENSG00000254277  ENSG00000254339  ENSG00000260342  \\\n",
       "hESC                         1                1                1   \n",
       "APS                          1                1                1   \n",
       "MPS                          1                1                1   \n",
       "DLL1pPXM                     0                0                1   \n",
       "ESMT                         0                0                1   \n",
       "Sclrtm                       0                0                0   \n",
       "D5CntrlDrmmtm                0                0                1   \n",
       "D2LtM                        0                0                1   \n",
       "\n",
       "               ENSG00000260834  ENSG00000280623  \n",
       "hESC                         1                0  \n",
       "APS                          1                0  \n",
       "MPS                          1                1  \n",
       "DLL1pPXM                     1                0  \n",
       "ESMT                         1                0  \n",
       "Sclrtm                       0                0  \n",
       "D5CntrlDrmmtm                1                0  \n",
       "D2LtM                        1                0  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustername_to_markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cluster info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run</th>\n",
       "      <th>LibraryName</th>\n",
       "      <th>phenoid</th>\n",
       "      <th>libsize.drop</th>\n",
       "      <th>feature.drop</th>\n",
       "      <th>total_features</th>\n",
       "      <th>log10_total_features</th>\n",
       "      <th>total_counts</th>\n",
       "      <th>log10_total_counts</th>\n",
       "      <th>pct_counts_top_50_features</th>\n",
       "      <th>pct_counts_top_100_features</th>\n",
       "      <th>pct_counts_top_200_features</th>\n",
       "      <th>pct_counts_top_500_features</th>\n",
       "      <th>is_cell_control</th>\n",
       "      <th>celltype</th>\n",
       "      <th>tSNE_1</th>\n",
       "      <th>tSNE_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SRR3952323</th>\n",
       "      <td>SRR3952323</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4895</td>\n",
       "      <td>3.689841</td>\n",
       "      <td>2.248411e+06</td>\n",
       "      <td>6.351876</td>\n",
       "      <td>18.278965</td>\n",
       "      <td>25.975390</td>\n",
       "      <td>35.537616</td>\n",
       "      <td>52.410941</td>\n",
       "      <td>False</td>\n",
       "      <td>hESC</td>\n",
       "      <td>9.973465</td>\n",
       "      <td>19.045918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR3952325</th>\n",
       "      <td>SRR3952325</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4887</td>\n",
       "      <td>3.689131</td>\n",
       "      <td>2.271617e+06</td>\n",
       "      <td>6.356335</td>\n",
       "      <td>24.672529</td>\n",
       "      <td>32.222803</td>\n",
       "      <td>41.547358</td>\n",
       "      <td>57.969233</td>\n",
       "      <td>False</td>\n",
       "      <td>hESC</td>\n",
       "      <td>10.366232</td>\n",
       "      <td>21.511833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR3952326</th>\n",
       "      <td>SRR3952326</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4888</td>\n",
       "      <td>3.689220</td>\n",
       "      <td>5.846824e+05</td>\n",
       "      <td>5.766921</td>\n",
       "      <td>22.732839</td>\n",
       "      <td>30.205988</td>\n",
       "      <td>39.431308</td>\n",
       "      <td>55.285817</td>\n",
       "      <td>False</td>\n",
       "      <td>hESC</td>\n",
       "      <td>9.881356</td>\n",
       "      <td>19.317197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR3952327</th>\n",
       "      <td>SRR3952327</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4879</td>\n",
       "      <td>3.688420</td>\n",
       "      <td>3.191810e+06</td>\n",
       "      <td>6.504037</td>\n",
       "      <td>20.867378</td>\n",
       "      <td>29.003904</td>\n",
       "      <td>38.785558</td>\n",
       "      <td>56.020859</td>\n",
       "      <td>False</td>\n",
       "      <td>hESC</td>\n",
       "      <td>8.483966</td>\n",
       "      <td>21.289459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR3952328</th>\n",
       "      <td>SRR3952328</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4873</td>\n",
       "      <td>3.687886</td>\n",
       "      <td>2.190385e+06</td>\n",
       "      <td>6.340521</td>\n",
       "      <td>21.287923</td>\n",
       "      <td>29.423689</td>\n",
       "      <td>39.307683</td>\n",
       "      <td>56.640975</td>\n",
       "      <td>False</td>\n",
       "      <td>hESC</td>\n",
       "      <td>9.017168</td>\n",
       "      <td>20.637262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Run LibraryName phenoid  libsize.drop  feature.drop  \\\n",
       "SRR3952323  SRR3952323      H7hESC  H7hESC         False         False   \n",
       "SRR3952325  SRR3952325      H7hESC  H7hESC         False         False   \n",
       "SRR3952326  SRR3952326      H7hESC  H7hESC         False         False   \n",
       "SRR3952327  SRR3952327      H7hESC  H7hESC         False         False   \n",
       "SRR3952328  SRR3952328      H7hESC  H7hESC         False         False   \n",
       "\n",
       "            total_features  log10_total_features  total_counts  \\\n",
       "SRR3952323            4895              3.689841  2.248411e+06   \n",
       "SRR3952325            4887              3.689131  2.271617e+06   \n",
       "SRR3952326            4888              3.689220  5.846824e+05   \n",
       "SRR3952327            4879              3.688420  3.191810e+06   \n",
       "SRR3952328            4873              3.687886  2.190385e+06   \n",
       "\n",
       "            log10_total_counts  pct_counts_top_50_features  \\\n",
       "SRR3952323            6.351876                   18.278965   \n",
       "SRR3952325            6.356335                   24.672529   \n",
       "SRR3952326            5.766921                   22.732839   \n",
       "SRR3952327            6.504037                   20.867378   \n",
       "SRR3952328            6.340521                   21.287923   \n",
       "\n",
       "            pct_counts_top_100_features  pct_counts_top_200_features  \\\n",
       "SRR3952323                    25.975390                    35.537616   \n",
       "SRR3952325                    32.222803                    41.547358   \n",
       "SRR3952326                    30.205988                    39.431308   \n",
       "SRR3952327                    29.003904                    38.785558   \n",
       "SRR3952328                    29.423689                    39.307683   \n",
       "\n",
       "            pct_counts_top_500_features  is_cell_control celltype     tSNE_1  \\\n",
       "SRR3952323                    52.410941            False     hESC   9.973465   \n",
       "SRR3952325                    57.969233            False     hESC  10.366232   \n",
       "SRR3952326                    55.285817            False     hESC   9.881356   \n",
       "SRR3952327                    56.020859            False     hESC   8.483966   \n",
       "SRR3952328                    56.640975            False     hESC   9.017168   \n",
       "\n",
       "               tSNE_2  \n",
       "SRR3952323  19.045918  \n",
       "SRR3952325  21.511833  \n",
       "SRR3952326  19.317197  \n",
       "SRR3952327  21.289459  \n",
       "SRR3952328  20.637262  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_data_meta=pd.read_csv('data/koh_extract/koh.metadata.tsv',sep='\\t')\n",
    "exp_data_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustername_unique=list(clustername_to_markers.index)\n",
    "exp_data_meta_clusterid_clusteridunique=exp_data_meta['celltype'].apply(lambda x: clustername_unique.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_unique=list(clustername_to_markers.columns)\n",
    "marker_unique_exp_data_idx=[exp_data_row.index(marker) for marker in marker_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_true=np.array([np.sum(exp_data_meta_clusterid_clusteridunique==i) for i in sorted(np.unique(exp_data_meta_clusterid_clusteridunique))])/exp_data_meta_clusterid_clusteridunique.shape[0]\n",
    "M_true=np.array([np.mean(exp_data[marker_unique_exp_data_idx,:][:,exp_data_meta_clusterid_clusteridunique==i],axis=1) for i in sorted(np.unique(exp_data_meta_clusterid_clusteridunique))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(446,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_size_factor=pd.read_csv('data/analysis/koh.size_factor_cluster.tsv',sep='\\t',header=None)[0].values.astype(float)#.reshape(-1,1)\n",
    "#cell_size_factor=np.ones_like(cell_size_factor)\n",
    "cell_size_factor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(446, 84)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=exp_data[marker_unique_exp_data_idx].transpose().astype(float)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#marker_onehot=np.array([np.sum(np.eye(len(marker_unique))[[marker_unique.tolist().index(marker) for marker in value]],axis=0) for key,value in clustername_to_markers.items()])\n",
    "#marker_onehot.shape\n",
    "#marker_onehot.T\n",
    "marker_onehot=clustername_to_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(446, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exp_data_col_patient=pd.Series(exp_data_col).str.slice(start=1,stop=2).astype(int).values\n",
    "#x_data_covariate=np.eye(len(np.unique(exp_data_col_patient)))[exp_data_col_patient-1]\n",
    "x_data_intercept=np.array([np.ones(Y.shape[0])]).transpose()\n",
    "x_data_null=np.concatenate([x_data_intercept],axis=1)\n",
    "x_data_null.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from basic_tools import Cell_Dataset,Masked\n",
    "class Masked_Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, mask):\n",
    "        #print('aaaa')\n",
    "        output=input\n",
    "        ctx.save_for_backward(input, mask)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, mask = ctx.saved_tensors\n",
    "        grad_input = grad_mask = None\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mul(mask)\n",
    "\n",
    "        return grad_input, grad_mask\n",
    "    \n",
    "class Masked(nn.Module):    \n",
    "    def __init__(self, mask):    \n",
    "        super(Masked, self).__init__()\n",
    "        \n",
    "        self.mask = nn.Parameter(torch.Tensor(mask)==1, requires_grad=False)    \n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        return Masked_Function.apply(input, self.mask)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'mask={}'.format(self.mask.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class NB_logprob(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NB_logprob,self).__init__()\n",
    "        \n",
    "    def forward(self,total_count,probs,value):\n",
    "        \n",
    "        #eps = torch.finfo(probs.dtype).eps\n",
    "        #probs_clamped=probs.clamp(min=eps, max=1 - eps)        \n",
    "        probs_clamped=probs\n",
    "        logits=torch.log(probs_clamped) - torch.log1p(-probs_clamped)\n",
    "        #logits=torch.log(probs_clamped)\n",
    "        \n",
    "        log_unnormalized_prob = (total_count * F.logsigmoid(-logits) +\n",
    "                                 value * F.logsigmoid(logits))\n",
    "        log_normalization = (-torch.lgamma(total_count + value) + torch.lgamma(1. + value) +\n",
    "                             torch.lgamma(total_count))\n",
    "\n",
    "        return log_unnormalized_prob - log_normalization\n",
    "    \n",
    "class Normal_logprob(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Normal_logprob,self).__init__()\n",
    "        \n",
    "    def forward(self,loc,scale,value):\n",
    "    \n",
    "        var = (scale ** 2)\n",
    "        log_scale = torch.log(scale)\n",
    "        \n",
    "        \n",
    "        #print(loc.shape,scale.shape,value.shape)\n",
    "        return -((value - loc) ** 2) / (2 * var) - log_scale - math.log(math.sqrt(2 * math.pi))    \n",
    "        #return log_scale\n",
    "        \n",
    "class Poisson_logprob(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Poisson_logprob,self).__init__()\n",
    "        \n",
    "    def forward(self,rate,value):\n",
    "        #rate=rate.clamp(min=1e-3)+(-1)/rate.clamp(max=-1e-5)\n",
    "        \n",
    "        return (rate.log() * value) - rate - (value + 1).lgamma()\n",
    "    \n",
    "class Dirichlet_logprob(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dirichlet_logprob,self).__init__()\n",
    "        \n",
    "    def forward(self,concentration,value):\n",
    "        \n",
    "        return ((torch.log(value) * (concentration - 1.0)).sum(-1) +\n",
    "                torch.lgamma(concentration.sum(-1)) -\n",
    "                torch.lgamma(concentration).sum(-1))\n",
    "    \n",
    "        \n",
    "NB_logprob=NB_logprob()        \n",
    "normal_logprob=Normal_logprob()\n",
    "poisson_logprob=Poisson_logprob()\n",
    "dirichlet_logprob=Dirichlet_logprob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell_Dataset(Dataset):\n",
    "    def __init__(self,Y,X,s):\n",
    "        self.Y=Y\n",
    "        self.X=X\n",
    "        self.s=s\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.Y.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        item= {\"Y\":self.Y[idx,:],\"X\":self.X[idx,:],\"s\":self.s[idx]}\n",
    "        return item  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_EM(model,optimizer,LL_diff_tolerance,Q_diff_tolerance,verbose=True):\n",
    "    global gamma_new,Q_new,LL_new\n",
    "    \n",
    "    print('Start time:',datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx,batch in enumerate(cell_dataloader):\n",
    "            batch_Y=batch['Y'].to(device)\n",
    "            batch_X=batch['X'].to(device)\n",
    "            batch_s=batch['s'].to(device)\n",
    "        gamma_new=Poisson_Function_Nomask.apply(batch_Y,batch_X,batch_s,model.delta_log,model.beta,'gamma')\n",
    "        gamma_fixed,_,LL_old=model(batch_Y,batch_X,batch_s,gamma_fixed=None,mode='LL')\n",
    "        _,Q_old,_=model(batch_Y,batch_X,batch_s,gamma_fixed=gamma_fixed,mode='M')\n",
    "\n",
    "    print(LL_old)\n",
    "    print(Q_old)\n",
    "\n",
    "    for em_idx in range(EM_ITER_MAX):#\n",
    "        #optimizer = optim.Adam(model.parameters(),lr=0.1,eps=1e-3,betas=(0.9,0.999))\n",
    "        LL_new=torch.zeros_like(LL_old)\n",
    "        #optimizer = optim.Adam(model.parameters(),lr=LR)\n",
    "        for batch_idx,batch in enumerate(cell_dataloader):\n",
    "            # It is usually just one iteration(batch).\n",
    "            # However, developer of cellAssign may have done this for extreme situation of larse sample size\n",
    "            batch_Y=batch['Y'].to(device)\n",
    "            batch_X=batch['X'].to(device)\n",
    "            batch_s=batch['s'].to(device)\n",
    "\n",
    "            #############\n",
    "            #E-step\n",
    "            ######### ####\n",
    "            with torch.no_grad():\n",
    "                gamma_new,_,_=model(batch_Y,batch_X,batch_s,gamma_fixed=None,mode='E')\n",
    "\n",
    "            #############\n",
    "            #M-step\n",
    "            #############\n",
    "            for m_idx in range(M_ITER_MAX):#\n",
    "            #for m_idx in range(20):#    \n",
    "                optimizer.zero_grad()\n",
    "                _,Q_new,_=model(batch_Y,batch_X,batch_s,gamma_fixed=gamma_new,mode='M')\n",
    "                Q_new.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                #Constraint\n",
    "                model.delta_log.data=model.delta_log.data.clamp(min=model.delta_log_min)\n",
    "                #model.NB_basis_a.data=model.NB_basis_a.data.clamp(min=0)\n",
    "\n",
    "                if m_idx%20==0:\n",
    "                    #print(sorted(model.delta_log.cpu().detach().numpy().flatten())[-10:])\n",
    "                    Q_diff=(Q_old-Q_new)/torch.abs(Q_old)\n",
    "                    Q_old=Q_new\n",
    "                    print('M: {}, Q: {} Q_diff: {}'.format(m_idx,Q_new,Q_diff))    \n",
    "                    if m_idx>0 and torch.abs(Q_diff)<Q_diff_tolerance:\n",
    "                        print('M break')\n",
    "                        break                \n",
    "            #############\n",
    "            #Look at LL\n",
    "            #############\n",
    "            with torch.no_grad():\n",
    "                _,_,LL_temp=model(batch_Y,batch_X,batch_s,gamma_fixed=None,mode='LL')\n",
    "                LL_new+=LL_temp\n",
    "\n",
    "        LL_diff=(LL_new-LL_old)/torch.abs(LL_old)\n",
    "        LL_old=LL_new\n",
    "        print('EM: {}, LL: {} LL_diff: {}'.format(em_idx,LL_new,LL_diff))\n",
    "        if LL_diff<LL_diff_tolerance:\n",
    "            print('EM break')\n",
    "            break\n",
    "    print('End time:',datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))    \n",
    "    return gamma_new,Q_new,LL_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_EM_trick(model,optimizer,LL_diff_tolerance,Q_diff_tolerance,verbose=True):\n",
    "    global gamma_new,LL_new\n",
    "    \n",
    "    if verbose:\n",
    "        print('Start time:',datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx,batch in enumerate(cell_dataloader):\n",
    "            batch_Y=batch['Y'].to(device)\n",
    "            batch_X=batch['X'].to(device)\n",
    "            batch_s=batch['s'].to(device)    \n",
    "        LL_old=model(batch_Y,batch_X,batch_s)\n",
    "        Q_old=LL_old\n",
    "    if verbose:\n",
    "        print(LL_old)\n",
    "\n",
    "    for em_idx in range(EM_ITER_MAX):#\n",
    "        #optimizer = optim.Adam(model.parameters(),lr=0.1,eps=1e-3,betas=(0.9,0.999))\n",
    "        LL_new=torch.zeros_like(LL_old)\n",
    "        #optimizer = optim.Adam(model.parameters(),lr=LR)\n",
    "        for batch_idx,batch in enumerate(cell_dataloader):\n",
    "            # It is usually just one iteration(batch).\n",
    "            # However, developer of cellAssign may have done this for extreme situation of larse sample size\n",
    "            batch_Y=batch['Y'].to(device)\n",
    "            batch_X=batch['X'].to(device)\n",
    "            batch_s=batch['s'].to(device)\n",
    "\n",
    "            #############\n",
    "            #M-step\n",
    "            #############\n",
    "            for m_idx in range(M_ITER_MAX):#\n",
    "            #for m_idx in range(20):#    \n",
    "                optimizer.zero_grad()\n",
    "                Q_new=-model(batch_Y,batch_X,batch_s)\n",
    "                Q_new.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                #Constraint\n",
    "                model.delta_log.data=model.delta_log.data.clamp(min=model.delta_log_min)\n",
    "                #model.NB_basis_a.data=model.NB_basis_a.data.clamp(min=0)\n",
    "\n",
    "                if m_idx%20==0:\n",
    "                    #print(sorted(model.delta_log.cpu().detach().numpy().flatten())[-10:])\n",
    "                    Q_diff=(Q_old-Q_new)/torch.abs(Q_old)\n",
    "                    Q_old=Q_new\n",
    "                    if verbose:\n",
    "                        print('M: {}, Q: {} Q_diff: {}'.format(m_idx,Q_new,Q_diff))    \n",
    "                    if m_idx>0 and torch.abs(Q_diff)<(Q_diff_tolerance):\n",
    "                        if verbose:\n",
    "                            print('M break')\n",
    "                        break                \n",
    "            #############\n",
    "            #Look at LL\n",
    "            #############\n",
    "            with torch.no_grad():\n",
    "                LL_temp=-Q_new\n",
    "                LL_new+=LL_temp\n",
    "\n",
    "        LL_diff=(LL_new-LL_old)/torch.abs(LL_old)\n",
    "        LL_old=LL_new\n",
    "        if verbose:\n",
    "            print('EM: {}, LL: {} LL_diff: {}'.format(em_idx,LL_new,LL_diff))\n",
    "        if LL_diff<LL_diff_tolerance:\n",
    "            if verbose:\n",
    "                print('EM break')\n",
    "            break\n",
    "    if verbose:\n",
    "        print('End time:',datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        gamma_new=Poisson_Function.apply(batch_Y,batch_X,batch_s,model.delta_log,model.beta,model.masked.mask,'gamma')\n",
    "    return gamma_new,Q_new,LL_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions.negative_binomial import NegativeBinomial\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.dirichlet import Dirichlet\n",
    "\n",
    "class Custom_Model(nn.Module):\n",
    "    def __init__(self,Y,rho,X_col=5,delta_min=2,LOWER_BOUND=1e-10,THETA_LOWER_BOUND=1e-20):\n",
    "        # Y,rho are needed for model parameter initialization\n",
    "        super(Custom_Model, self).__init__()\n",
    "        \n",
    "        #rho\n",
    "        self.masked=Masked(rho)\n",
    "         \n",
    "        #delta\n",
    "        self.delta_log_min=np.log(delta_min) #\n",
    "        self.delta_log=nn.Parameter(torch.Tensor(np.random.uniform(-2,2,size=rho.shape)),requires_grad=True) # (C,G)\n",
    "        self.delta_log.data=self.delta_log.data.clamp(min=self.delta_log_min)\n",
    "          \n",
    "        \n",
    "        #beta\n",
    "        Y_colmean=np.mean(Y,axis=0)\n",
    "        beta_init=np.hstack([((Y_colmean-Y_colmean.mean())/np.std(Y_colmean)).reshape(-1,1),\\\n",
    "                     np.zeros((Y.shape[1],X_col-1))]).T\n",
    "        self.beta=nn.Parameter(torch.Tensor(beta_init),requires_grad=True) # (P,G)\n",
    "        \n",
    "        \n",
    "    def forward(self,Y,X,s,gamma_fixed=None,mode='E'):\n",
    "        delta_log_masked=self.masked(self.delta_log) #(C,G)\n",
    "        delta=torch.exp(delta_log_masked)*self.masked.mask\n",
    "        \n",
    "        X_beta_s=X.matmul(self.beta)+torch.log(s.view(-1, 1)) #(N,P)*(P,G) + (N,1) = (N,G)\n",
    "        \n",
    "        mu_log=X_beta_s.unsqueeze(dim=1).repeat(1,delta.shape[0],1)+delta #(N,1,G)+(C,G) = (N,C,G)\n",
    "        \n",
    "        mu=torch.exp(mu_log) # (N,C,G)\n",
    "        \n",
    "        #Y_extend=Y.view(Y.shape[0],1,Y.shape[1]).repeat(1,mu_log.shape[1],1) # (N,C,G)\n",
    "        Y_extend=Y.unsqueeze(dim=1).repeat(1,mu_log.shape[1],1)\n",
    "        \n",
    "        # Poisson\n",
    "        Y_logprob=poisson_logprob(rate=mu,value=Y_extend) # (N,C,G)\n",
    "\n",
    "        Y_logprob_reduce=torch.sum(Y_logprob,axis=2)\n",
    "        \n",
    "        Y_logprob_reduce_reduce=torch.logsumexp(Y_logprob_reduce,dim=1).view(-1,1) # (N,1)\n",
    "        \n",
    "        gamma=torch.exp(Y_logprob_reduce-Y_logprob_reduce_reduce) # (N,C)\n",
    "\n",
    "        if mode=='E':\n",
    "            return gamma,None,None\n",
    "        elif mode=='M' or mode=='LL':\n",
    "            if mode=='M':\n",
    "                Q=-torch.sum(gamma_fixed*Y_logprob_reduce) # (N,C) (N,C)\n",
    "                return gamma,Q,None\n",
    "            elif mode=='LL':      \n",
    "                LL=torch.sum(Y_logprob_reduce_reduce) # product of likelihood(y_i)-> (1) \n",
    "                return gamma,None,LL\n",
    "        else:\n",
    "            raise          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions.negative_binomial import NegativeBinomial\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.dirichlet import Dirichlet\n",
    "\n",
    "\n",
    "class Poisson_Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Y, X, s, delta_log, beta, mask,to_return='LL'):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #mu=torch.exp((X.matmul(beta)+torch.log(s.view(-1, 1))).unsqueeze(dim=1).repeat(1,delta_log.shape[0],1)+torch.exp(delta_log))\n",
    "            mu=torch.exp((X.matmul(beta)+torch.log(s.view(-1, 1))).unsqueeze(dim=1).repeat(1,delta_log.shape[0],1)+torch.exp(delta_log)*mask)\n",
    "            Y_extend=Y.unsqueeze(dim=1).repeat(1,mu.shape[1],1)\n",
    "            Y_logprob=poisson_logprob(rate=mu,value=Y_extend) # (N,C,G)\n",
    "            Y_logprob_reduce=Y_logprob.sum(axis=2)\n",
    "            \n",
    "            Y_logprob_reduce_reduce=torch.logsumexp(Y_logprob_reduce,dim=1).view(-1,1)\n",
    "            \n",
    "            LL=torch.sum(Y_logprob_reduce_reduce)\n",
    "            \n",
    "            gamma=torch.exp(Y_logprob_reduce-Y_logprob_reduce_reduce)\n",
    "            A=mu-Y.unsqueeze(dim=1)        \n",
    "            \n",
    "            #gradient\n",
    "            grad_delta_log=(A*gamma.unsqueeze(dim=2)).sum(axis=0)\n",
    "            grad_beta=(X.unsqueeze(dim=2)@gamma.unsqueeze(dim=1)@A).sum(axis=0)\n",
    "        \n",
    "            ctx.save_for_backward(grad_delta_log,grad_beta)\n",
    "            \n",
    "        if to_return=='LL':\n",
    "            return LL\n",
    "        else:\n",
    "            return gamma\n",
    "            \n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \n",
    "        grad_Y = grad_X = grad_s = grad_delta_log = grad_beta = grad_mask=None\n",
    "        grad_delta_log,grad_beta = ctx.saved_tensors\n",
    "\n",
    "        return grad_Y, grad_X, grad_s, grad_delta_log, grad_beta, grad_mask\n",
    "\n",
    "class Poisson_Function_Nomask(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Y, X, s, delta_log, beta, to_return='LL'):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #mu=torch.exp((X.matmul(beta)+torch.log(s.view(-1, 1))).unsqueeze(dim=1).repeat(1,delta_log.shape[0],1)+torch.exp(delta_log))\n",
    "            mu=torch.exp((X.matmul(beta)+torch.log(s.view(-1, 1))).unsqueeze(dim=1).repeat(1,delta_log.shape[0],1)+torch.exp(delta_log))\n",
    "            Y_extend=Y.unsqueeze(dim=1).repeat(1,mu.shape[1],1)\n",
    "            Y_logprob=poisson_logprob(rate=mu,value=Y_extend) # (N,C,G)\n",
    "            Y_logprob_reduce=Y_logprob.sum(axis=2)\n",
    "            \n",
    "            Y_logprob_reduce_reduce=torch.logsumexp(Y_logprob_reduce,dim=1).view(-1,1)\n",
    "            \n",
    "            LL=torch.sum(Y_logprob_reduce_reduce)\n",
    "            \n",
    "            gamma=torch.exp(Y_logprob_reduce-Y_logprob_reduce_reduce)\n",
    "            A=mu-Y.unsqueeze(dim=1)        \n",
    "            \n",
    "            #gradient\n",
    "            grad_delta_log=(A*gamma.unsqueeze(dim=2)).sum(axis=0)\n",
    "            grad_beta=(X.unsqueeze(dim=2)@gamma.unsqueeze(dim=1)@A).sum(axis=0)\n",
    "        \n",
    "            ctx.save_for_backward(grad_delta_log,grad_beta)\n",
    "            \n",
    "        if to_return=='LL':\n",
    "            return LL\n",
    "        else:\n",
    "            return gamma\n",
    "            \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \n",
    "        grad_Y = grad_X = grad_s = grad_delta_log = grad_beta = None\n",
    "        grad_delta_log,grad_beta = ctx.saved_tensors\n",
    "\n",
    "        return grad_Y, grad_X, grad_s, grad_delta_log, grad_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Model_Trick_Hard(nn.Module):\n",
    "    def __init__(self,Y,rho,X_col=5,delta_min=2,LOWER_BOUND=1e-10,THETA_LOWER_BOUND=1e-20):\n",
    "        # Y,rho are needed for model parameter initialization\n",
    "        super(Custom_Model_Trick_Hard, self).__init__()\n",
    "        \n",
    "        #rho\n",
    "        self.masked=Masked(rho)\n",
    "         \n",
    "        #delta\n",
    "        self.delta_log_min=np.log(delta_min) #\n",
    "        self.delta_log=nn.Parameter(torch.Tensor(np.random.uniform(-2,2,size=rho.shape)),requires_grad=True) # (C,G)\n",
    "        self.delta_log.data=self.delta_log.data.clamp(min=self.delta_log_min)\n",
    "          \n",
    "        #beta\n",
    "        Y_colmean=np.mean(Y,axis=0)\n",
    "        beta_init=np.hstack([((Y_colmean-Y_colmean.mean())/(np.std(Y_colmean) if len(Y_colmean)>1 else 1)).reshape(-1,1),\\\n",
    "                     np.zeros((Y.shape[1],X_col-1))]).T\n",
    "        self.beta=nn.Parameter(torch.Tensor(beta_init),requires_grad=True) # (P,G)\n",
    "        \n",
    "        \n",
    "    def init_parameter(self,rho=None,Y=None,delta_min=None):\n",
    "        if delta_min is not None:\n",
    "            self.delta_log_min=np.log(delta_min) #\n",
    "            self.delta_log.data[:]=torch.Tensor(np.random.uniform(-2,2,size=self.delta_log.data.shape))\n",
    "            self.delta_log.data=self.delta_log.data.clamp(min=self.delta_log_min)\n",
    "\n",
    "        if rho is not None:\n",
    "            self.masked.mask.data=(rho==1)\n",
    "        \n",
    "        if Y is not None:\n",
    "            Y_colmean=np.mean(Y,axis=0)\n",
    "            beta_init=np.hstack([((Y_colmean-Y_colmean.mean())/(np.std(Y_colmean) if len(Y_colmean)>1 else 1)).reshape(-1,1),\\\n",
    "                         np.zeros((Y.shape[1],self.beta.shape[0]-1))]).T      \n",
    "            self.beta.data[:]=torch.Tensor(beta_init)\n",
    "        \n",
    "    def forward(self,Y,X,s):\n",
    "        delta_log_masked=self.masked(self.delta_log) #(C,G)\n",
    "        #delta=torch.exp(delta_log_masked)*self.masked.mask\n",
    "        \n",
    "        LL=Poisson_Function.apply(Y,X,s,delta_log_masked,self.beta,self.masked.mask)\n",
    "\n",
    "        return LL  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Model_Trick_Hard_Nomask(nn.Module):\n",
    "    def __init__(self,Y,rho,X_col=5,delta_min=2,LOWER_BOUND=1e-10,THETA_LOWER_BOUND=1e-20):\n",
    "        # Y,rho are needed for model parameter initialization\n",
    "        super(Custom_Model_Trick_Hard_Nomask, self).__init__()\n",
    "         \n",
    "        #delta\n",
    "        self.delta_log_min=np.log(delta_min) #\n",
    "        self.delta_log=nn.Parameter(torch.Tensor(np.random.uniform(-2,2,size=rho.shape)),requires_grad=True) # (C,G)\n",
    "        self.delta_log.data=self.delta_log.data.clamp(min=self.delta_log_min)\n",
    "          \n",
    "        #beta\n",
    "        Y_colmean=np.mean(Y,axis=0)\n",
    "        beta_init=np.hstack([((Y_colmean-Y_colmean.mean())/np.std(Y_colmean)).reshape(-1,1),\\\n",
    "                     np.zeros((Y.shape[1],X_col-1))]).T\n",
    "        self.beta=nn.Parameter(torch.Tensor(beta_init),requires_grad=True) # (P,G)\n",
    "        \n",
    "        \n",
    "    def init_parameter(self,Y=None,delta_min=None):\n",
    "        if delta_min is not None:\n",
    "            self.delta_log_min=np.log(delta_min) #\n",
    "            self.delta_log.data[:]=torch.Tensor(np.random.uniform(-2,2,size=self.delta_log.data.shape))\n",
    "            self.delta_log.data=self.delta_log.data.clamp(min=self.delta_log_min)\n",
    "        \n",
    "        if Y is not None:\n",
    "            Y_colmean=np.mean(Y,axis=0)\n",
    "            beta_init=np.hstack([((Y_colmean-Y_colmean.mean())/np.std(Y_colmean)).reshape(-1,1),\\\n",
    "                         np.zeros((Y.shape[1],self.beta.shape[0]-1))]).T        \n",
    "            self.beta.data[:]=torch.Tensor(beta_init)\n",
    "        \n",
    "    def forward(self,Y,X,s):\n",
    "        \n",
    "        LL=Poisson_Function_Nomask.apply(Y,X,s,self.delta_log,self.beta)\n",
    "\n",
    "        return LL     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Model_Trick(nn.Module):\n",
    "    def __init__(self,Y,rho,X_col=5,delta_min=2,LOWER_BOUND=1e-10,THETA_LOWER_BOUND=1e-20):\n",
    "        # Y,rho are needed for model parameter initialization\n",
    "        super(Custom_Model_Trick, self).__init__()\n",
    "        \n",
    "        #rho\n",
    "        self.masked=Masked(rho)\n",
    "         \n",
    "        #delta\n",
    "        self.delta_log_min=np.log(delta_min) #\n",
    "        self.delta_log=nn.Parameter(torch.Tensor(np.random.uniform(-2,2,size=rho.shape)),requires_grad=True) # (C,G)\n",
    "        self.delta_log.data=self.delta_log.data.clamp(min=self.delta_log_min)\n",
    "          \n",
    "        #beta\n",
    "        Y_colmean=np.mean(Y,axis=0)\n",
    "        beta_init=np.hstack([((Y_colmean-Y_colmean.mean())/(np.std(Y_colmean) if len(Y_colmean)>1 else 1)).reshape(-1,1),\\\n",
    "                     np.zeros((Y.shape[1],X_col-1))]).T\n",
    "        self.beta=nn.Parameter(torch.Tensor(beta_init),requires_grad=True) # (P,G)\n",
    "        \n",
    "        \n",
    "    def init_parameter(self,rho=None,Y=None,delta_min=None):\n",
    "        if delta_min is not None:\n",
    "            self.delta_log_min=np.log(delta_min) #\n",
    "            self.delta_log.data[:]=torch.Tensor(np.random.uniform(-2,2,size=self.delta_log.data.shape))\n",
    "            self.delta_log.data=self.delta_log.data.clamp(min=self.delta_log_min)\n",
    "\n",
    "        if rho is not None:\n",
    "            self.masked.mask.data=(rho==1)\n",
    "        \n",
    "        if Y is not None:\n",
    "            Y_colmean=np.mean(Y,axis=0)\n",
    "            beta_init=np.hstack([((Y_colmean-Y_colmean.mean())/(np.std(Y_colmean) if len(Y_colmean)>1 else 1)).reshape(-1,1),\\\n",
    "                         np.zeros((Y.shape[1],self.beta.shape[0]-1))]).T      \n",
    "            self.beta.data[:]=torch.Tensor(beta_init)\n",
    "        \n",
    "    def forward(self,Y,X,s):\n",
    "        delta_log_masked=self.masked(self.delta_log) #(C,G)\n",
    "        #delta=torch.exp(delta_log_masked)*self.masked.mask\n",
    "        \n",
    "        mu=torch.exp((X.matmul(self.beta)+torch.log(s.view(-1, 1))).unsqueeze(dim=1).repeat(1,self.delta_log.shape[0],1)+torch.exp(delta_log_masked)*self.masked.mask)\n",
    "        Y_extend=Y.unsqueeze(dim=1).repeat(1,mu.shape[1],1)\n",
    "        Y_logprob=poisson_logprob(rate=mu,value=Y_extend) # (N,C,G)\n",
    "        Y_logprob_reduce=Y_logprob.sum(axis=2) # (N,C)\n",
    "\n",
    "        Y_logprob_reduce_reduce=torch.logsumexp(Y_logprob_reduce,dim=1).view(-1,1) # (N,1)\n",
    "\n",
    "        #gamma=torch.exp(Y_logprob_reduce-Y_logprob_reduce_reduce) # (N,C)\n",
    "        \n",
    "        LL=torch.sum(Y_logprob_reduce_reduce) # (N,C)\n",
    "        \n",
    "        return LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asuume that the following variables are initialized\n",
    "\n",
    "# Input\n",
    "Y\n",
    "s=cell_size_factor\n",
    "#X=x_data_null.copy()[:,[0, 1,2,3,4]]\n",
    "#X=x_data_null.copy()[:,[0, 1,2]]\n",
    "#X=x_data_null.copy()[:,[0,1,2,3]]\n",
    "X=x_data_null.copy()#[:,[0,1,2]]\n",
    "# 234 x\n",
    "# 34 x\n",
    "# 123 x\n",
    "rho=marker_onehot\n",
    "\n",
    "delta_min=2\n",
    "LR=1e-1\n",
    "\n",
    "# Optional\n",
    "EM_ITER_MAX=20\n",
    "M_ITER_MAX=10000\n",
    "\n",
    "BATCH_SIZE=Y.shape[0]\n",
    "NUM_WORKERS=0\n",
    "\n",
    "LOWER_BOUND=1e-10\n",
    "THETA_LOWER_BOUND=1e-20\n",
    "\n",
    "\n",
    "Q_diff_tolerance=1e-4\n",
    "LL_diff_tolerance=1e-4\n",
    "\n",
    "device='cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(device)\n",
    "#device_cpu=\n",
    "#device_cuda_list=[torch.device(\"cuda:{}\".format(i)) for i in range(6)][::-1]\n",
    "NUM_WORKERS=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing k=1\n",
      " Complete number of Gene: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ch6845/tools/miniconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/ch6845/tools/miniconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:23: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Complete number of Gene: 3300"
     ]
    }
   ],
   "source": [
    "LL_list_list=[]\n",
    "for num_clsuter in [1,2,3]:\n",
    "    print('testing k={}'.format(num_clsuter))\n",
    "    LL_list=[]\n",
    "    for iter_idx,exp_data_idx in enumerate(np.arange(exp_data.shape[0])):\n",
    "        if iter_idx%100==0:\n",
    "            sys.stdout.write('\\r Complete number of Gene: {}'.format(iter_idx))        \n",
    "        Y=exp_data[exp_data_idx:exp_data_idx+1].transpose().astype(float)\n",
    "\n",
    "        cell_dataset=Cell_Dataset(Y,X,s)\n",
    "        cell_dataloader=DataLoader(dataset=cell_dataset,shuffle=False,batch_size=BATCH_SIZE,num_workers=NUM_WORKERS)    \n",
    "\n",
    "        if iter_idx==0:\n",
    "            model=Custom_Model_Trick_Hard(Y,np.ones((num_clsuter,1)),X_col=X.shape[1],delta_min=0,LOWER_BOUND=LOWER_BOUND,THETA_LOWER_BOUND=THETA_LOWER_BOUND).to(device)\n",
    "        else:\n",
    "            model.init_parameter(Y=Y,delta_min=0)\n",
    "        optimizer = optim.Adam(model.parameters(),lr=LR)#,betas=(0.92, 0.999))\n",
    "        #LL=run_EM_trick(model,optimizer,LL_diff_tolerance=LL_diff_tolerance,Q_diff_tolerance=Q_diff_tolerance)[1].detach().cpu().numpy()\n",
    "        LL=run_EM_trick(model,optimizer,LL_diff_tolerance=LL_diff_tolerance,Q_diff_tolerance=Q_diff_tolerance,verbose=False)[1].detach().cpu().numpy()\n",
    "        LL_list.append(LL)\n",
    "    LL_list_list.append(LL_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "LL_df=pd.DataFrame(LL_list_list).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "LL_df.columns=[1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LL_diff_tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              1\n",
       "1       0.999999\n",
       "2        1.00001\n",
       "3        3.03955\n",
       "4       0.999999\n",
       "          ...   \n",
       "4893     3.31153\n",
       "4894     3.23716\n",
       "4895     2.66095\n",
       "4896     2.28697\n",
       "4897     2.49238\n",
       "Length: 4898, dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.scatter(LL_df[1]/LL_df[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(LL_list).to_csv('LL_{}.csv'.format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
