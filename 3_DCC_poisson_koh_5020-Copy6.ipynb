{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.io import mmread\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ch6845/tools/miniconda3/envs/pytorch/lib/python3.6/os.py'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## expression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data=mmread('data/koh_extract/koh.data.counts.mm').toarray().astype(float)\n",
    "with open('data/koh_extract/koh.data.col','r') as f: exp_data_col=[i.strip().strip('\"') for i in f.read().split()]\n",
    "with open('data/koh_extract/koh.data.row','r') as f: exp_data_row=[i.strip().strip('\"') for i in f.read().split()]\n",
    "assert exp_data.shape==(len(exp_data_row),len(exp_data_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(set(exp_data_row))==len(exp_data_row)\n",
    "assert len(set(exp_data_col))==len(exp_data_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.93134327e+04, 5.81785206e+04, 9.08678570e+03, ...,\n",
       "         7.58501657e+04, 8.94289538e+04, 1.01937411e+05],\n",
       "        [2.48547207e+04, 4.67361235e+04, 1.09620933e+04, ...,\n",
       "         1.85699498e+04, 1.85806217e+04, 2.49668288e+04],\n",
       "        [1.18570138e+04, 2.80836990e+04, 6.80287185e+03, ...,\n",
       "         3.53701816e+04, 5.75775170e+04, 5.59807979e+04],\n",
       "        ...,\n",
       "        [4.83851544e+01, 1.19916350e+01, 6.96361343e+00, ...,\n",
       "         4.88990717e+00, 8.49751588e+00, 7.80405857e+01],\n",
       "        [2.35818377e+01, 4.26839000e+01, 1.80639917e+01, ...,\n",
       "         7.54470439e+00, 2.84437127e+00, 8.88332407e+00],\n",
       "        [6.10992450e+01, 2.59814157e+01, 2.45930809e+01, ...,\n",
       "         4.85579286e-01, 0.00000000e+00, 0.00000000e+00]]), (4898, 446))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_data,exp_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ENSG00000198804',\n",
       "  'ENSG00000210082',\n",
       "  'ENSG00000198712',\n",
       "  'ENSG00000198938',\n",
       "  'ENSG00000198727'],\n",
       " ['SRR3952323', 'SRR3952325', 'SRR3952326', 'SRR3952327', 'SRR3952328'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_data_row[:5],exp_data_col[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cluster info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run</th>\n",
       "      <th>LibraryName</th>\n",
       "      <th>phenoid</th>\n",
       "      <th>libsize.drop</th>\n",
       "      <th>feature.drop</th>\n",
       "      <th>total_features</th>\n",
       "      <th>log10_total_features</th>\n",
       "      <th>total_counts</th>\n",
       "      <th>log10_total_counts</th>\n",
       "      <th>pct_counts_top_50_features</th>\n",
       "      <th>pct_counts_top_100_features</th>\n",
       "      <th>pct_counts_top_200_features</th>\n",
       "      <th>pct_counts_top_500_features</th>\n",
       "      <th>is_cell_control</th>\n",
       "      <th>celltype</th>\n",
       "      <th>tSNE_1</th>\n",
       "      <th>tSNE_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SRR3952323</th>\n",
       "      <td>SRR3952323</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4895</td>\n",
       "      <td>3.689841</td>\n",
       "      <td>2.248411e+06</td>\n",
       "      <td>6.351876</td>\n",
       "      <td>18.278965</td>\n",
       "      <td>25.975390</td>\n",
       "      <td>35.537616</td>\n",
       "      <td>52.410941</td>\n",
       "      <td>False</td>\n",
       "      <td>hESC</td>\n",
       "      <td>9.973465</td>\n",
       "      <td>19.045918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR3952325</th>\n",
       "      <td>SRR3952325</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4887</td>\n",
       "      <td>3.689131</td>\n",
       "      <td>2.271617e+06</td>\n",
       "      <td>6.356335</td>\n",
       "      <td>24.672529</td>\n",
       "      <td>32.222803</td>\n",
       "      <td>41.547358</td>\n",
       "      <td>57.969233</td>\n",
       "      <td>False</td>\n",
       "      <td>hESC</td>\n",
       "      <td>10.366232</td>\n",
       "      <td>21.511833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR3952326</th>\n",
       "      <td>SRR3952326</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4888</td>\n",
       "      <td>3.689220</td>\n",
       "      <td>5.846824e+05</td>\n",
       "      <td>5.766921</td>\n",
       "      <td>22.732839</td>\n",
       "      <td>30.205988</td>\n",
       "      <td>39.431308</td>\n",
       "      <td>55.285817</td>\n",
       "      <td>False</td>\n",
       "      <td>hESC</td>\n",
       "      <td>9.881356</td>\n",
       "      <td>19.317197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR3952327</th>\n",
       "      <td>SRR3952327</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4879</td>\n",
       "      <td>3.688420</td>\n",
       "      <td>3.191810e+06</td>\n",
       "      <td>6.504037</td>\n",
       "      <td>20.867378</td>\n",
       "      <td>29.003904</td>\n",
       "      <td>38.785558</td>\n",
       "      <td>56.020859</td>\n",
       "      <td>False</td>\n",
       "      <td>hESC</td>\n",
       "      <td>8.483966</td>\n",
       "      <td>21.289459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRR3952328</th>\n",
       "      <td>SRR3952328</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>H7hESC</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4873</td>\n",
       "      <td>3.687886</td>\n",
       "      <td>2.190385e+06</td>\n",
       "      <td>6.340521</td>\n",
       "      <td>21.287923</td>\n",
       "      <td>29.423689</td>\n",
       "      <td>39.307683</td>\n",
       "      <td>56.640975</td>\n",
       "      <td>False</td>\n",
       "      <td>hESC</td>\n",
       "      <td>9.017168</td>\n",
       "      <td>20.637262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Run LibraryName phenoid  libsize.drop  feature.drop  \\\n",
       "SRR3952323  SRR3952323      H7hESC  H7hESC         False         False   \n",
       "SRR3952325  SRR3952325      H7hESC  H7hESC         False         False   \n",
       "SRR3952326  SRR3952326      H7hESC  H7hESC         False         False   \n",
       "SRR3952327  SRR3952327      H7hESC  H7hESC         False         False   \n",
       "SRR3952328  SRR3952328      H7hESC  H7hESC         False         False   \n",
       "\n",
       "            total_features  log10_total_features  total_counts  \\\n",
       "SRR3952323            4895              3.689841  2.248411e+06   \n",
       "SRR3952325            4887              3.689131  2.271617e+06   \n",
       "SRR3952326            4888              3.689220  5.846824e+05   \n",
       "SRR3952327            4879              3.688420  3.191810e+06   \n",
       "SRR3952328            4873              3.687886  2.190385e+06   \n",
       "\n",
       "            log10_total_counts  pct_counts_top_50_features  \\\n",
       "SRR3952323            6.351876                   18.278965   \n",
       "SRR3952325            6.356335                   24.672529   \n",
       "SRR3952326            5.766921                   22.732839   \n",
       "SRR3952327            6.504037                   20.867378   \n",
       "SRR3952328            6.340521                   21.287923   \n",
       "\n",
       "            pct_counts_top_100_features  pct_counts_top_200_features  \\\n",
       "SRR3952323                    25.975390                    35.537616   \n",
       "SRR3952325                    32.222803                    41.547358   \n",
       "SRR3952326                    30.205988                    39.431308   \n",
       "SRR3952327                    29.003904                    38.785558   \n",
       "SRR3952328                    29.423689                    39.307683   \n",
       "\n",
       "            pct_counts_top_500_features  is_cell_control celltype     tSNE_1  \\\n",
       "SRR3952323                    52.410941            False     hESC   9.973465   \n",
       "SRR3952325                    57.969233            False     hESC  10.366232   \n",
       "SRR3952326                    55.285817            False     hESC   9.881356   \n",
       "SRR3952327                    56.020859            False     hESC   8.483966   \n",
       "SRR3952328                    56.640975            False     hESC   9.017168   \n",
       "\n",
       "               tSNE_2  \n",
       "SRR3952323  19.045918  \n",
       "SRR3952325  21.511833  \n",
       "SRR3952326  19.317197  \n",
       "SRR3952327  21.289459  \n",
       "SRR3952328  20.637262  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_data_meta=pd.read_csv('data/koh_extract/koh.metadata.tsv',sep='\\t')\n",
    "exp_data_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`clusterid_to_clustername` is used to convert integers in `res.0.8` to cell-type name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panglao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>official gene symbol</th>\n",
       "      <th>cell type</th>\n",
       "      <th>nicknames</th>\n",
       "      <th>ubiquitousness index</th>\n",
       "      <th>product description</th>\n",
       "      <th>gene type</th>\n",
       "      <th>canonical marker</th>\n",
       "      <th>germ layer</th>\n",
       "      <th>organ</th>\n",
       "      <th>sensitivity_human</th>\n",
       "      <th>sensitivity_mouse</th>\n",
       "      <th>specificity_human</th>\n",
       "      <th>specificity_mouse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CTRB1</td>\n",
       "      <td>Acinar cells</td>\n",
       "      <td>CTRB</td>\n",
       "      <td>0.017</td>\n",
       "      <td>chymotrypsinogen B1</td>\n",
       "      <td>protein-coding gene</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Endoderm</td>\n",
       "      <td>Pancreas</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.015920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KLK1</td>\n",
       "      <td>Acinar cells</td>\n",
       "      <td>Klk6</td>\n",
       "      <td>0.013</td>\n",
       "      <td>kallikrein 1</td>\n",
       "      <td>protein-coding gene</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Endoderm</td>\n",
       "      <td>Pancreas</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.012826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RBPJL</td>\n",
       "      <td>Acinar cells</td>\n",
       "      <td>RBP-L|SUHL|RBPSUHL</td>\n",
       "      <td>0.001</td>\n",
       "      <td>recombination signal binding protein for immun...</td>\n",
       "      <td>protein-coding gene</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Endoderm</td>\n",
       "      <td>Pancreas</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PTF1A</td>\n",
       "      <td>Acinar cells</td>\n",
       "      <td>PTF1-p48|bHLHa29</td>\n",
       "      <td>0.001</td>\n",
       "      <td>pancreas associated transcription factor 1a</td>\n",
       "      <td>protein-coding gene</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Endoderm</td>\n",
       "      <td>Pancreas</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.000773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CELA3A</td>\n",
       "      <td>Acinar cells</td>\n",
       "      <td>ELA3|ELA3A</td>\n",
       "      <td>0.001</td>\n",
       "      <td>chymotrypsin like elastase family member 3A</td>\n",
       "      <td>protein-coding gene</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Endoderm</td>\n",
       "      <td>Pancreas</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  official gene symbol     cell type           nicknames  \\\n",
       "0                CTRB1  Acinar cells                CTRB   \n",
       "1                 KLK1  Acinar cells                Klk6   \n",
       "2                RBPJL  Acinar cells  RBP-L|SUHL|RBPSUHL   \n",
       "3                PTF1A  Acinar cells    PTF1-p48|bHLHa29   \n",
       "5               CELA3A  Acinar cells          ELA3|ELA3A   \n",
       "\n",
       "   ubiquitousness index                                product description  \\\n",
       "0                 0.017                                chymotrypsinogen B1   \n",
       "1                 0.013                                       kallikrein 1   \n",
       "2                 0.001  recombination signal binding protein for immun...   \n",
       "3                 0.001        pancreas associated transcription factor 1a   \n",
       "5                 0.001        chymotrypsin like elastase family member 3A   \n",
       "\n",
       "             gene type  canonical marker germ layer     organ  \\\n",
       "0  protein-coding gene               1.0   Endoderm  Pancreas   \n",
       "1  protein-coding gene               1.0   Endoderm  Pancreas   \n",
       "2  protein-coding gene               1.0   Endoderm  Pancreas   \n",
       "3  protein-coding gene               1.0   Endoderm  Pancreas   \n",
       "5  protein-coding gene               1.0   Endoderm  Pancreas   \n",
       "\n",
       "   sensitivity_human  sensitivity_mouse  specificity_human  specificity_mouse  \n",
       "0           1.000000           0.957143           0.000629           0.015920  \n",
       "1           0.833333           0.314286           0.005031           0.012826  \n",
       "2           0.000000           0.000000           0.000000           0.000000  \n",
       "3           0.000000           0.157143           0.000629           0.000773  \n",
       "5           0.833333           0.128571           0.000000           0.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markers_db=pd.read_csv(\"data/PanglaoDB_markers_27_Mar_2020.tsv.gz\",sep='\\t')\n",
    "markers_db=markers_db[markers_db['species'].str.contains('Hs')].drop(columns='species')\n",
    "markers_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#markers_db[(markers_db['official gene symbol']=='FXDY2') |(markers_db['nicknames'].str.contains('FXDY2'))],'FXDY2' in exp_data_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marker info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustername_to_markers=pd.read_csv('data/koh_extract/koh.rho.tsv',sep='\\t').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000008311</th>\n",
       "      <th>ENSG00000018625</th>\n",
       "      <th>ENSG00000019549</th>\n",
       "      <th>ENSG00000026025</th>\n",
       "      <th>ENSG00000039068</th>\n",
       "      <th>ENSG00000042493</th>\n",
       "      <th>ENSG00000053438</th>\n",
       "      <th>ENSG00000075340</th>\n",
       "      <th>ENSG00000079102</th>\n",
       "      <th>ENSG00000092068</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000240563</th>\n",
       "      <th>ENSG00000241186</th>\n",
       "      <th>ENSG00000243004</th>\n",
       "      <th>ENSG00000249532</th>\n",
       "      <th>ENSG00000250305</th>\n",
       "      <th>ENSG00000254277</th>\n",
       "      <th>ENSG00000254339</th>\n",
       "      <th>ENSG00000260342</th>\n",
       "      <th>ENSG00000260834</th>\n",
       "      <th>ENSG00000280623</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hESC</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MPS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLL1pPXM</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESMT</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sclrtm</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D5CntrlDrmmtm</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2LtM</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ENSG00000008311  ENSG00000018625  ENSG00000019549  \\\n",
       "hESC                         1                1                0   \n",
       "APS                          0                0                1   \n",
       "MPS                          0                0                1   \n",
       "DLL1pPXM                     0                1                1   \n",
       "ESMT                         0                1                1   \n",
       "Sclrtm                       0                1                1   \n",
       "D5CntrlDrmmtm                0                1                1   \n",
       "D2LtM                        0                1                1   \n",
       "\n",
       "               ENSG00000026025  ENSG00000039068  ENSG00000042493  \\\n",
       "hESC                         0                1                1   \n",
       "APS                          0                1                1   \n",
       "MPS                          0                1                1   \n",
       "DLL1pPXM                     1                1                1   \n",
       "ESMT                         1                1                1   \n",
       "Sclrtm                       1                0                0   \n",
       "D5CntrlDrmmtm                1                0                1   \n",
       "D2LtM                        1                1                1   \n",
       "\n",
       "               ENSG00000053438  ENSG00000075340  ENSG00000079102  \\\n",
       "hESC                         0                1                1   \n",
       "APS                          0                1                1   \n",
       "MPS                          0                1                1   \n",
       "DLL1pPXM                     0                1                0   \n",
       "ESMT                         1                1                1   \n",
       "Sclrtm                       0                0                1   \n",
       "D5CntrlDrmmtm                0                1                1   \n",
       "D2LtM                        0                1                1   \n",
       "\n",
       "               ENSG00000092068  ...  ENSG00000240563  ENSG00000241186  \\\n",
       "hESC                         1  ...                1                1   \n",
       "APS                          1  ...                1                1   \n",
       "MPS                          1  ...                1                1   \n",
       "DLL1pPXM                     1  ...                0                0   \n",
       "ESMT                         1  ...                0                0   \n",
       "Sclrtm                       0  ...                0                0   \n",
       "D5CntrlDrmmtm                1  ...                0                0   \n",
       "D2LtM                        1  ...                0                0   \n",
       "\n",
       "               ENSG00000243004  ENSG00000249532  ENSG00000250305  \\\n",
       "hESC                         1                1                0   \n",
       "APS                          1                1                1   \n",
       "MPS                          1                1                1   \n",
       "DLL1pPXM                     1                0                1   \n",
       "ESMT                         1                0                1   \n",
       "Sclrtm                       0                0                1   \n",
       "D5CntrlDrmmtm                0                0                1   \n",
       "D2LtM                        1                0                1   \n",
       "\n",
       "               ENSG00000254277  ENSG00000254339  ENSG00000260342  \\\n",
       "hESC                         1                1                1   \n",
       "APS                          1                1                1   \n",
       "MPS                          1                1                1   \n",
       "DLL1pPXM                     0                0                1   \n",
       "ESMT                         0                0                1   \n",
       "Sclrtm                       0                0                0   \n",
       "D5CntrlDrmmtm                0                0                1   \n",
       "D2LtM                        0                0                1   \n",
       "\n",
       "               ENSG00000260834  ENSG00000280623  \n",
       "hESC                         1                0  \n",
       "APS                          1                0  \n",
       "MPS                          1                1  \n",
       "DLL1pPXM                     1                0  \n",
       "ESMT                         1                0  \n",
       "Sclrtm                       0                0  \n",
       "D5CntrlDrmmtm                1                0  \n",
       "D2LtM                        1                0  \n",
       "\n",
       "[8 rows x 84 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustername_to_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 50)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clustername_to_markers.loc[['Sclrtm','D5CntrlDrmmtm']].T.iloc[:,1][clustername_to_markers.loc[['Sclrtm','D5CntrlDrmmtm']].T.iloc[:,1]==1].index.intersection(\n",
    "    clustername_to_markers.loc[['Sclrtm','D5CntrlDrmmtm']].T.iloc[:,0][clustername_to_markers.loc[['Sclrtm','D5CntrlDrmmtm']].T.iloc[:,0]==1].index\n",
    ")),len(clustername_to_markers.loc[['Sclrtm','D5CntrlDrmmtm']].T.iloc[:,1][clustername_to_markers.loc[['Sclrtm','D5CntrlDrmmtm']].T.iloc[:,1]==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustername_unique=list(clustername_to_markers.index)\n",
    "exp_data_meta_clusterid_clusteridunique=exp_data_meta['celltype'].apply(lambda x: clustername_unique.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_unique=list(clustername_to_markers.columns)\n",
    "marker_unique_exp_data_idx=[exp_data_row.index(marker) for marker in marker_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_true=np.array([np.sum(exp_data_meta_clusterid_clusteridunique==i) for i in sorted(np.unique(exp_data_meta_clusterid_clusteridunique))])/exp_data_meta_clusterid_clusteridunique.shape[0]\n",
    "M_true=np.array([np.mean(exp_data[marker_unique_exp_data_idx,:][:,exp_data_meta_clusterid_clusteridunique==i],axis=1) for i in sorted(np.unique(exp_data_meta_clusterid_clusteridunique))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(446,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_size_factor=pd.read_csv('data/analysis/koh.size_factor_cluster.tsv',sep='\\t',header=None)[0].values.astype(float)#.reshape(-1,1)\n",
    "#cell_size_factor=np.ones_like(cell_size_factor)\n",
    "cell_size_factor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(446, 84)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=exp_data[marker_unique_exp_data_idx].transpose().astype(float)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#marker_onehot=np.array([np.sum(np.eye(len(marker_unique))[[marker_unique.tolist().index(marker) for marker in value]],axis=0) for key,value in clustername_to_markers.items()])\n",
    "#marker_onehot.shape\n",
    "#marker_onehot.T\n",
    "marker_onehot=clustername_to_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(446, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exp_data_col_patient=pd.Series(exp_data_col).str.slice(start=1,stop=2).astype(int).values\n",
    "#x_data_covariate=np.eye(len(np.unique(exp_data_col_patient)))[exp_data_col_patient-1]\n",
    "x_data_intercept=np.array([np.ones(Y.shape[0])]).transpose()\n",
    "x_data_null=np.concatenate([x_data_intercept],axis=1)\n",
    "x_data_null.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(446, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Asuume that the following variables are initialized\n",
    "\n",
    "# Input\n",
    "Y\n",
    "s=cell_size_factor\n",
    "#X=x_data_null.copy()[:,[0, 1,2,3,4]]\n",
    "#X=x_data_null.copy()[:,[0, 1,2]]\n",
    "#X=x_data_null.copy()[:,[0,1,2,3]]\n",
    "X=x_data_null.copy()#[:,[0,1,2]]\n",
    "# 234 x\n",
    "# 34 x\n",
    "# 123 x\n",
    "rho=marker_onehot\n",
    "\n",
    "delta_min=2\n",
    "B=10\n",
    "LR=1e-1\n",
    "\n",
    "# Optional\n",
    "EM_ITER_MAX=20\n",
    "M_ITER_MAX=10000\n",
    "\n",
    "BATCH_SIZE=Y.shape[0]\n",
    "NUM_WORKERS=0\n",
    "\n",
    "LOWER_BOUND=1e-10\n",
    "THETA_LOWER_BOUND=1e-20\n",
    "\n",
    "\n",
    "Q_diff_tolerance=1e-4\n",
    "LL_diff_tolerance=1e-4\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_cpu=torch.device(\"cpu\")\n",
    "device_cuda_list=[torch.device(\"cuda:{}\".format(i)) for i in range(6)][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from basic_tools import Cell_Dataset,Masked\n",
    "class Masked_Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, mask):\n",
    "        #print('aaaa')\n",
    "        output=input\n",
    "        ctx.save_for_backward(input, mask)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, mask = ctx.saved_tensors\n",
    "        grad_input = grad_mask = None\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mul(mask)\n",
    "\n",
    "        return grad_input, grad_mask\n",
    "    \n",
    "class Masked(nn.Module):    \n",
    "    def __init__(self, mask):    \n",
    "        super(Masked, self).__init__()\n",
    "        \n",
    "        self.mask = nn.Parameter(torch.Tensor(mask)==1, requires_grad=False)    \n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        return Masked_Function.apply(input, self.mask)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'mask={}'.format(self.mask.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class NB_logprob(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NB_logprob,self).__init__()\n",
    "        \n",
    "    def forward(self,total_count,probs,value):\n",
    "        \n",
    "        #eps = torch.finfo(probs.dtype).eps\n",
    "        #probs_clamped=probs.clamp(min=eps, max=1 - eps)        \n",
    "        probs_clamped=probs\n",
    "        logits=torch.log(probs_clamped) - torch.log1p(-probs_clamped)\n",
    "        #logits=torch.log(probs_clamped)\n",
    "        \n",
    "        log_unnormalized_prob = (total_count * F.logsigmoid(-logits) +\n",
    "                                 value * F.logsigmoid(logits))\n",
    "        log_normalization = (-torch.lgamma(total_count + value) + torch.lgamma(1. + value) +\n",
    "                             torch.lgamma(total_count))\n",
    "\n",
    "        return log_unnormalized_prob - log_normalization\n",
    "    \n",
    "class Normal_logprob(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Normal_logprob,self).__init__()\n",
    "        \n",
    "    def forward(self,loc,scale,value):\n",
    "    \n",
    "        var = (scale ** 2)\n",
    "        log_scale = torch.log(scale)\n",
    "        \n",
    "        \n",
    "        #print(loc.shape,scale.shape,value.shape)\n",
    "        return -((value - loc) ** 2) / (2 * var) - log_scale - math.log(math.sqrt(2 * math.pi))    \n",
    "        #return log_scale\n",
    "        \n",
    "class Poisson_logprob(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Poisson_logprob,self).__init__()\n",
    "        \n",
    "    def forward(self,rate,value):\n",
    "        #rate=rate.clamp(min=1e-3)+(-1)/rate.clamp(max=-1e-5)\n",
    "        \n",
    "        return (rate.log() * value) - rate - (value + 1).lgamma()\n",
    "    \n",
    "class Dirichlet_logprob(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dirichlet_logprob,self).__init__()\n",
    "        \n",
    "    def forward(self,concentration,value):\n",
    "        \n",
    "        return ((torch.log(value) * (concentration - 1.0)).sum(-1) +\n",
    "                torch.lgamma(concentration.sum(-1)) -\n",
    "                torch.lgamma(concentration).sum(-1))\n",
    "    \n",
    "        \n",
    "NB_logprob=NB_logprob()        \n",
    "normal_logprob=Normal_logprob()\n",
    "poisson_logprob=Poisson_logprob()\n",
    "dirichlet_logprob=Dirichlet_logprob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell_Dataset(Dataset):\n",
    "    def __init__(self,Y,X,s):\n",
    "        self.Y=Y\n",
    "        self.X=X\n",
    "        self.s=s\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.Y.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        item= {\"Y\":self.Y[idx,:],\"X\":self.X[idx,:],\"s\":self.s[idx]}\n",
    "        return item  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_EM(model,optimizer):\n",
    "    global gamma_new,Q_new,LL_new\n",
    "    \n",
    "    print('Start time:',datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx,batch in enumerate(cell_dataloader):\n",
    "            batch_Y=batch['Y'].to(device)\n",
    "            batch_X=batch['X'].to(device)\n",
    "            batch_s=batch['s'].to(device)\n",
    "        gamma_new=Poisson_Function_Nomask.apply(batch_Y,batch_X,batch_s,model.delta_log,model.beta,'gamma')\n",
    "        gamma_fixed,_,LL_old=model(batch_Y,batch_X,batch_s,gamma_fixed=None,mode='LL')\n",
    "        _,Q_old,_=model(batch_Y,batch_X,batch_s,gamma_fixed=gamma_fixed,mode='M')\n",
    "\n",
    "    print(LL_old)\n",
    "    print(Q_old)\n",
    "\n",
    "    for em_idx in range(EM_ITER_MAX):#\n",
    "        #optimizer = optim.Adam(model.parameters(),lr=0.1,eps=1e-3,betas=(0.9,0.999))\n",
    "        LL_new=torch.zeros_like(LL_old)\n",
    "        #optimizer = optim.Adam(model.parameters(),lr=LR)\n",
    "        for batch_idx,batch in enumerate(cell_dataloader):\n",
    "            # It is usually just one iteration(batch).\n",
    "            # However, developer of cellAssign may have done this for extreme situation of larse sample size\n",
    "            batch_Y=batch['Y'].to(device)\n",
    "            batch_X=batch['X'].to(device)\n",
    "            batch_s=batch['s'].to(device)\n",
    "\n",
    "            #############\n",
    "            #E-step\n",
    "            ######### ####\n",
    "            with torch.no_grad():\n",
    "                gamma_new,_,_=model(batch_Y,batch_X,batch_s,gamma_fixed=None,mode='E')\n",
    "\n",
    "            #############\n",
    "            #M-step\n",
    "            #############\n",
    "            for m_idx in range(M_ITER_MAX):#\n",
    "            #for m_idx in range(20):#    \n",
    "                optimizer.zero_grad()\n",
    "                _,Q_new,_=model(batch_Y,batch_X,batch_s,gamma_fixed=gamma_new,mode='M')\n",
    "                Q_new.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                #Constraint\n",
    "                model.delta_log.data=model.delta_log.data.clamp(min=model.delta_log_min)\n",
    "                #model.NB_basis_a.data=model.NB_basis_a.data.clamp(min=0)\n",
    "\n",
    "                if m_idx%20==0:\n",
    "                    #print(sorted(model.delta_log.cpu().detach().numpy().flatten())[-10:])\n",
    "                    Q_diff=(Q_old-Q_new)/torch.abs(Q_old)\n",
    "                    Q_old=Q_new\n",
    "                    print('M: {}, Q: {} Q_diff: {}'.format(m_idx,Q_new,Q_diff))    \n",
    "                    if m_idx>0 and torch.abs(Q_diff)<Q_diff_tolerance:\n",
    "                        print('M break')\n",
    "                        break                \n",
    "            #############\n",
    "            #Look at LL\n",
    "            #############\n",
    "            with torch.no_grad():\n",
    "                _,_,LL_temp=model(batch_Y,batch_X,batch_s,gamma_fixed=None,mode='LL')\n",
    "                LL_new+=LL_temp\n",
    "\n",
    "        LL_diff=(LL_new-LL_old)/torch.abs(LL_old)\n",
    "        LL_old=LL_new\n",
    "        print('EM: {}, LL: {} LL_diff: {}'.format(em_idx,LL_new,LL_diff))\n",
    "        if LL_diff<LL_diff_tolerance:\n",
    "            print('EM break')\n",
    "            break\n",
    "    print('End time:',datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))    \n",
    "    return gamma_new,Q_new,LL_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_EM_trick(model,optimizer,LL_diff_tolerance,Q_diff_tolerance,verbose=True):\n",
    "    global gamma_new,LL_new\n",
    "    \n",
    "    if verbose:\n",
    "        print('Start time:',datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx,batch in enumerate(cell_dataloader):\n",
    "            batch_Y=batch['Y'].to(device)\n",
    "            batch_X=batch['X'].to(device)\n",
    "            batch_s=batch['s'].to(device)    \n",
    "        LL_old=model(batch_Y,batch_X,batch_s)\n",
    "        Q_old=LL_old\n",
    "    if verbose:\n",
    "        print(LL_old)\n",
    "\n",
    "    for em_idx in range(EM_ITER_MAX):#\n",
    "        #optimizer = optim.Adam(model.parameters(),lr=0.1,eps=1e-3,betas=(0.9,0.999))\n",
    "        LL_new=torch.zeros_like(LL_old)\n",
    "        #optimizer = optim.Adam(model.parameters(),lr=LR)\n",
    "        for batch_idx,batch in enumerate(cell_dataloader):\n",
    "            # It is usually just one iteration(batch).\n",
    "            # However, developer of cellAssign may have done this for extreme situation of larse sample size\n",
    "            batch_Y=batch['Y'].to(device)\n",
    "            batch_X=batch['X'].to(device)\n",
    "            batch_s=batch['s'].to(device)\n",
    "\n",
    "            #############\n",
    "            #M-step\n",
    "            #############\n",
    "            for m_idx in range(M_ITER_MAX):#\n",
    "            #for m_idx in range(20):#    \n",
    "                optimizer.zero_grad()\n",
    "                Q_new=-model(batch_Y,batch_X,batch_s)\n",
    "                Q_new.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                #Constraint\n",
    "                model.delta_log.data=model.delta_log.data.clamp(min=model.delta_log_min)\n",
    "                #model.NB_basis_a.data=model.NB_basis_a.data.clamp(min=0)\n",
    "\n",
    "                if m_idx%20==0:\n",
    "                    #print(sorted(model.delta_log.cpu().detach().numpy().flatten())[-10:])\n",
    "                    Q_diff=(Q_old-Q_new)/torch.abs(Q_old)\n",
    "                    Q_old=Q_new\n",
    "                    if verbose:\n",
    "                        print('M: {}, Q: {} Q_diff: {}'.format(m_idx,Q_new,Q_diff))    \n",
    "                    if m_idx>0 and torch.abs(Q_diff)<(Q_diff_tolerance):\n",
    "                        if verbose:\n",
    "                            print('M break')\n",
    "                        break                \n",
    "            #############\n",
    "            #Look at LL\n",
    "            #############\n",
    "            with torch.no_grad():\n",
    "                LL_temp=-Q_new\n",
    "                LL_new+=LL_temp\n",
    "\n",
    "        LL_diff=(LL_new-LL_old)/torch.abs(LL_old)\n",
    "        LL_old=LL_new\n",
    "        if verbose:\n",
    "            print('EM: {}, LL: {} LL_diff: {}'.format(em_idx,LL_new,LL_diff))\n",
    "        if LL_diff<LL_diff_tolerance:\n",
    "            if verbose:\n",
    "                print('EM break')\n",
    "            break\n",
    "    if verbose:\n",
    "        print('End time:',datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        gamma_new=Poisson_Function.apply(batch_Y,batch_X,batch_s,model.delta_log,model.beta,model.masked.mask,'gamma')\n",
    "    return gamma_new,Q_new,LL_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef run_quasi_trick(model,optimizer):\\n    global gamma_new,LL_new\\n    \\n    print('Start time:',datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\\n    \\n    with torch.no_grad():\\n        for batch_idx,batch in enumerate(cell_dataloader):\\n            batch_Y=batch['Y'].to(device)\\n            batch_X=batch['X'].to(device)\\n            batch_s=batch['s'].to(device)    \\n        LL_old=model(batch_Y,batch_X,batch_s)\\n        Q_old=LL_old\\n    print(LL_old)\\n\\n\\n    \\n    \\n    for em_idx in range(EM_ITER_MAX):#\\n        #optimizer = optim.Adam(model.parameters(),lr=0.1,eps=1e-3,betas=(0.9,0.999))\\n        LL_new=torch.zeros_like(LL_old)\\n        #optimizer = optim.Adam(model.parameters(),lr=LR)\\n        for batch_idx,batch in enumerate(cell_dataloader):\\n            # It is usually just one iteration(batch).\\n            # However, developer of cellAssign may have done this for extreme situation of larse sample size\\n            batch_Y=batch['Y'].to(device)\\n            batch_X=batch['X'].to(device)\\n            batch_s=batch['s'].to(device)\\n\\n            #############\\n            #M-step\\n            #############\\n            \\n            \\n            for m_idx in range(M_ITER_MAX):#\\n            #for m_idx in range(20):#    \\n         \\n                optimizer.step(closure)\\n                \\n                #Constraint\\n                model.delta_log.data=model.delta_log.data.clamp(min=model.delta_log_min)\\n                #model.NB_basis_a.data=model.NB_basis_a.data.clamp(min=0)\\n                \\n                #with torch.no_grad():\\n                #    Q=Poisson_Function.apply(batch_Y,batch_X,batch_s,model.delta_log,model.beta,model.masked.mask,'LL')\\n                #    gamma_new=Poisson_Function.apply(batch_Y,batch_X,batch_s,model.delta_log,model.beta,model.masked.mask,'gamma')\\n                #print(Q,gamma_new[:3])                   \\n\\n\\n                if m_idx%20==0:\\n                    #print(sorted(model.delta_log.cpu().detach().numpy().flatten())[-10:])\\n                    Q_diff=(Q_old-Q_new)/torch.abs(Q_old)\\n                    Q_old=Q_new\\n                    print('M: {}, Q: {} Q_diff: {}'.format(m_idx,Q_new,Q_diff))    \\n                    if m_idx>0 and torch.abs(Q_diff)<(Q_diff_tolerance):\\n                        print('M break')\\n                        break                \\n            #############\\n            #Look at LL\\n            #############\\n            with torch.no_grad():\\n                LL_temp=-Q_new\\n                LL_new+=LL_temp\\n\\n        LL_diff=(LL_new-LL_old)/torch.abs(LL_old)\\n        LL_old=LL_new\\n        print('EM: {}, LL: {} LL_diff: {}'.format(em_idx,LL_new,LL_diff))\\n        if LL_diff<LL_diff_tolerance:\\n            print('EM break')\\n            break\\n    print('End time:',datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))    \\n    \\n    with torch.no_grad():\\n        gamma_new=Poisson_Function.apply(batch_Y,batch_X,batch_s,model.delta_log,model.beta,model.masked.mask,'gamma')\\n    return gamma_new,Q_new,LL_new\\n\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_quasi_trick(model,optimizer,LL_diff_tolerance,Q_diff_tolerance,verbose=True):\n",
    "    global gamma_new,LL_new\n",
    "    \n",
    "    if verbose:\n",
    "        print('Start time:',datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx,batch in enumerate(cell_dataloader):\n",
    "            batch_Y=batch['Y'].to(device)\n",
    "            batch_X=batch['X'].to(device)\n",
    "            batch_s=batch['s'].to(device)    \n",
    "        LL_old=model(batch_Y,batch_X,batch_s)\n",
    "        Q_old=LL_old\n",
    "    if verbose:\n",
    "        print(LL_old)\n",
    "        \n",
    "        \n",
    "    def closure():\n",
    "        global Q_new\n",
    "        optimizer.zero_grad()\n",
    "        Q_new = -model(batch_Y,batch_X,batch_s)\n",
    "        Q_new.backward()\n",
    "        print(Q_new)\n",
    "        return Q_new\n",
    "    \n",
    "\n",
    "    for em_idx in range(EM_ITER_MAX):#\n",
    "        #optimizer = optim.Adam(model.parameters(),lr=0.1,eps=1e-3,betas=(0.9,0.999))\n",
    "        LL_new=torch.zeros_like(LL_old)\n",
    "        #optimizer = optim.Adam(model.parameters(),lr=LR)\n",
    "        for batch_idx,batch in enumerate(cell_dataloader):\n",
    "            # It is usually just one iteration(batch).\n",
    "            # However, developer of cellAssign may have done this for extreme situation of larse sample size\n",
    "            batch_Y=batch['Y'].to(device)\n",
    "            batch_X=batch['X'].to(device)\n",
    "            batch_s=batch['s'].to(device)\n",
    "\n",
    "            #############\n",
    "            #M-step\n",
    "            #############\n",
    "            for m_idx in range(M_ITER_MAX):#\n",
    "            #for m_idx in range(20):#    \n",
    "                optimizer.zero_grad()\n",
    "                optimizer.step(closure)\n",
    "                \n",
    "                #Constraint\n",
    "                model.delta_log.data=model.delta_log.data.clamp(min=model.delta_log_min)\n",
    "                #model.NB_basis_a.data=model.NB_basis_a.data.clamp(min=0)\n",
    "\n",
    "                if m_idx%20==0:\n",
    "                    #print(sorted(model.delta_log.cpu().detach().numpy().flatten())[-10:])\n",
    "                    Q_diff=(Q_old-Q_new)/torch.abs(Q_old)\n",
    "                    Q_old=Q_new\n",
    "                    if verbose:\n",
    "                        print('M: {}, Q: {} Q_diff: {}'.format(m_idx,Q_new,Q_diff))    \n",
    "                    if m_idx>0 and torch.abs(Q_diff)<(Q_diff_tolerance):\n",
    "                        if verbose:\n",
    "                            print('M break')\n",
    "                        break                \n",
    "            #############\n",
    "            #Look at LL\n",
    "            #############\n",
    "            with torch.no_grad():\n",
    "                LL_temp=-Q_new\n",
    "                LL_new+=LL_temp\n",
    "\n",
    "        LL_diff=(LL_new-LL_old)/torch.abs(LL_old)\n",
    "        LL_old=LL_new\n",
    "        if verbose:\n",
    "            print('EM: {}, LL: {} LL_diff: {}'.format(em_idx,LL_new,LL_diff))\n",
    "        if LL_diff<LL_diff_tolerance:\n",
    "            if verbose:\n",
    "                print('EM break')\n",
    "            break\n",
    "    if verbose:\n",
    "        print('End time:',datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        gamma_new=Poisson_Function.apply(batch_Y,batch_X,batch_s,model.delta_log,model.beta,model.masked.mask,'gamma')\n",
    "    return gamma_new,Q_new,LL_new\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def run_quasi_trick(model,optimizer):\n",
    "    global gamma_new,LL_new\n",
    "    \n",
    "    print('Start time:',datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx,batch in enumerate(cell_dataloader):\n",
    "            batch_Y=batch['Y'].to(device)\n",
    "            batch_X=batch['X'].to(device)\n",
    "            batch_s=batch['s'].to(device)    \n",
    "        LL_old=model(batch_Y,batch_X,batch_s)\n",
    "        Q_old=LL_old\n",
    "    print(LL_old)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    for em_idx in range(EM_ITER_MAX):#\n",
    "        #optimizer = optim.Adam(model.parameters(),lr=0.1,eps=1e-3,betas=(0.9,0.999))\n",
    "        LL_new=torch.zeros_like(LL_old)\n",
    "        #optimizer = optim.Adam(model.parameters(),lr=LR)\n",
    "        for batch_idx,batch in enumerate(cell_dataloader):\n",
    "            # It is usually just one iteration(batch).\n",
    "            # However, developer of cellAssign may have done this for extreme situation of larse sample size\n",
    "            batch_Y=batch['Y'].to(device)\n",
    "            batch_X=batch['X'].to(device)\n",
    "            batch_s=batch['s'].to(device)\n",
    "\n",
    "            #############\n",
    "            #M-step\n",
    "            #############\n",
    "            \n",
    "            \n",
    "            for m_idx in range(M_ITER_MAX):#\n",
    "            #for m_idx in range(20):#    \n",
    "         \n",
    "                optimizer.step(closure)\n",
    "                \n",
    "                #Constraint\n",
    "                model.delta_log.data=model.delta_log.data.clamp(min=model.delta_log_min)\n",
    "                #model.NB_basis_a.data=model.NB_basis_a.data.clamp(min=0)\n",
    "                \n",
    "                #with torch.no_grad():\n",
    "                #    Q=Poisson_Function.apply(batch_Y,batch_X,batch_s,model.delta_log,model.beta,model.masked.mask,'LL')\n",
    "                #    gamma_new=Poisson_Function.apply(batch_Y,batch_X,batch_s,model.delta_log,model.beta,model.masked.mask,'gamma')\n",
    "                #print(Q,gamma_new[:3])                   \n",
    "\n",
    "\n",
    "                if m_idx%20==0:\n",
    "                    #print(sorted(model.delta_log.cpu().detach().numpy().flatten())[-10:])\n",
    "                    Q_diff=(Q_old-Q_new)/torch.abs(Q_old)\n",
    "                    Q_old=Q_new\n",
    "                    print('M: {}, Q: {} Q_diff: {}'.format(m_idx,Q_new,Q_diff))    \n",
    "                    if m_idx>0 and torch.abs(Q_diff)<(Q_diff_tolerance):\n",
    "                        print('M break')\n",
    "                        break                \n",
    "            #############\n",
    "            #Look at LL\n",
    "            #############\n",
    "            with torch.no_grad():\n",
    "                LL_temp=-Q_new\n",
    "                LL_new+=LL_temp\n",
    "\n",
    "        LL_diff=(LL_new-LL_old)/torch.abs(LL_old)\n",
    "        LL_old=LL_new\n",
    "        print('EM: {}, LL: {} LL_diff: {}'.format(em_idx,LL_new,LL_diff))\n",
    "        if LL_diff<LL_diff_tolerance:\n",
    "            print('EM break')\n",
    "            break\n",
    "    print('End time:',datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        gamma_new=Poisson_Function.apply(batch_Y,batch_X,batch_s,model.delta_log,model.beta,model.masked.mask,'gamma')\n",
    "    return gamma_new,Q_new,LL_new\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions.negative_binomial import NegativeBinomial\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.dirichlet import Dirichlet\n",
    "\n",
    "class Custom_Model(nn.Module):\n",
    "    def __init__(self,Y,rho,X_col=5,delta_min=2,LOWER_BOUND=1e-10,THETA_LOWER_BOUND=1e-20):\n",
    "        # Y,rho are needed for model parameter initialization\n",
    "        super(Custom_Model, self).__init__()\n",
    "        \n",
    "        #rho\n",
    "        self.masked=Masked(rho)\n",
    "         \n",
    "        #delta\n",
    "        self.delta_log_min=np.log(delta_min) #\n",
    "        self.delta_log=nn.Parameter(torch.Tensor(np.random.uniform(-2,2,size=rho.shape)),requires_grad=True) # (C,G)\n",
    "        self.delta_log.data=self.delta_log.data.clamp(min=self.delta_log_min)\n",
    "          \n",
    "        \n",
    "        #beta\n",
    "        Y_colmean=np.mean(Y,axis=0)\n",
    "        beta_init=np.hstack([((Y_colmean-Y_colmean.mean())/np.std(Y_colmean)).reshape(-1,1),\\\n",
    "                     np.zeros((Y.shape[1],X_col-1))]).T\n",
    "        self.beta=nn.Parameter(torch.Tensor(beta_init),requires_grad=True) # (P,G)\n",
    "        \n",
    "\n",
    "        # lower bounds\n",
    "        self.LOWER_BOUND=LOWER_BOUND\n",
    "        self.THETA_LOWER_BOUND=THETA_LOWER_BOUND\n",
    "        \n",
    "    def forward(self,Y,X,s,gamma_fixed=None,mode='E'):\n",
    "        delta_log_masked=self.masked(self.delta_log) #(C,G)\n",
    "        delta=torch.exp(delta_log_masked)*self.masked.mask\n",
    "        \n",
    "        X_beta_s=X.matmul(self.beta)+torch.log(s.view(-1, 1)) #(N,P)*(P,G) + (N,1) = (N,G)\n",
    "        \n",
    "        mu_log=X_beta_s.unsqueeze(dim=1).repeat(1,delta.shape[0],1)+delta #(N,1,G)+(C,G) = (N,C,G)\n",
    "        \n",
    "        mu=torch.exp(mu_log) # (N,C,G)\n",
    "        \n",
    "        #Y_extend=Y.view(Y.shape[0],1,Y.shape[1]).repeat(1,mu_log.shape[1],1) # (N,C,G)\n",
    "        Y_extend=Y.unsqueeze(dim=1).repeat(1,mu_log.shape[1],1)\n",
    "        \n",
    "        # Poisson\n",
    "        Y_logprob=poisson_logprob(rate=mu,value=Y_extend) # (N,C,G)\n",
    "\n",
    "        Y_logprob_reduce=torch.sum(Y_logprob,axis=2)\n",
    "        \n",
    "        Y_logprob_reduce_reduce=torch.logsumexp(Y_logprob_reduce,dim=1).view(-1,1) # (N,1)\n",
    "        \n",
    "        gamma=torch.exp(Y_logprob_reduce-Y_logprob_reduce_reduce) # (N,C)\n",
    "\n",
    "        if mode=='E':\n",
    "            return gamma,None,None\n",
    "        elif mode=='M' or mode=='LL':\n",
    "            if mode=='M':\n",
    "                Q=-torch.sum(gamma_fixed*Y_logprob_reduce) # (N,C) (N,C)\n",
    "                return gamma,Q,None\n",
    "            elif mode=='LL':      \n",
    "                LL=torch.sum(Y_logprob_reduce_reduce) # product of likelihood(y_i)-> (1) \n",
    "                return gamma,None,LL\n",
    "        else:\n",
    "            raise          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions.negative_binomial import NegativeBinomial\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.dirichlet import Dirichlet\n",
    "\n",
    "class Custom_Model(nn.Module):\n",
    "    def __init__(self,Y,rho,X_col=5,delta_min=2,LOWER_BOUND=1e-10,THETA_LOWER_BOUND=1e-20):\n",
    "        # Y,rho are needed for model parameter initialization\n",
    "        super(Custom_Model, self).__init__()\n",
    "        \n",
    "        #rho\n",
    "        self.masked=Masked(rho)\n",
    "         \n",
    "        #delta\n",
    "        self.delta_log_min=np.log(delta_min) #\n",
    "        self.delta_log=nn.Parameter(torch.Tensor(np.random.uniform(-2,2,size=rho.shape)),requires_grad=True) # (C,G)\n",
    "        self.delta_log.data=self.delta_log.data.clamp(min=self.delta_log_min)\n",
    "          \n",
    "        \n",
    "        #beta\n",
    "        Y_colmean=np.mean(Y,axis=0)\n",
    "        beta_init=np.hstack([((Y_colmean-Y_colmean.mean())/np.std(Y_colmean)).reshape(-1,1),\\\n",
    "                     np.zeros((Y.shape[1],X_col-1))]).T\n",
    "        self.beta=nn.Parameter(torch.Tensor(beta_init),requires_grad=True) # (P,G)\n",
    "        \n",
    "        \n",
    "    def forward(self,Y,X,s,gamma_fixed=None,mode='E'):\n",
    "        delta_log_masked=self.masked(self.delta_log) #(C,G)\n",
    "        delta=torch.exp(delta_log_masked)*self.masked.mask\n",
    "        \n",
    "        X_beta_s=X.matmul(self.beta)+torch.log(s.view(-1, 1)) #(N,P)*(P,G) + (N,1) = (N,G)\n",
    "        \n",
    "        mu_log=X_beta_s.unsqueeze(dim=1).repeat(1,delta.shape[0],1)+delta #(N,1,G)+(C,G) = (N,C,G)\n",
    "        \n",
    "        mu=torch.exp(mu_log) # (N,C,G)\n",
    "        \n",
    "        #Y_extend=Y.view(Y.shape[0],1,Y.shape[1]).repeat(1,mu_log.shape[1],1) # (N,C,G)\n",
    "        Y_extend=Y.unsqueeze(dim=1).repeat(1,mu_log.shape[1],1)\n",
    "        \n",
    "        # Poisson\n",
    "        Y_logprob=poisson_logprob(rate=mu,value=Y_extend) # (N,C,G)\n",
    "\n",
    "        Y_logprob_reduce=torch.sum(Y_logprob,axis=2)\n",
    "        \n",
    "        Y_logprob_reduce_reduce=torch.logsumexp(Y_logprob_reduce,dim=1).view(-1,1) # (N,1)\n",
    "        \n",
    "        gamma=torch.exp(Y_logprob_reduce-Y_logprob_reduce_reduce) # (N,C)\n",
    "\n",
    "        if mode=='E':\n",
    "            return gamma,None,None\n",
    "        elif mode=='M' or mode=='LL':\n",
    "            if mode=='M':\n",
    "                Q=-torch.sum(gamma_fixed*Y_logprob_reduce) # (N,C) (N,C)\n",
    "                return gamma,Q,None\n",
    "            elif mode=='LL':      \n",
    "                LL=torch.sum(Y_logprob_reduce_reduce) # product of likelihood(y_i)-> (1) \n",
    "                return gamma,None,LL\n",
    "        else:\n",
    "            raise          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions.negative_binomial import NegativeBinomial\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.dirichlet import Dirichlet\n",
    "\n",
    "\n",
    "class Poisson_Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Y, X, s, delta_log, beta, mask,to_return='LL'):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #mu=torch.exp((X.matmul(beta)+torch.log(s.view(-1, 1))).unsqueeze(dim=1).repeat(1,delta_log.shape[0],1)+torch.exp(delta_log))\n",
    "            mu=torch.exp((X.matmul(beta)+torch.log(s.view(-1, 1))).unsqueeze(dim=1).repeat(1,delta_log.shape[0],1)+torch.exp(delta_log)*mask)\n",
    "            Y_extend=Y.unsqueeze(dim=1).repeat(1,mu.shape[1],1)\n",
    "            Y_logprob=poisson_logprob(rate=mu,value=Y_extend) # (N,C,G)\n",
    "            Y_logprob_reduce=Y_logprob.sum(axis=2)\n",
    "            \n",
    "            Y_logprob_reduce_reduce=torch.logsumexp(Y_logprob_reduce,dim=1).view(-1,1)\n",
    "            \n",
    "            LL=torch.sum(Y_logprob_reduce_reduce)\n",
    "            \n",
    "            gamma=torch.exp(Y_logprob_reduce-Y_logprob_reduce_reduce)\n",
    "            A=mu-Y.unsqueeze(dim=1)        \n",
    "            \n",
    "            #gradient\n",
    "            grad_delta_log=(A*gamma.unsqueeze(dim=2)).sum(axis=0)\n",
    "            grad_beta=(X.unsqueeze(dim=2)@gamma.unsqueeze(dim=1)@A).sum(axis=0)\n",
    "        \n",
    "            ctx.save_for_backward(grad_delta_log,grad_beta)\n",
    "            \n",
    "        if to_return=='LL':\n",
    "            return LL\n",
    "        else:\n",
    "            return gamma\n",
    "            \n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \n",
    "        grad_Y = grad_X = grad_s = grad_delta_log = grad_beta = grad_mask=None\n",
    "        grad_delta_log,grad_beta = ctx.saved_tensors\n",
    "\n",
    "        return grad_Y, grad_X, grad_s, grad_delta_log, grad_beta, grad_mask\n",
    "\n",
    "class Poisson_Function_Nomask(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Y, X, s, delta_log, beta, to_return='LL'):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #mu=torch.exp((X.matmul(beta)+torch.log(s.view(-1, 1))).unsqueeze(dim=1).repeat(1,delta_log.shape[0],1)+torch.exp(delta_log))\n",
    "            mu=torch.exp((X.matmul(beta)+torch.log(s.view(-1, 1))).unsqueeze(dim=1).repeat(1,delta_log.shape[0],1)+torch.exp(delta_log))\n",
    "            Y_extend=Y.unsqueeze(dim=1).repeat(1,mu.shape[1],1)\n",
    "            Y_logprob=poisson_logprob(rate=mu,value=Y_extend) # (N,C,G)\n",
    "            Y_logprob_reduce=Y_logprob.sum(axis=2)\n",
    "            \n",
    "            Y_logprob_reduce_reduce=torch.logsumexp(Y_logprob_reduce,dim=1).view(-1,1)\n",
    "            \n",
    "            LL=torch.sum(Y_logprob_reduce_reduce)\n",
    "            \n",
    "            gamma=torch.exp(Y_logprob_reduce-Y_logprob_reduce_reduce)\n",
    "            A=mu-Y.unsqueeze(dim=1)        \n",
    "            \n",
    "            #gradient\n",
    "            grad_delta_log=(A*gamma.unsqueeze(dim=2)).sum(axis=0)\n",
    "            grad_beta=(X.unsqueeze(dim=2)@gamma.unsqueeze(dim=1)@A).sum(axis=0)\n",
    "        \n",
    "            ctx.save_for_backward(grad_delta_log,grad_beta)\n",
    "            \n",
    "        if to_return=='LL':\n",
    "            return LL\n",
    "        else:\n",
    "            return gamma\n",
    "            \n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \n",
    "        grad_Y = grad_X = grad_s = grad_delta_log = grad_beta = None\n",
    "        grad_delta_log,grad_beta = ctx.saved_tensors\n",
    "\n",
    "        return grad_Y, grad_X, grad_s, grad_delta_log, grad_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class Custom_Model_Trick_Hard(nn.Module):\n",
    "    def __init__(self,Y,rho,X_col=5,delta_min=2,LOWER_BOUND=1e-10,THETA_LOWER_BOUND=1e-20):\n",
    "        # Y,rho are needed for model parameter initialization\n",
    "        super(Custom_Model_Trick_Hard, self).__init__()\n",
    "        \n",
    "        #rho\n",
    "        self.masked=Masked(rho)\n",
    "         \n",
    "        #delta\n",
    "        self.delta_log_min=np.log(delta_min) #\n",
    "        self.delta_log=nn.Parameter(torch.Tensor(np.random.uniform(-2,2,size=rho.shape)),requires_grad=True) # (C,G)\n",
    "        self.delta_log.data=self.delta_log.data.clamp(min=self.delta_log_min)\n",
    "          \n",
    "        #beta\n",
    "        Y_colmean=np.mean(Y,axis=0)\n",
    "        beta_init=np.hstack([((Y_colmean-Y_colmean.mean())/(np.std(Y_colmean) if len(Y_colmean)>1 else 1)).reshape(-1,1),\\\n",
    "                     np.zeros((Y.shape[1],X_col-1))]).T\n",
    "        self.beta=nn.Parameter(torch.Tensor(beta_init),requires_grad=True) # (P,G)\n",
    "        \n",
    "        \n",
    "    def init_parameter(self,rho=None,Y=None,delta_min=None):\n",
    "        if delta_min is not None:\n",
    "            self.delta_log_min=np.log(delta_min) #\n",
    "            self.delta_log.data[:]=torch.Tensor(np.random.uniform(-2,2,size=self.delta_log.data.shape))\n",
    "            self.delta_log.data=self.delta_log.data.clamp(min=self.delta_log_min)\n",
    "\n",
    "        if rho is not None:\n",
    "            self.masked.mask.data=(rho==1)\n",
    "        \n",
    "        if Y is not None:\n",
    "            Y_colmean=np.mean(Y,axis=0)\n",
    "            beta_init=np.hstack([((Y_colmean-Y_colmean.mean())/(np.std(Y_colmean) if len(Y_colmean)>1 else 1)).reshape(-1,1),\\\n",
    "                         np.zeros((Y.shape[1],self.beta.shape[0]-1))]).T      \n",
    "            self.beta.data[:]=torch.Tensor(beta_init)\n",
    "        \n",
    "    def forward(self,Y,X,s):\n",
    "        delta_log_masked=self.masked(self.delta_log) #(C,G)\n",
    "        #delta=torch.exp(delta_log_masked)*self.masked.mask\n",
    "        \n",
    "        LL=Poisson_Function.apply(Y,X,s,delta_log_masked,self.beta,self.masked.mask)\n",
    "\n",
    "        return LL  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class Custom_Model_Trick_Hard_Nomask(nn.Module):\n",
    "    def __init__(self,Y,rho,X_col=5,delta_min=2,LOWER_BOUND=1e-10,THETA_LOWER_BOUND=1e-20):\n",
    "        # Y,rho are needed for model parameter initialization\n",
    "        super(Custom_Model_Trick_Hard_Nomask, self).__init__()\n",
    "         \n",
    "        #delta\n",
    "        self.delta_log_min=np.log(delta_min) #\n",
    "        self.delta_log=nn.Parameter(torch.Tensor(np.random.uniform(-2,2,size=rho.shape)),requires_grad=True) # (C,G)\n",
    "        self.delta_log.data=self.delta_log.data.clamp(min=self.delta_log_min)\n",
    "          \n",
    "        #beta\n",
    "        Y_colmean=np.mean(Y,axis=0)\n",
    "        beta_init=np.hstack([((Y_colmean-Y_colmean.mean())/np.std(Y_colmean)).reshape(-1,1),\\\n",
    "                     np.zeros((Y.shape[1],X_col-1))]).T\n",
    "        self.beta=nn.Parameter(torch.Tensor(beta_init),requires_grad=True) # (P,G)\n",
    "        \n",
    "        \n",
    "    def init_parameter(self,Y=None,delta_min=None):\n",
    "        if delta_min is not None:\n",
    "            self.delta_log_min=np.log(delta_min) #\n",
    "            self.delta_log.data[:]=torch.Tensor(np.random.uniform(-2,2,size=self.delta_log.data.shape))\n",
    "            self.delta_log.data=self.delta_log.data.clamp(min=self.delta_log_min)\n",
    "        \n",
    "        if Y is not None:\n",
    "            Y_colmean=np.mean(Y,axis=0)\n",
    "            beta_init=np.hstack([((Y_colmean-Y_colmean.mean())/np.std(Y_colmean)).reshape(-1,1),\\\n",
    "                         np.zeros((Y.shape[1],self.beta.shape[0]-1))]).T        \n",
    "            self.beta.data[:]=torch.Tensor(beta_init)\n",
    "        \n",
    "    def forward(self,Y,X,s):\n",
    "        \n",
    "        LL=Poisson_Function_Nomask.apply(Y,X,s,self.delta_log,self.beta)\n",
    "\n",
    "        return LL     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Model_Trick(nn.Module):\n",
    "    def __init__(self,Y,rho,X_col=5,delta_min=2,LOWER_BOUND=1e-10,THETA_LOWER_BOUND=1e-20):\n",
    "        # Y,rho are needed for model parameter initialization\n",
    "        super(Custom_Model_Trick, self).__init__()\n",
    "        \n",
    "        #rho\n",
    "        self.masked=Masked(rho)\n",
    "         \n",
    "        #delta\n",
    "        self.delta_log_min=np.log(delta_min) #\n",
    "        self.delta_log=nn.Parameter(torch.Tensor(np.random.uniform(-2,2,size=rho.shape)),requires_grad=True) # (C,G)\n",
    "        self.delta_log.data=self.delta_log.data.clamp(min=self.delta_log_min)\n",
    "          \n",
    "        #beta\n",
    "        Y_colmean=np.mean(Y,axis=0)\n",
    "        beta_init=np.hstack([((Y_colmean-Y_colmean.mean())/(np.std(Y_colmean) if len(Y_colmean)>1 else 1)).reshape(-1,1),\\\n",
    "                     np.zeros((Y.shape[1],X_col-1))]).T\n",
    "        self.beta=nn.Parameter(torch.Tensor(beta_init),requires_grad=True) # (P,G)\n",
    "        \n",
    "        \n",
    "    def init_parameter(self,rho=None,Y=None,delta_min=None):\n",
    "        if delta_min is not None:\n",
    "            self.delta_log_min=np.log(delta_min) #\n",
    "            self.delta_log.data[:]=torch.Tensor(np.random.uniform(-2,2,size=self.delta_log.data.shape))\n",
    "            self.delta_log.data=self.delta_log.data.clamp(min=self.delta_log_min)\n",
    "\n",
    "        if rho is not None:\n",
    "            self.masked.mask.data=(rho==1)\n",
    "        \n",
    "        if Y is not None:\n",
    "            Y_colmean=np.mean(Y,axis=0)\n",
    "            beta_init=np.hstack([((Y_colmean-Y_colmean.mean())/(np.std(Y_colmean) if len(Y_colmean)>1 else 1)).reshape(-1,1),\\\n",
    "                         np.zeros((Y.shape[1],self.beta.shape[0]-1))]).T      \n",
    "            self.beta.data[:]=torch.Tensor(beta_init)\n",
    "        \n",
    "    def forward(self,Y,X,s):\n",
    "        delta_log_masked=self.masked(self.delta_log) #(C,G)\n",
    "        #delta=torch.exp(delta_log_masked)*self.masked.mask\n",
    "        \n",
    "        mu=torch.exp((X.matmul(self.beta)+torch.log(s.view(-1, 1))).unsqueeze(dim=1).repeat(1,self.delta_log.shape[0],1)+torch.exp(delta_log_masked)*self.masked.mask)\n",
    "        Y_extend=Y.unsqueeze(dim=1).repeat(1,mu.shape[1],1)\n",
    "        Y_logprob=poisson_logprob(rate=mu,value=Y_extend) # (N,C,G)\n",
    "        Y_logprob_reduce=Y_logprob.sum(axis=2) # (N,C)\n",
    "\n",
    "        Y_logprob_reduce_reduce=torch.logsumexp(Y_logprob_reduce,dim=1).view(-1,1) # (N,1)\n",
    "\n",
    "        #gamma=torch.exp(Y_logprob_reduce-Y_logprob_reduce_reduce) # (N,C)\n",
    "        \n",
    "        LL=torch.sum(Y_logprob_reduce_reduce) # (N,C)\n",
    "        \n",
    "        return LL      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=device_cuda_list[5]#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ch6845/tools/miniconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/ch6845/tools/miniconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:23: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n",
      "1040\n",
      "1050\n",
      "1060\n",
      "1070\n",
      "1080\n",
      "1090\n",
      "1100\n",
      "1110\n",
      "1120\n",
      "1130\n",
      "1140\n",
      "1150\n",
      "1160\n",
      "1170\n",
      "1180\n",
      "1190\n",
      "1200\n",
      "1210\n",
      "1220\n",
      "1230\n",
      "1240\n",
      "1250\n",
      "1260\n",
      "1270\n",
      "1280\n",
      "1290\n",
      "1300\n",
      "1310\n",
      "1320\n",
      "1330\n",
      "1340\n",
      "1350\n",
      "1360\n",
      "1370\n",
      "1380\n",
      "1390\n",
      "1400\n",
      "1410\n",
      "1420\n",
      "1430\n",
      "1440\n",
      "1450\n",
      "1460\n",
      "1470\n",
      "1480\n",
      "1490\n",
      "1500\n",
      "1510\n",
      "1520\n",
      "1530\n",
      "1540\n",
      "1550\n",
      "1560\n",
      "1570\n",
      "1580\n",
      "1590\n",
      "1600\n",
      "1610\n",
      "1620\n",
      "1630\n",
      "1640\n",
      "1650\n",
      "1660\n",
      "1670\n",
      "1680\n",
      "1690\n",
      "1700\n",
      "1710\n",
      "1720\n",
      "1730\n",
      "1740\n",
      "1750\n",
      "1760\n",
      "1770\n",
      "1780\n",
      "1790\n",
      "1800\n",
      "1810\n",
      "1820\n",
      "1830\n",
      "1840\n",
      "1850\n",
      "1860\n",
      "1870\n",
      "1880\n",
      "1890\n",
      "1900\n",
      "1910\n",
      "1920\n",
      "1930\n",
      "1940\n",
      "1950\n",
      "1960\n",
      "1970\n",
      "1980\n",
      "1990\n",
      "2000\n",
      "2010\n",
      "2020\n",
      "2030\n",
      "2040\n",
      "2050\n",
      "2060\n",
      "2070\n",
      "2080\n",
      "2090\n",
      "2100\n",
      "2110\n",
      "2120\n",
      "2130\n",
      "2140\n",
      "2150\n",
      "2160\n",
      "2170\n",
      "2180\n",
      "2190\n",
      "2200\n",
      "2210\n",
      "2220\n",
      "2230\n",
      "2240\n",
      "2250\n",
      "2260\n",
      "2270\n",
      "2280\n",
      "2290\n",
      "2300\n",
      "2310\n",
      "2320\n",
      "2330\n",
      "2340\n",
      "2350\n",
      "2360\n",
      "2370\n",
      "2380\n",
      "2390\n",
      "2400\n",
      "2410\n",
      "2420\n",
      "2430\n",
      "2440\n",
      "2450\n",
      "2460\n",
      "2470\n",
      "2480\n",
      "2490\n",
      "2500\n",
      "2510\n",
      "2520\n",
      "2530\n",
      "2540\n",
      "2550\n",
      "2560\n",
      "2570\n",
      "2580\n",
      "2590\n",
      "2600\n",
      "2610\n",
      "2620\n",
      "2630\n",
      "2640\n",
      "2650\n",
      "2660\n",
      "2670\n",
      "2680\n",
      "2690\n",
      "2700\n",
      "2710\n",
      "2720\n",
      "2730\n",
      "2740\n",
      "2750\n",
      "2760\n",
      "2770\n",
      "2780\n",
      "2790\n",
      "2800\n",
      "2810\n",
      "2820\n",
      "2830\n",
      "2840\n",
      "2850\n",
      "2860\n",
      "2870\n",
      "2880\n",
      "2890\n",
      "2900\n",
      "2910\n",
      "2920\n",
      "2930\n",
      "2940\n",
      "2950\n",
      "2960\n",
      "2970\n",
      "2980\n",
      "2990\n",
      "3000\n",
      "3010\n",
      "3020\n",
      "3030\n",
      "3040\n",
      "3050\n",
      "3060\n",
      "3070\n",
      "3080\n",
      "3090\n",
      "3100\n",
      "3110\n",
      "3120\n",
      "3130\n",
      "3140\n",
      "3150\n",
      "3160\n",
      "3170\n",
      "3180\n",
      "3190\n",
      "3200\n",
      "3210\n",
      "3220\n",
      "3230\n",
      "3240\n",
      "3250\n",
      "3260\n",
      "3270\n",
      "3280\n",
      "3290\n",
      "3300\n",
      "3310\n",
      "3320\n",
      "3330\n",
      "3340\n",
      "3350\n",
      "3360\n",
      "3370\n",
      "3380\n",
      "3390\n",
      "3400\n",
      "3410\n",
      "3420\n",
      "3430\n",
      "3440\n",
      "3450\n",
      "3460\n",
      "3470\n",
      "3480\n",
      "3490\n",
      "3500\n",
      "3510\n",
      "3520\n",
      "3530\n",
      "3540\n",
      "3550\n",
      "3560\n",
      "3570\n",
      "3580\n",
      "3590\n",
      "3600\n",
      "3610\n",
      "3620\n",
      "3630\n",
      "3640\n",
      "3650\n",
      "3660\n",
      "3670\n",
      "3680\n",
      "3690\n",
      "3700\n",
      "3710\n",
      "3720\n",
      "3730\n",
      "3740\n",
      "3750\n",
      "3760\n",
      "3770\n",
      "3780\n",
      "3790\n",
      "3800\n",
      "3810\n",
      "3820\n",
      "3830\n",
      "3840\n",
      "3850\n",
      "3860\n",
      "3870\n",
      "3880\n",
      "3890\n",
      "3900\n",
      "3910\n",
      "3920\n",
      "3930\n",
      "3940\n",
      "3950\n",
      "3960\n",
      "3970\n",
      "3980\n",
      "3990\n",
      "4000\n",
      "4010\n",
      "4020\n",
      "4030\n",
      "4040\n",
      "4050\n",
      "4060\n",
      "4070\n",
      "4080\n",
      "4090\n",
      "4100\n",
      "4110\n",
      "4120\n",
      "4130\n",
      "4140\n",
      "4150\n",
      "4160\n",
      "4170\n",
      "4180\n",
      "4190\n",
      "4200\n",
      "4210\n",
      "4220\n",
      "4230\n",
      "4240\n",
      "4250\n",
      "4260\n",
      "4270\n",
      "4280\n",
      "4290\n",
      "4300\n",
      "4310\n",
      "4320\n",
      "4330\n",
      "4340\n",
      "4350\n",
      "4360\n",
      "4370\n",
      "4380\n",
      "4390\n",
      "4400\n",
      "4410\n",
      "4420\n",
      "4430\n",
      "4440\n",
      "4450\n",
      "4460\n",
      "4470\n",
      "4480\n",
      "4490\n",
      "4500\n",
      "4510\n",
      "4520\n",
      "4530\n",
      "4540\n",
      "4550\n",
      "4560\n",
      "4570\n",
      "4580\n",
      "4590\n",
      "4600\n",
      "4610\n",
      "4620\n",
      "4630\n",
      "4640\n",
      "4650\n",
      "4660\n",
      "4670\n",
      "4680\n",
      "4690\n",
      "4700\n",
      "4710\n",
      "4720\n",
      "4730\n",
      "4740\n",
      "4750\n",
      "4760\n",
      "4770\n",
      "4780\n",
      "4790\n",
      "4800\n",
      "4810\n",
      "4820\n",
      "4830\n",
      "4840\n",
      "4850\n",
      "4860\n",
      "4870\n",
      "4880\n",
      "4890\n"
     ]
    }
   ],
   "source": [
    "LL_list_list=[]\n",
    "for c in [20]:\n",
    "    print(c)\n",
    "    LL_list=[]\n",
    "    for iter_idx,exp_data_idx in enumerate(np.arange(exp_data.shape[0])):\n",
    "        if iter_idx%10==0:\n",
    "            print(iter_idx)\n",
    "        Y=exp_data[exp_data_idx:exp_data_idx+1].transpose().astype(float)\n",
    "\n",
    "        cell_dataset=Cell_Dataset(Y,X,s)\n",
    "        cell_dataloader=DataLoader(dataset=cell_dataset,shuffle=False,batch_size=BATCH_SIZE,num_workers=NUM_WORKERS)    \n",
    "\n",
    "        if iter_idx==0:\n",
    "            model=Custom_Model_Trick_Hard(Y,np.ones((1,c)),X_col=X.shape[1],delta_min=0,LOWER_BOUND=LOWER_BOUND,THETA_LOWER_BOUND=THETA_LOWER_BOUND).to(device)\n",
    "        else:\n",
    "            model.init_parameter(Y=Y,delta_min=0)\n",
    "        optimizer = optim.Adam(model.parameters(),lr=LR)#,betas=(0.92, 0.999))\n",
    "        #LL=run_EM_trick(model,optimizer,LL_diff_tolerance=LL_diff_tolerance,Q_diff_tolerance=Q_diff_tolerance)[1].detach().cpu().numpy()\n",
    "        LL=run_EM_trick(model,optimizer,LL_diff_tolerance=LL_diff_tolerance,Q_diff_tolerance=Q_diff_tolerance,verbose=False)[1].detach().cpu().numpy()\n",
    "        LL_list.append(LL)\n",
    "    LL_list_list.append(LL_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ch6845/tools/miniconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pd.Series(LL_list).to_csv('false_LL_{}.csv'.format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
