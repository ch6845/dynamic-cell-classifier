{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips for using colab Environment\n",
    "\n",
    "\n",
    "## Click the following URL to load this notebook on Colab environment\n",
    "\n",
    "* https://colab.research.google.com/github/ch6845/dynamic-cell-classifier/blob/master/3_DCC.ipynb\n",
    "\n",
    "* You can use free GPUs from Google on Colab. (For more information, see https://colab.research.google.com/notebooks/welcome.ipynb)\n",
    "\n",
    "## Enabling and testing the GPU\n",
    "* Navigate to Editâ†’Notebook Settings\n",
    "* select GPU from the Hardware Accelerator drop-down\n",
    "\n",
    "## Load data\n",
    "* Upload your data or, mount a storage to Colab virtual environment.\n",
    "(For more information, see https://colab.research.google.com/notebooks/io.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.io import mmread\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ch6845/tools/miniconda3/envs/pytorch/lib/python3.6/os.py'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## expression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data=mmread('data/HumanLiver_extract/HumanLiver.data.counts.mm').toarray().astype(float)\n",
    "with open('data/HumanLiver_extract/HumanLiver.data.col','r') as f: exp_data_col=[i.strip().strip('\"') for i in f.read().split()]\n",
    "with open('data/HumanLiver_extract/HumanLiver.data.row','r') as f: exp_data_row=[i.strip().strip('\"') for i in f.read().split()]\n",
    "assert exp_data.shape==(len(exp_data_row),len(exp_data_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(set(exp_data_row))==len(exp_data_row)\n",
    "assert len(set(exp_data_col))==len(exp_data_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]), (20007, 8444))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_data,exp_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['RP11-34P13.7', 'FO538757.2', 'AP006222.2', 'RP4-669L17.10', 'RP5-857K21.4'],\n",
       " ['P1TLH_AAACCTGAGCAGCCTC_1',\n",
       "  'P1TLH_AAACCTGTCCTCATTA_1',\n",
       "  'P1TLH_AAACCTGTCTAAGCCA_1',\n",
       "  'P1TLH_AAACGGGAGTAGGCCA_1',\n",
       "  'P1TLH_AAACGGGGTTCGGGCT_1'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_data_row[:5],exp_data_col[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cluster info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_counts</th>\n",
       "      <th>total_features</th>\n",
       "      <th>orig.ident</th>\n",
       "      <th>res.0.8</th>\n",
       "      <th>S.Score</th>\n",
       "      <th>G2M.Score</th>\n",
       "      <th>Phase</th>\n",
       "      <th>tSNE_1</th>\n",
       "      <th>tSNE_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P1TLH_AAACCTGAGCAGCCTC_1</th>\n",
       "      <td>2943</td>\n",
       "      <td>1427</td>\n",
       "      <td>P1TLH</td>\n",
       "      <td>12</td>\n",
       "      <td>0.046089</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>S</td>\n",
       "      <td>12.331038</td>\n",
       "      <td>4.044869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1TLH_AAACCTGTCCTCATTA_1</th>\n",
       "      <td>10897</td>\n",
       "      <td>2522</td>\n",
       "      <td>P1TLH</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>G2M</td>\n",
       "      <td>-10.186342</td>\n",
       "      <td>-50.465799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1TLH_AAACCTGTCTAAGCCA_1</th>\n",
       "      <td>1914</td>\n",
       "      <td>1018</td>\n",
       "      <td>P1TLH</td>\n",
       "      <td>12</td>\n",
       "      <td>0.012811</td>\n",
       "      <td>-0.056561</td>\n",
       "      <td>S</td>\n",
       "      <td>15.618844</td>\n",
       "      <td>6.213892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1TLH_AAACGGGAGTAGGCCA_1</th>\n",
       "      <td>5574</td>\n",
       "      <td>1798</td>\n",
       "      <td>P1TLH</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.011324</td>\n",
       "      <td>-0.047102</td>\n",
       "      <td>G1</td>\n",
       "      <td>4.710776</td>\n",
       "      <td>10.386157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P1TLH_AAACGGGGTTCGGGCT_1</th>\n",
       "      <td>3700</td>\n",
       "      <td>1417</td>\n",
       "      <td>P1TLH</td>\n",
       "      <td>2</td>\n",
       "      <td>0.057467</td>\n",
       "      <td>-0.003861</td>\n",
       "      <td>S</td>\n",
       "      <td>-14.954216</td>\n",
       "      <td>19.197842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          total_counts  total_features orig.ident  res.0.8  \\\n",
       "P1TLH_AAACCTGAGCAGCCTC_1          2943            1427      P1TLH       12   \n",
       "P1TLH_AAACCTGTCCTCATTA_1         10897            2522      P1TLH       17   \n",
       "P1TLH_AAACCTGTCTAAGCCA_1          1914            1018      P1TLH       12   \n",
       "P1TLH_AAACGGGAGTAGGCCA_1          5574            1798      P1TLH       10   \n",
       "P1TLH_AAACGGGGTTCGGGCT_1          3700            1417      P1TLH        2   \n",
       "\n",
       "                           S.Score  G2M.Score Phase     tSNE_1     tSNE_2  \n",
       "P1TLH_AAACCTGAGCAGCCTC_1  0.046089   0.000349     S  12.331038   4.044869  \n",
       "P1TLH_AAACCTGTCCTCATTA_1 -0.000357   0.009434   G2M -10.186342 -50.465799  \n",
       "P1TLH_AAACCTGTCTAAGCCA_1  0.012811  -0.056561     S  15.618844   6.213892  \n",
       "P1TLH_AAACGGGAGTAGGCCA_1 -0.011324  -0.047102    G1   4.710776  10.386157  \n",
       "P1TLH_AAACGGGGTTCGGGCT_1  0.057467  -0.003861     S -14.954216  19.197842  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_data_meta=pd.read_csv('data/HumanLiver_extract/HumanLiver.metadata.tsv',sep='\\t')\n",
    "exp_data_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`clusterid_to_clustername` is used to convert integers in `res.0.8` to cell-type name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterid_to_clustername=pd.read_csv('data/HumanLiver_extract/HumanLiver.clusterid_to_clustername.tsv',sep='\\t',header=None,index_col=0)\n",
    "clusterid_to_clustername[1]=clusterid_to_clustername[1].str.replace(' ','_')\n",
    "len(clusterid_to_clustername[1].unique()),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panglao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>official gene symbol</th>\n",
       "      <th>cell type</th>\n",
       "      <th>nicknames</th>\n",
       "      <th>ubiquitousness index</th>\n",
       "      <th>product description</th>\n",
       "      <th>gene type</th>\n",
       "      <th>canonical marker</th>\n",
       "      <th>germ layer</th>\n",
       "      <th>organ</th>\n",
       "      <th>sensitivity_human</th>\n",
       "      <th>sensitivity_mouse</th>\n",
       "      <th>specificity_human</th>\n",
       "      <th>specificity_mouse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CTRB1</td>\n",
       "      <td>Acinar cells</td>\n",
       "      <td>CTRB</td>\n",
       "      <td>0.017</td>\n",
       "      <td>chymotrypsinogen B1</td>\n",
       "      <td>protein-coding gene</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Endoderm</td>\n",
       "      <td>Pancreas</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.015920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KLK1</td>\n",
       "      <td>Acinar cells</td>\n",
       "      <td>Klk6</td>\n",
       "      <td>0.013</td>\n",
       "      <td>kallikrein 1</td>\n",
       "      <td>protein-coding gene</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Endoderm</td>\n",
       "      <td>Pancreas</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.012826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RBPJL</td>\n",
       "      <td>Acinar cells</td>\n",
       "      <td>RBP-L|SUHL|RBPSUHL</td>\n",
       "      <td>0.001</td>\n",
       "      <td>recombination signal binding protein for immun...</td>\n",
       "      <td>protein-coding gene</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Endoderm</td>\n",
       "      <td>Pancreas</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PTF1A</td>\n",
       "      <td>Acinar cells</td>\n",
       "      <td>PTF1-p48|bHLHa29</td>\n",
       "      <td>0.001</td>\n",
       "      <td>pancreas associated transcription factor 1a</td>\n",
       "      <td>protein-coding gene</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Endoderm</td>\n",
       "      <td>Pancreas</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.000773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CELA3A</td>\n",
       "      <td>Acinar cells</td>\n",
       "      <td>ELA3|ELA3A</td>\n",
       "      <td>0.001</td>\n",
       "      <td>chymotrypsin like elastase family member 3A</td>\n",
       "      <td>protein-coding gene</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Endoderm</td>\n",
       "      <td>Pancreas</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  official gene symbol     cell type           nicknames  \\\n",
       "0                CTRB1  Acinar cells                CTRB   \n",
       "1                 KLK1  Acinar cells                Klk6   \n",
       "2                RBPJL  Acinar cells  RBP-L|SUHL|RBPSUHL   \n",
       "3                PTF1A  Acinar cells    PTF1-p48|bHLHa29   \n",
       "5               CELA3A  Acinar cells          ELA3|ELA3A   \n",
       "\n",
       "   ubiquitousness index                                product description  \\\n",
       "0                 0.017                                chymotrypsinogen B1   \n",
       "1                 0.013                                       kallikrein 1   \n",
       "2                 0.001  recombination signal binding protein for immun...   \n",
       "3                 0.001        pancreas associated transcription factor 1a   \n",
       "5                 0.001        chymotrypsin like elastase family member 3A   \n",
       "\n",
       "             gene type  canonical marker germ layer     organ  \\\n",
       "0  protein-coding gene               1.0   Endoderm  Pancreas   \n",
       "1  protein-coding gene               1.0   Endoderm  Pancreas   \n",
       "2  protein-coding gene               1.0   Endoderm  Pancreas   \n",
       "3  protein-coding gene               1.0   Endoderm  Pancreas   \n",
       "5  protein-coding gene               1.0   Endoderm  Pancreas   \n",
       "\n",
       "   sensitivity_human  sensitivity_mouse  specificity_human  specificity_mouse  \n",
       "0           1.000000           0.957143           0.000629           0.015920  \n",
       "1           0.833333           0.314286           0.005031           0.012826  \n",
       "2           0.000000           0.000000           0.000000           0.000000  \n",
       "3           0.000000           0.157143           0.000629           0.000773  \n",
       "5           0.833333           0.128571           0.000000           0.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markers_db=pd.read_csv(\"data/PanglaoDB_markers_27_Mar_2020.tsv.gz\",sep='\\t')\n",
    "markers_db=markers_db[markers_db['species'].str.contains('Hs')].drop(columns='species')\n",
    "markers_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#markers_db[(markers_db['official gene symbol']=='FXDY2') |(markers_db['nicknames'].str.contains('FXDY2'))],'FXDY2' in exp_data_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marker info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclustername_to_markers={'Hepatocytes':['ALB','HAMP','ARG1','PCK1','AFP','BCHE'],\\n'LSECs':['CALCRL','FCGR2B','VWF'],\\n'Cholangiocytes':['KRT19','EPCAM','FXYD2','CLDN4','CLDN10','SOX9','MMP7','CXCL1','CFTR','TFF2','KRT7','CD24'],\\n'Hepatic_Stellate_Cells':['ACTA2','COL1A1','TAGLN','COL1A2','COL3A1','SPARC','RBP1','DCN','MYL9'],\\n'Macrophages':['CD68','MARCO'],\\n'ab_T_cells':['CD2','CD3D','TRAC','IL32','CD3E'],\\n'gd_T_cells':['NKG7','FCGR3A','HOPX','GNLY'],\\n'NK_cells':['GZMK','KLRF1','CCL3','CMC1'],\\n'Plasma_cells':['CD27','IGHG1'],\\n'Mature_B_cells':['MS4A1','LTB','CD52','IGHD'],\\n'Erythroid_cells':['HBB','SLC25A37','CA1','ALAS2'],\\n'other':[]    \\n}\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# New one\n",
    "\n",
    "clustername_to_markers={'Cholangiocytes':['KRT19','EPCAM','FXYD2','CLDN4','CLDN10','SOX9','MMP7','CXCL1','CFTR','TFF2','KRT7','CD24'],\n",
    "'Mature_B_cells':['MS4A1','IGHD','CD79A','PTPRC','IGKC','CD19'],\n",
    "'Hepatocytes':['ALB','HAMP','ARG1','PCK1','AFP','BCHE'],\n",
    "'LSECs':['CALCRL','VWF','PECAM1','CLEC14A','EMCN'],\n",
    "'Hepatic_Stellate_Cells':['ACTA2','COL1A1','TAGLN','COL1A2','COL3A1','SPARC','RBP1','DCN','MYL9'],\n",
    "'Macrophages':['CD68','MARCO','FCGR3A','LYZ','PTPRC','AIF1'],\n",
    "'ab_T_cells':['CD2','CD3D','TRAC','IL32','CD3E','PTPRC'],\n",
    "'gd_T_cells':['NKG7','FCGR3A','HOPX','GNLY','CMC1','KLRF1','CCL3','PTPRC'],\n",
    "'NK_cells':['GZMK','KLRF1','CCL3','CMC1','NKG7','PTPRC'],\n",
    "'Plasma_cells':['CD27','IGHG1','IGHA1','IGHM','CD79A','PTPRC','IGKC'],\n",
    "'Erythroid_cells':['HBB','SLC25A37','CA1','ALAS2'],\n",
    "'other':[]                           \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Old one\n",
    "\"\"\"\n",
    "clustername_to_markers={'Hepatocytes':['ALB','HAMP','ARG1','PCK1','AFP','BCHE'],\n",
    "'LSECs':['CALCRL','FCGR2B','VWF'],\n",
    "'Cholangiocytes':['KRT19','EPCAM','FXYD2','CLDN4','CLDN10','SOX9','MMP7','CXCL1','CFTR','TFF2','KRT7','CD24'],\n",
    "'Hepatic_Stellate_Cells':['ACTA2','COL1A1','TAGLN','COL1A2','COL3A1','SPARC','RBP1','DCN','MYL9'],\n",
    "'Macrophages':['CD68','MARCO'],\n",
    "'ab_T_cells':['CD2','CD3D','TRAC','IL32','CD3E'],\n",
    "'gd_T_cells':['NKG7','FCGR3A','HOPX','GNLY'],\n",
    "'NK_cells':['GZMK','KLRF1','CCL3','CMC1'],\n",
    "'Plasma_cells':['CD27','IGHG1'],\n",
    "'Mature_B_cells':['MS4A1','LTB','CD52','IGHD'],\n",
    "'Erythroid_cells':['HBB','SLC25A37','CA1','ALAS2'],\n",
    "'other':[]    \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "#for key,value in clustername2markers_new.items():\n",
    "#    for i in value:\n",
    "#        print(markers_db[(markers_db['official gene symbol']==i)].shape)\n",
    "#markers_db[(markers_db['official gene symbol']=='CD32B') |(markers_db['nicknames'].str.contains('CD32B'))]\n",
    "#clustername_to_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustername_unique=list(clustername_to_markers.keys())\n",
    "exp_data_meta_clusterid_clusteridunique=clusterid_to_clustername.loc[exp_data_meta['res.0.8'].values][1].apply(lambda x: clustername_unique.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['ACTA2', 'AFP', 'AIF1', 'ALAS2', 'ALB', 'ARG1', 'BCHE', 'CA1',\n",
       "        'CALCRL', 'CCL3', 'CD19', 'CD2', 'CD24', 'CD27', 'CD3D', 'CD3E',\n",
       "        'CD68', 'CD79A', 'CFTR', 'CLDN10', 'CLDN4', 'CLEC14A', 'CMC1',\n",
       "        'COL1A1', 'COL1A2', 'COL3A1', 'CXCL1', 'DCN', 'EMCN', 'EPCAM',\n",
       "        'FCGR3A', 'FXYD2', 'GNLY', 'GZMK', 'HAMP', 'HBB', 'HOPX', 'IGHA1',\n",
       "        'IGHD', 'IGHG1', 'IGHM', 'IGKC', 'IL32', 'KLRF1', 'KRT19', 'KRT7',\n",
       "        'LYZ', 'MARCO', 'MMP7', 'MS4A1', 'MYL9', 'NKG7', 'PCK1', 'PECAM1',\n",
       "        'PTPRC', 'RBP1', 'SLC25A37', 'SOX9', 'SPARC', 'TAGLN', 'TFF2',\n",
       "        'TRAC', 'VWF'], dtype='<U8'), 63)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marker_unique=np.unique([j for i in list(clustername_to_markers.values()) for j in i])\n",
    "marker_unique_exp_data_idx=[exp_data_row.index(marker) for marker in marker_unique]\n",
    "marker_unique,len(marker_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_true=np.array([np.sum(exp_data_meta_clusterid_clusteridunique==i) for i in sorted(np.unique(exp_data_meta_clusterid_clusteridunique))])/exp_data_meta_clusterid_clusteridunique.shape[0]\n",
    "M_true=np.array([np.mean(exp_data[marker_unique_exp_data_idx,:][:,exp_data_meta_clusterid_clusteridunique==i],axis=1) for i in sorted(np.unique(exp_data_meta_clusterid_clusteridunique))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8444,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_size_factor=pd.read_csv('data/analysis/size_factor_cluster.tsv',sep='\\t',header=None)[0].values.astype(float)#.reshape(-1,1)\n",
    "#cell_size_factor=np.ones_like(cell_size_factor)\n",
    "cell_size_factor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8444, 63)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=exp_data[marker_unique_exp_data_idx].transpose().astype(float)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 63)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marker_onehot=np.array([np.sum(np.eye(len(marker_unique))[[marker_unique.tolist().index(marker) for marker in value]],axis=0) for key,value in clustername_to_markers.items()])\n",
    "marker_onehot.shape\n",
    "#marker_onehot.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8444, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_data_col_patient=pd.Series(exp_data_col).str.slice(start=1,stop=2).astype(int).values\n",
    "x_data_covariate=np.eye(len(np.unique(exp_data_col_patient)))[exp_data_col_patient-1]\n",
    "x_data_intercept=np.array([np.ones(Y.shape[0])]).transpose()\n",
    "x_data_null=np.concatenate([x_data_intercept,x_data_covariate[:,:]],axis=1)\n",
    "x_data_null.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch porting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8444, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Asuume that the following variables are initialized\n",
    "\n",
    "# Input\n",
    "Y\n",
    "s=cell_size_factor\n",
    "#X=x_data_null.copy()[:,[0, 1,2,3,4]]\n",
    "#X=x_data_null.copy()[:,[0, 1,2]]\n",
    "#X=x_data_null.copy()[:,[0,1,2,3]]\n",
    "X=x_data_null.copy()[:,[0,1,2]]\n",
    "# 234 x\n",
    "# 34 x\n",
    "# 123 x\n",
    "rho=marker_onehot\n",
    "\n",
    "delta_min=2\n",
    "B=10\n",
    "LR=1e-1\n",
    "\n",
    "# Optional\n",
    "EM_ITER_MAX=20\n",
    "M_ITER_MAX=10000\n",
    "\n",
    "BATCH_SIZE=Y.shape[0]\n",
    "NUM_WORKERS=0\n",
    "\n",
    "LOWER_BOUND=1e-10\n",
    "THETA_LOWER_BOUND=1e-20\n",
    "\n",
    "\n",
    "Q_diff_tolerance=1e-4\n",
    "LL_diff_tolerance=1e-4\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_cpu=torch.device(\"cpu\")\n",
    "device_cuda_list=[torch.device(\"cuda:{}\".format(i)) for i in range(6)][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from basic_tools import Cell_Dataset,Masked\n",
    "class Masked_Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, mask):\n",
    "        #print('aaaa')\n",
    "        output=input\n",
    "        ctx.save_for_backward(input, mask)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, mask = ctx.saved_tensors\n",
    "        grad_input = grad_mask = None\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mul(mask)\n",
    "\n",
    "        return grad_input, grad_mask\n",
    "    \n",
    "class Masked(nn.Module):    \n",
    "    def __init__(self, mask):    \n",
    "        super(Masked, self).__init__()\n",
    "        \n",
    "        self.mask = nn.Parameter(torch.Tensor(mask)==1, requires_grad=False)    \n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        return Masked_Function.apply(input, self.mask)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'mask={}'.format(self.mask.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class NB_logprob(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NB_logprob,self).__init__()\n",
    "        \n",
    "    def forward(self,total_count,probs,value):\n",
    "        \n",
    "        #eps = torch.finfo(probs.dtype).eps\n",
    "        #probs_clamped=probs.clamp(min=eps, max=1 - eps)        \n",
    "        probs_clamped=probs\n",
    "        logits=torch.log(probs_clamped) - torch.log1p(-probs_clamped)\n",
    "        #logits=torch.log(probs_clamped)\n",
    "        \n",
    "        log_unnormalized_prob = (total_count * F.logsigmoid(-logits) +\n",
    "                                 value * F.logsigmoid(logits))\n",
    "        log_normalization = (-torch.lgamma(total_count + value) + torch.lgamma(1. + value) +\n",
    "                             torch.lgamma(total_count))\n",
    "\n",
    "        return log_unnormalized_prob - log_normalization\n",
    "    \n",
    "class Normal_logprob(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Normal_logprob,self).__init__()\n",
    "        \n",
    "    def forward(self,loc,scale,value):\n",
    "    \n",
    "        var = (scale ** 2)\n",
    "        log_scale = torch.log(scale)\n",
    "        \n",
    "        \n",
    "        #print(loc.shape,scale.shape,value.shape)\n",
    "        return -((value - loc) ** 2) / (2 * var) - log_scale - math.log(math.sqrt(2 * math.pi))    \n",
    "        #return log_scale\n",
    "        \n",
    "class Poisson_logprob(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Poisson_logprob,self).__init__()\n",
    "        \n",
    "    def forward(self,rate,value):\n",
    "        #rate=rate.clamp(min=1e-3)+(-1)/rate.clamp(max=-1e-5)\n",
    "        \n",
    "        return (rate.log() * value) - rate - (value + 1).lgamma()\n",
    "    \n",
    "class Dirichlet_logprob(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dirichlet_logprob,self).__init__()\n",
    "        \n",
    "    def forward(self,concentration,value):\n",
    "        \n",
    "        return ((torch.log(value) * (concentration - 1.0)).sum(-1) +\n",
    "                torch.lgamma(concentration.sum(-1)) -\n",
    "                torch.lgamma(concentration).sum(-1))\n",
    "    \n",
    "        \n",
    "NB_logprob=NB_logprob()        \n",
    "normal_logprob=Normal_logprob()\n",
    "poisson_logprob=Poisson_logprob()\n",
    "dirichlet_logprob=Dirichlet_logprob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_random=np.random.uniform(-2,2,size=rho.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_dataset=Cell_Dataset(Y,X,s)\n",
    "cell_dataloader=DataLoader(dataset=cell_dataset,shuffle=False,batch_size=BATCH_SIZE,num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions.negative_binomial import NegativeBinomial\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.dirichlet import Dirichlet\n",
    "\n",
    "class Custom_Model(nn.Module):\n",
    "    def __init__(self,Y,rho,X_col=5,delta_min=2,LOWER_BOUND=1e-10,THETA_LOWER_BOUND=1e-20):\n",
    "        # Y,rho are needed for model parameter initialization\n",
    "        super(Custom_Model, self).__init__()\n",
    "         \n",
    "        #delta\n",
    "        self.delta_log_min=np.log(delta_min) #\n",
    "        self.delta_log=nn.Parameter(torch.Tensor(delta_random),requires_grad=True) # (C,G)\n",
    "        \n",
    "        #beta\n",
    "        Y_colmean=np.mean(Y,axis=0)\n",
    "        beta_init=np.hstack([((Y_colmean-Y_colmean.mean())/np.std(Y_colmean)).reshape(-1,1),\\\n",
    "                     np.zeros((Y.shape[1],X_col-1))]).T\n",
    "        self.beta=nn.Parameter(torch.Tensor(beta_init),requires_grad=True) # (P,G)\n",
    "        \n",
    "    def forward(self,Y,X,s,gamma_fixed=None,mode='E'):\n",
    "        delta=torch.exp(self.delta_log)\n",
    "        \n",
    "        X_beta_s=X.matmul(self.beta)+torch.log(s.view(-1, 1)) #(N,P)*(P,G) + (N,1) = (N,G)\n",
    "        \n",
    "        mu_log=X_beta_s.unsqueeze(dim=1).repeat(1,delta.shape[0],1)+delta #(N,1,G)+(C,G) = (N,C,G)\n",
    "        \n",
    "        mu=torch.exp(mu_log) # (N,C,G)\n",
    "        \n",
    "        #Y_extend=Y.view(Y.shape[0],1,Y.shape[1]).repeat(1,mu_log.shape[1],1) # (N,C,G)\n",
    "        Y_extend=Y.unsqueeze(dim=1).repeat(1,mu_log.shape[1],1)\n",
    "        \n",
    "        # Poisson\n",
    "        Y_logprob=poisson_logprob(rate=mu,value=Y_extend) # (N,C,G)\n",
    "        \n",
    "        Y_logprob_reduce=torch.sum(Y_logprob,axis=2)\n",
    "        \n",
    "        Y_logprob_reduce_reduce=torch.logsumexp(Y_logprob_reduce,dim=1).view(-1,1) # (N,1)\n",
    "        \n",
    "        gamma=torch.exp(Y_logprob_reduce-Y_logprob_reduce_reduce) # (N,C)\n",
    "\n",
    "        if mode=='E':\n",
    "            return gamma,None,None\n",
    "        elif mode=='M' or mode=='LL':\n",
    "            if mode=='M':\n",
    "                Q=-torch.sum(gamma_fixed*Y_logprob_reduce) # (N,C) (N,C)\n",
    "                return gamma,Q,None\n",
    "            elif mode=='LL':      \n",
    "                LL=torch.sum(Y_logprob_reduce_reduce) # product of likelihood(y_i)-> (1) \n",
    "                return gamma,None,LL\n",
    "        else:\n",
    "            raise          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "from numba import prange, njit\n",
    "import scipy.special as sc\n",
    "\n",
    "@njit\n",
    "def logsumexp(vec):\n",
    "    val = vec[-1]\n",
    "    for i in range(vec.shape[0]-1):\n",
    "        val = np.logaddexp(val, vec[i])\n",
    "    return val\n",
    "\n",
    "\n",
    "@njit\n",
    "def log_poisson_pdf(n, mu):\n",
    "    return sc.xlogy(n, mu) - sc.gammaln(n+1) - mu\n",
    "\n",
    "@njit\n",
    "def get_log_poisson_prod(ns, mus):\n",
    "    val = 0\n",
    "    for g in range(ns.shape[0]):\n",
    "        val += log_poisson_pdf(ns[g], mus[g])\n",
    "    return val\n",
    "\n",
    "@njit(parallel=True)\n",
    "def MuProd(N, X, Beta, Gamma, S):\n",
    "    n, g, p, t = N.shape[0], Beta.shape[0], Beta.shape[1], Gamma.shape[1]\n",
    "    Mu = np.zeros(shape=(n, g, t))\n",
    "    Xbeta = X @ Beta.T\n",
    "    for i in prange(N.shape[0]):\n",
    "        Mu[i] = np.exp(Xbeta[i].reshape(g,1) + Gamma) * S[i]\n",
    "\n",
    "    Prod = np.zeros(shape=(n, t))\n",
    "    for i in prange(n):\n",
    "        for j in range(t):\n",
    "            Prod[i,j] = get_log_poisson_prod(N[i], Mu[i,:,j])\n",
    "\n",
    "    return Mu, Prod\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def target_grad(N, X, Beta, Gamma, S, Mark=np.zeros(0)):\n",
    "    n_sample, g, p, t = N.shape[0], Beta.shape[0], Beta.shape[1], Gamma.shape[1]\n",
    "    # call Mu and Prod\n",
    "    Mu, Prod = MuProd(N, X, Beta, Gamma, S)\n",
    "    \n",
    "    # compute target and grad\n",
    "    target = 0\n",
    "    grad_Beta = np.zeros(shape=Beta.shape)\n",
    "    grad_Gamma = np.zeros(shape=Gamma.shape)\n",
    "    for i in prange(n_sample):\n",
    "        # target\n",
    "        target += logsumexp(Prod[i])\n",
    "\n",
    "        # gradient\n",
    "        A = -Mu[i] + N[i].reshape(g,1)\n",
    "        B = np.exp(Prod[i] - logsumexp(Prod[i]))\n",
    "\n",
    "        grad_Beta += (A @ B.reshape((t,1))) @ X[i].reshape((1,p))\n",
    "        grad_Gamma += np.dot(A, np.diag(B))\n",
    "    \n",
    "    # do not update non-marker gene if Mark provided\n",
    "    #if Mark.shape[0] > 0:\n",
    "        #grad_Gamma = np.multiply(grad_Gamma, Mark)\n",
    "    \n",
    "    return -target/n_sample, -grad_Beta/n_sample, -grad_Gamma/n_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@njit(parallel=True)\n",
    "def target_grad_(N, X, Beta, Gamma, S, Mu,Prod,Mark=np.zeros(0)):\n",
    "    n_sample, g, p, t = N.shape[0], Beta.shape[0], Beta.shape[1], Gamma.shape[1]\n",
    "    # call Mu and Prod\n",
    "    \n",
    "    # compute target and grad\n",
    "    target = 0\n",
    "    grad_Beta = np.zeros(shape=Beta.shape)\n",
    "    grad_Gamma = np.zeros(shape=Gamma.shape)\n",
    "    for i in prange(n_sample):\n",
    "        # target\n",
    "        target += logsumexp(Prod[i])\n",
    "\n",
    "        # gradient\n",
    "        A = -Mu[i] + N[i].reshape(g,1)\n",
    "        B = np.exp(Prod[i] - logsumexp(Prod[i]))\n",
    "\n",
    "        grad_Beta += (A @ B.reshape((t,1))) @ X[i].reshape((1,p))\n",
    "        grad_Gamma += np.dot(A, np.diag(B))\n",
    "    \n",
    "    # do not update non-marker gene if Mark provided\n",
    "    #if Mark.shape[0] > 0:\n",
    "        #grad_Gamma = np.multiply(grad_Gamma, Mark)\n",
    "    \n",
    "    return -target/n_sample, -grad_Beta/n_sample, -grad_Gamma/n_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c=target_grad(batch_Y.detach().numpy(),batch_X.detach().numpy(),model.beta.detach().numpy().T,model.delta_log.detach().numpy().T,batch_s.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.14104303e+05,  6.75288591e+04,  3.55251356e+04],\n",
       "       [ 3.88072566e+05,  6.67078910e+04,  2.93493857e+04],\n",
       "       [ 1.08179119e+04, -1.10616379e+03,  2.73488929e+03],\n",
       "       [ 8.72241703e+03,  1.59439890e+03,  7.51682679e+02],\n",
       "       [ 6.90908307e+06,  1.22528381e+06,  5.58384881e+05],\n",
       "       [ 4.51099159e+05,  7.77380234e+04,  3.40304923e+04],\n",
       "       [ 1.07763635e+04,  1.92319589e+03,  1.37442987e+03],\n",
       "       [ 3.61723778e+03,  1.25198895e+03,  4.97759535e+02],\n",
       "       [ 1.02119976e+04,  1.65521939e+03,  8.78840103e+02],\n",
       "       [ 3.54517298e+05,  6.06096744e+04,  2.80821673e+04],\n",
       "       [ 2.15021841e+06,  3.69113938e+05,  1.62516468e+05],\n",
       "       [ 7.18739520e+04,  3.01614112e+03,  9.90859377e+03],\n",
       "       [ 2.04462639e+04,  3.49647056e+03,  1.47000604e+03],\n",
       "       [ 7.09083649e+04,  1.17572666e+04,  5.38867286e+03],\n",
       "       [ 8.74868174e+03,  1.23270423e+03,  9.88058031e+02],\n",
       "       [ 1.30758818e+04,  1.78597853e+03,  1.16888069e+03],\n",
       "       [ 4.65282023e+06,  7.97645683e+05,  3.51108586e+05],\n",
       "       [ 3.42743758e+05,  4.91553688e+04,  3.70418492e+04],\n",
       "       [ 2.03085020e+04,  3.43087702e+03,  1.54989434e+03],\n",
       "       [ 1.39725946e+04,  2.33337224e+03,  1.13524329e+03],\n",
       "       [ 1.10867759e+04,  1.31733640e+03,  1.47419827e+03],\n",
       "       [ 4.23283312e+05,  7.23265541e+04,  3.23356955e+04],\n",
       "       [ 4.44125807e+05,  7.50749815e+04,  3.40560567e+04],\n",
       "       [ 8.11812611e+04,  1.40714846e+04,  6.22557499e+03],\n",
       "       [ 1.93428304e+04,  3.41022618e+03,  1.60109146e+03],\n",
       "       [ 1.26653084e+04,  2.17909942e+03,  1.19181857e+03],\n",
       "       [ 1.39880853e+04,  1.28466319e+03,  2.30781947e+03],\n",
       "       [ 5.33074348e+04,  2.99302781e+03,  5.70327115e+03],\n",
       "       [ 6.13500593e+04,  1.04440959e+04,  4.67635297e+03],\n",
       "       [ 2.82015358e+04,  4.64655768e+03,  2.33244324e+03],\n",
       "       [ 1.07723195e+04,  1.29874918e+03,  9.07815594e+02],\n",
       "       [ 1.88774219e+04,  3.17560263e+03,  1.48842376e+03],\n",
       "       [-9.58457214e+02,  4.49753132e+02,  6.32495499e+02],\n",
       "       [ 8.07208081e+03,  1.21766028e+03,  8.24286942e+02],\n",
       "       [-1.49377244e+04,  6.85045590e+02,  3.33832397e+01],\n",
       "       [-1.65285200e+05,  1.41777450e+04,  3.63997627e+03],\n",
       "       [ 8.00295844e+03,  1.25227616e+03,  8.15478478e+02],\n",
       "       [-6.29158935e+04,  2.93585113e+03, -1.35530746e+04],\n",
       "       [ 9.46804194e+03,  1.59959399e+03,  7.40743779e+02],\n",
       "       [-5.83231719e+04, -1.40115821e+04, -6.65088202e+03],\n",
       "       [-2.49794999e+04,  3.00450578e+03, -2.67089579e+04],\n",
       "       [-2.06759834e+04,  2.67170984e+04, -4.78542777e+04],\n",
       "       [ 1.07910302e+06,  1.76460512e+05,  9.17880224e+04],\n",
       "       [ 7.40710931e+03,  7.98495680e+02,  1.16032416e+03],\n",
       "       [ 2.18669305e+04,  2.11753833e+03,  3.55201433e+03],\n",
       "       [ 2.34245851e+04,  3.29114490e+03,  1.80569288e+03],\n",
       "       [-4.80316080e+02, -1.66585305e+03,  4.32016422e+02],\n",
       "       [ 1.35011689e+04,  1.00336104e+03,  1.28393503e+03],\n",
       "       [ 1.43755989e+04,  2.19958153e+03,  1.38546468e+03],\n",
       "       [ 2.88853334e+04,  3.38651556e+03,  2.56298629e+03],\n",
       "       [ 1.72524957e+04,  2.90887313e+03,  1.37259587e+03],\n",
       "       [ 1.29807355e+05, -1.12571921e+03,  3.23676961e+04],\n",
       "       [ 2.25307854e+04,  4.89246345e+03,  2.16552040e+03],\n",
       "       [ 1.01430851e+04,  1.68959606e+03,  8.18130561e+02],\n",
       "       [ 3.08485602e+05,  5.24703882e+04,  2.34193882e+04],\n",
       "       [ 2.11528310e+04,  1.21285363e+03,  2.40382656e+03],\n",
       "       [ 3.72130272e+04,  5.64809909e+03,  3.95457620e+03],\n",
       "       [ 1.26076919e+04,  1.57399268e+03,  1.66359253e+03],\n",
       "       [ 1.20419423e+04,  1.07310823e+03,  1.37971102e+03],\n",
       "       [ 6.52392800e+03,  1.08850324e+03,  7.95734841e+02],\n",
       "       [ 5.12944427e+04,  9.03525133e+03,  3.94961250e+03],\n",
       "       [ 3.73664060e+04,  5.16347168e+03,  3.66017611e+03],\n",
       "       [ 6.79260639e+03,  1.05577669e+03,  6.63053850e+02]])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(target_grad_(batch_Y.detach().numpy(),batch_X.detach().numpy(),model.beta.detach().numpy().T,model.delta_log.detach().numpy().T,batch_s.detach().numpy(),mu.transpose(1,2).detach().numpy(),Y_logprob_reduce.detach().numpy()))[1]*8444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=(target_grad_(batch_Y.detach().numpy(),batch_X.detach().numpy(),model.beta.detach().numpy().T,model.delta_log.detach().numpy().T,batch_s.detach().numpy(),mu.transpose(1,2).detach().numpy(),Y_logprob_reduce.detach().numpy()))[2]*8444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00000000e+000, -0.00000000e+000,  2.10641089e+004,\n",
       "         3.92736973e+005, -0.00000000e+000,  2.59971157e+002,\n",
       "         5.84550568e-104,  8.44321928e-001, -0.00000000e+000,\n",
       "         4.24061048e+001, -0.00000000e+000, -0.00000000e+000],\n",
       "       [-0.00000000e+000, -0.00000000e+000,  9.91228535e+001,\n",
       "         3.87776214e+005, -0.00000000e+000,  6.30731732e+001,\n",
       "         1.38449225e-103,  1.89512022e+001, -0.00000000e+000,\n",
       "         1.15204584e+002, -0.00000000e+000, -0.00000000e+000],\n",
       "       [-0.00000000e+000, -0.00000000e+000,  8.55693684e+003,\n",
       "         1.73198177e+003, -0.00000000e+000,  4.81804593e+001,\n",
       "         2.40295314e-103,  3.07571148e-001, -0.00000000e+000,\n",
       "         4.80505239e+002, -0.00000000e+000, -0.00000000e+000],\n",
       "       [-0.00000000e+000, -0.00000000e+000,  2.16373928e+002,\n",
       "         9.17676584e+003, -0.00000000e+000,  3.35144339e+001,\n",
       "         1.67737594e-103, -6.77699246e+001, -0.00000000e+000,\n",
       "        -6.36467241e+002, -0.00000000e+000, -0.00000000e+000],\n",
       "       [-0.00000000e+000, -0.00000000e+000,  6.22786630e+004,\n",
       "         6.57822342e+006, -0.00000000e+000,  1.65966904e+005,\n",
       "         1.82704862e-100,  5.82686705e+004, -0.00000000e+000,\n",
       "         4.43454154e+004, -0.00000000e+000, -0.00000000e+000]])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  2.1064e+04,  3.9274e+05,  0.0000e+00,\n",
       "          2.5997e+02, 5.8455e-104,  8.4432e-01,  0.0000e+00,  4.2406e+01,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  9.9123e+01,  3.8778e+05,  0.0000e+00,\n",
       "          6.3073e+01, 1.3845e-103,  1.8951e+01,  0.0000e+00,  1.1520e+02,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  8.5569e+03,  1.7320e+03,  0.0000e+00,\n",
       "          4.8180e+01, 2.4030e-103,  3.0757e-01,  0.0000e+00,  4.8051e+02,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  2.1637e+02,  9.1768e+03,  0.0000e+00,\n",
       "          3.3514e+01, 1.6774e-103, -6.7770e+01,  0.0000e+00, -6.3647e+02,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  6.2279e+04,  6.5782e+06,  0.0000e+00,\n",
       "          1.6597e+05, 1.8270e-100,  5.8269e+04,  0.0000e+00,  4.4345e+04,\n",
       "          0.0000e+00,  0.0000e+00]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_grad.T[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.isclose(c,delta_grad.detach().T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-20683053.9205, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logsumexp(Y_logprob_reduce,dim=1).view(-1,1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_idx,batch in enumerate(cell_dataloader):\n",
    "        batch_Y=batch['Y'].to(device)\n",
    "        batch_X=batch['X'].to(device)\n",
    "        batch_s=batch['s'].to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=device_cpu\n",
    "model=Custom_Model(Y,rho,X_col=X.shape[1],delta_min=delta_min,LOWER_BOUND=LOWER_BOUND,THETA_LOWER_BOUND=THETA_LOWER_BOUND).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    gamma_temp,_,_=model(batch_Y,batch_X,batch_s,gamma_fixed=None,mode='E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,Q,_=model(batch_Y,batch_X,batch_s,gamma_fixed=gamma_temp,mode='M')#_,_,LL=model(batch_Y,batch_X,batch_s,gamma_fixed=gamma_temp,mode='LL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,LL=model(batch_Y,batch_X,batch_s,gamma_fixed=gamma_temp,mode='LL')#_,_,LL=model(batch_Y,batch_X,batch_s,gamma_fixed=gamma_temp,mode='LL')\n",
    "LL.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c=\n",
    "#np.isclose(c,-model.beta.grad.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4872636.085002236, None, tensor(-20683053.9205, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*8444,Q,LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6.2829e+02,  6.3041e+01,  3.6685e+01,  1.2390e+01,  3.4925e+05,\n",
       "          1.1223e+03,  6.6232e+03,  7.3122e+00,  2.3879e+03,  1.5176e+01,\n",
       "          6.1710e+04,  2.4184e+01,  5.4429e+01, -1.8969e+01,  1.4890e+03,\n",
       "          6.1197e+01,  6.5552e+00, -2.9832e+01,  4.3286e+00,  1.2071e+01,\n",
       "          2.7357e+01,  7.6733e+00,  7.6254e+00,  2.8476e+02,  7.1157e+03,\n",
       "          1.4170e+04,  5.6454e+01,  3.3321e+04,  1.7969e+03,  3.3266e+01,\n",
       "          3.9181e+01,  3.6222e+01,  6.0340e+00,  4.3138e+02,  1.2268e+02,\n",
       "          5.7704e+02,  2.0729e+01, -1.3299e+04,  6.4671e+01, -9.2646e+03,\n",
       "         -1.0268e+00, -3.8964e+05,  5.7448e+01,  8.8921e+01,  1.7279e+01,\n",
       "          7.3627e+00,  4.8185e+02,  8.0261e+03,  1.5212e+02,  3.4433e+02,\n",
       "          7.7162e+00,  1.2745e+01,  4.5042e+00,  1.1877e+03,  1.3211e+03,\n",
       "          5.0680e+00,  3.1544e+02,  4.9985e+03,  1.9573e+04,  2.1948e+01,\n",
       "          3.2754e+03,  5.5023e+04,  5.6786e+02]),\n",
       " tensor([ 2.5997e+02,  6.3073e+01,  4.8180e+01,  3.3514e+01,  1.6597e+05,\n",
       "          4.0390e+02,  1.5812e+03,  2.9555e+01,  7.0082e+02,  3.1464e+01,\n",
       "          1.0134e+04,  4.1892e+01,  5.8191e+01, -5.8998e+01,  4.9008e+02,\n",
       "          6.2472e+01,  2.7081e+01, -4.2765e+01,  2.6965e+01,  3.3124e+01,\n",
       "          4.3280e+01,  2.9326e+01,  2.5799e+01,  1.5166e+02,  1.6638e+03,\n",
       "          2.9269e+03,  5.9501e+01,  5.9942e+03,  5.6250e+02,  4.6229e+01,\n",
       "          5.0082e+01,  4.8778e+01,  2.8073e+01,  2.0073e+02,  1.0050e+02,\n",
       "          6.3534e+02,  3.8779e+01, -1.6974e+04,  6.3568e+01, -2.6509e+04,\n",
       "         -4.6452e+00, -1.0156e+05,  6.1162e+01,  7.5906e+01,  3.6855e+01,\n",
       "          2.9594e+01,  2.2130e+02,  1.8482e+03,  1.0203e+02,  1.7144e+02,\n",
       "          2.7550e+01,  3.5699e+01,  2.8397e+01,  4.0835e+02,  4.4942e+02,\n",
       "          2.6270e+01,  1.6314e+02,  1.2508e+03,  3.8397e+03,  4.0946e+01,\n",
       "          8.9698e+02,  9.2165e+03,  2.4297e+02], grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.delta_log.grad[5],delta_grad[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_col=3\n",
    "\n",
    "delta_log_min=np.log(delta_min) #\n",
    "delta_log=nn.Parameter(torch.Tensor(delta_random),requires_grad=True) # (C,G)\n",
    "\n",
    "#beta\n",
    "Y_colmean=np.mean(Y,axis=0)\n",
    "beta_init=np.hstack([((Y_colmean-Y_colmean.mean())/np.std(Y_colmean)).reshape(-1,1),\\\n",
    "             np.zeros((Y.shape[1],X_col-1))]).T\n",
    "beta=nn.Parameter(torch.Tensor(beta_init),requires_grad=True) # (P,G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu=torch.exp((batch_X.matmul(beta)+torch.log(batch_s.view(-1, 1))).unsqueeze(dim=1).repeat(1,delta_log.shape[0],1)+torch.exp(delta_log))\n",
    "Y_extend=batch_Y.unsqueeze(dim=1).repeat(1,mu.shape[1],1)\n",
    "Y_logprob=poisson_logprob(rate=mu,value=Y_extend) # (N,C,G)\n",
    "Y_logprob_reduce=Y_logprob.sum(axis=2)\n",
    "\n",
    "\n",
    "gamma=torch.exp(Y_logprob_reduce-torch.logsumexp(Y_logprob_reduce,dim=1).view(-1,1))\n",
    "\n",
    "A=mu-batch_Y.unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "    Q=-torch.sum(gamma_fixed*Y_logprob_reduce) # (N,C) (N,C)\n",
    "    return gamma,Q,None\n",
    "elif mode=='LL':      \n",
    "    LL=torch.sum(Y_logprob_reduce_reduce) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from basic_tools import Cell_Dataset,Masked\n",
    "class Poisson_Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Y, X, s, delta_log, beta):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mu=torch.exp((X.matmul(beta)+torch.log(s.view(-1, 1))).unsqueeze(dim=1).repeat(1,delta_log.shape[0],1)+torch.exp(delta_log))\n",
    "            Y_extend=Y.unsqueeze(dim=1).repeat(1,mu.shape[1],1)\n",
    "            Y_logprob=poisson_logprob(rate=mu,value=Y_extend) # (N,C,G)\n",
    "            Y_logprob_reduce=Y_logprob.sum(axis=2)\n",
    "            \n",
    "            Y_logprob_reduce_reduce=torch.logsumexp(Y_logprob_reduce,dim=1).view(-1,1)\n",
    "            \n",
    "            LL=torch.sum(Y_logprob_reduce_reduce)\n",
    "            \n",
    "            gamma=torch.exp(Y_logprob_reduce-Y_logprob_reduce_reduce)\n",
    "            A=mu-Y.unsqueeze(dim=1)        \n",
    "            \n",
    "            #gradient\n",
    "            grad_delta_log=(A*gamma.unsqueeze(dim=2)).sum(axis=0)\n",
    "            grad_beta=(X.unsqueeze(dim=2)@gamma.unsqueeze(dim=1)@A).sum(axis=0)\n",
    "        \n",
    "            ctx.save_for_backward(grad_delta_log,grad_beta)\n",
    "            \n",
    "        return LL\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \n",
    "        grad_Y = grad_X = grad_s = grad_delta_log = grad_beta = None\n",
    "        grad_delta_log,grad_beta = ctx.saved_tensors\n",
    "\n",
    "        return grad_Y, grad_X, grad_s, grad_delta_log, grad_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "LL=Poisson_Function.apply(batch_Y,batch_X,batch_s,model.delta_log,model.beta)\n",
    "LL.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from basic_tools import Cell_Dataset,Masked\n",
    "class Masked_Function(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, mask):\n",
    "        #print('aaaa')\n",
    "        output=input\n",
    "        ctx.save_for_backward(input, mask)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, mask = ctx.saved_tensors\n",
    "        grad_input = grad_mask = None\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mul(mask)\n",
    "\n",
    "        return grad_input, grad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.]), array([0.]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(gamma_temp.detach().numpy()[:,0]),np.unique(gamma.detach().numpy()[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(batch_X.matmul(beta))*batch_s.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_min\n",
    "delta_log_min=np.log(delta_min) #\n",
    "delta_log=nn.Parameter(torch.Tensor(np.random.uniform(-2,2,size=rho.shape)),requires_grad=True) # (C,G)\n",
    "delta_log.data=delta_log.data.clamp(min=delta_log_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat(1,delta_log.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_log.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx,batch in enumerate(cell_dataloader):\n",
    "    # It is usually just one iteration(batch).\n",
    "    # However, developer of cellAssign may have done this for extreme situation of larse sample size\n",
    "    batch_Y=batch['Y']\n",
    "    batch_X=batch['X']\n",
    "    batch_s=batch['s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 4., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_col=X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mu 8444 63 12\n",
    "N 8444 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8444, 12, 63]), torch.Size([8444, 63]), torch.Size([8444, 1, 63]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu.shape,batch_Y.shape,(batch_Y.unsqueeze(dim=1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8444, 12, 63])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_logprob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8444, 12, 63])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12]), torch.Size([8444, 12]))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(B).shape,B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8444, 12, 12])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((B.unsqueeze(dim=2).repeat(1,1,B.shape[1]))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8444, 12]), torch.Size([8444, 1, 12]))"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.shape,B.unsqueeze(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 63])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  8.5264,   5.8808,   6.2914,  ...,   5.8966,   5.9644,  11.9139]],\n",
       "\n",
       "        [[ 19.6877,  13.5789,  14.5270,  ...,  13.6153,  13.7719,  27.5093]],\n",
       "\n",
       "        [[  6.0289,   4.1583,   4.4486,  ...,   4.1694,   4.2173,   8.4241]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 12.0223,   8.2920,   8.8710,  ...,   8.3142,   8.4098,  16.7986]],\n",
       "\n",
       "        [[  2.9404,   2.9305, 462.4824,  ...,   2.9383,   2.9721,   2.9666]],\n",
       "\n",
       "        [[  2.9625,   2.2966, 132.5130,  ...,   2.3028,   2.3293,   3.8190]]],\n",
       "       grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(B.unsqueeze(dim=1),A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8444, 1, 63])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(B.unsqueeze(dim=1),A).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor to have size 63 at dimension 1, but got size 12 for argument #2 'batch2' (while checking arguments for bmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-edfdd8b1db99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor to have size 63 at dimension 1, but got size 12 for argument #2 'batch2' (while checking arguments for bmm)"
     ]
    }
   ],
   "source": [
    "torch.matmul(A,(B.unsqueeze(dim=2).repeat(1,1,B.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-862619b52749>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "B.shape,np.array([[1,1][1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 4],\n",
       "        [3, 8]]), array([[1, 0],\n",
       "        [0, 2]]))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(np.array([[1,2],[3,4]]), np.diag([1,2])),np.diag([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 2, 0],\n",
       "       [0, 0, 3]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3365e-10, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 3.2606e-11, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "       grad_fn=<DiagBackward>)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell_Dataset(Dataset):\n",
    "    def __init__(self,Y,X,s):\n",
    "        self.Y=Y\n",
    "        self.X=X\n",
    "        self.s=s\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.Y.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        item= {\"Y\":self.Y[idx,:],\"X\":self.X[idx,:],\"s\":self.s[idx]}\n",
    "        return item  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_EM():\n",
    "    global gamma_new,Q_new,LL_new\n",
    "    \n",
    "    print('Start time:',datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx,batch in enumerate(cell_dataloader):\n",
    "            batch_Y=batch['Y'].to(device)\n",
    "            batch_X=batch['X'].to(device)\n",
    "            batch_s=batch['s'].to(device)    \n",
    "        gamma_fixed,_,LL_old=model(batch_Y,batch_X,batch_s,gamma_fixed=None,mode='LL')\n",
    "        _,Q_old,_=model(batch_Y,batch_X,batch_s,gamma_fixed=gamma_fixed,mode='M')\n",
    "\n",
    "    print(LL_old)\n",
    "    print(Q_old)\n",
    "\n",
    "    for em_idx in range(EM_ITER_MAX):#\n",
    "        #optimizer = optim.Adam(model.parameters(),lr=0.1,eps=1e-3,betas=(0.9,0.999))\n",
    "        LL_new=torch.zeros_like(LL_old)\n",
    "        #optimizer = optim.Adam(model.parameters(),lr=LR)\n",
    "        for batch_idx,batch in enumerate(cell_dataloader):\n",
    "            # It is usually just one iteration(batch).\n",
    "            # However, developer of cellAssign may have done this for extreme situation of larse sample size\n",
    "            batch_Y=batch['Y'].to(device)\n",
    "            batch_X=batch['X'].to(device)\n",
    "            batch_s=batch['s'].to(device)\n",
    "\n",
    "            #############\n",
    "            #E-step\n",
    "            ######### ####\n",
    "            with torch.no_grad():\n",
    "                gamma_new,_,_=model(batch_Y,batch_X,batch_s,gamma_fixed=None,mode='E')\n",
    "\n",
    "            #############\n",
    "            #M-step\n",
    "            #############\n",
    "            for m_idx in range(M_ITER_MAX):#\n",
    "            #for m_idx in range(20):#    \n",
    "                optimizer.zero_grad()\n",
    "                _,Q_new,_=model(batch_Y,batch_X,batch_s,gamma_fixed=gamma_new,mode='M')\n",
    "                Q_new.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                #Constraint\n",
    "                model.delta_log.data=model.delta_log.data.clamp(min=model.delta_log_min)\n",
    "                model.NB_basis_a.data=model.NB_basis_a.data.clamp(min=0)\n",
    "\n",
    "                if m_idx%20==0:\n",
    "                    #print(sorted(model.delta_log.cpu().detach().numpy().flatten())[-10:])\n",
    "                    Q_diff=(Q_old-Q_new)/torch.abs(Q_old)\n",
    "                    Q_old=Q_new\n",
    "                    print('M: {}, Q: {} Q_diff: {}'.format(m_idx,Q_new,Q_diff))    \n",
    "                    if m_idx>0 and torch.abs(Q_diff)<Q_diff_tolerance:\n",
    "                        print('M break')\n",
    "                        break                \n",
    "            #############\n",
    "            #Look at LL\n",
    "            #############\n",
    "            with torch.no_grad():\n",
    "                _,_,LL_temp=model(batch_Y,batch_X,batch_s,gamma_fixed=None,mode='LL')\n",
    "                LL_new+=LL_temp\n",
    "\n",
    "        LL_diff=(LL_new-LL_old)/torch.abs(LL_old)\n",
    "        LL_old=LL_new\n",
    "        print('EM: {}, LL: {} LL_diff: {}'.format(em_idx,LL_new,LL_diff))\n",
    "        if LL_diff<LL_diff_tolerance:\n",
    "            print('EM break')\n",
    "            break\n",
    "    print('End time:',datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))    \n",
    "    return gamma_new,Q_new,LL_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions.negative_binomial import NegativeBinomial\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.dirichlet import Dirichlet\n",
    "\n",
    "class Custom_Model(nn.Module):\n",
    "    def __init__(self,Y,rho,X_col=5,delta_min=2,LOWER_BOUND=1e-10,THETA_LOWER_BOUND=1e-20):\n",
    "        # Y,rho are needed for model parameter initialization\n",
    "        super(Custom_Model, self).__init__()\n",
    "        \n",
    "        #rho\n",
    "        self.masked=Masked(rho)\n",
    "         \n",
    "        #delta\n",
    "        self.delta_log_min=np.log(delta_min) #\n",
    "        self.delta_log=nn.Parameter(torch.Tensor(np.random.uniform(-2,2,size=rho.shape)),requires_grad=True) # (C,G)\n",
    "        self.delta_log.data=self.delta_log.data.clamp(min=self.delta_log_min)\n",
    "\n",
    "        #delta prior\n",
    "        self.delta_log_mean=nn.Parameter(0*torch.ones(1),requires_grad=True)        \n",
    "        self.delta_log_variance=nn.Parameter(1*torch.ones(1),requires_grad=True)           \n",
    "        \n",
    "        #beta\n",
    "        Y_colmean=np.mean(Y,axis=0)\n",
    "        beta_init=np.hstack([((Y_colmean-Y_colmean.mean())/np.std(Y_colmean)).reshape(-1,1),\\\n",
    "                     np.zeros((Y.shape[1],X_col-1))]).T\n",
    "        self.beta=nn.Parameter(torch.Tensor(beta_init),requires_grad=True) # (P,G)\n",
    "        \n",
    "        #print(((np.mean(Y,axis=0)-np.mean(Y))/np.std(np.mean(Y,axis=0))).reshape(-1,1))\n",
    "        #print(self.beta)\n",
    "        \n",
    "        #NB parameters\n",
    "        self.NB_basis_mean=nn.Parameter(torch.Tensor(np.linspace(np.min(Y),np.max(Y),B)), requires_grad=False)\n",
    "        self.NB_basis_a=nn.Parameter(torch.ones(B),requires_grad=True) # not consistent with paper.. hmm. strange...\n",
    "        self.NB_basis_b=nn.Parameter((1/(2*((np.max(Y)-np.min(Y))/(B-1))**2))*torch.ones(B),requires_grad=False) # hmm... strange\n",
    "        \n",
    "        #theta\n",
    "        self.theta_logit=nn.Parameter(torch.Tensor(np.random.normal(loc=0.,scale=1.,size=rho.shape[0])),requires_grad=True)\n",
    "        \n",
    "        # lower bounds\n",
    "        self.LOWER_BOUND=LOWER_BOUND\n",
    "        self.THETA_LOWER_BOUND=THETA_LOWER_BOUND\n",
    "        \n",
    "    def forward(self,Y,X,s,gamma_fixed=None,mode='E'):\n",
    "        delta_log_masked=self.masked(self.delta_log) #(C,G)\n",
    "        delta=torch.exp(delta_log_masked)*self.masked.mask\n",
    "        \n",
    "        X_beta_s=X.matmul(self.beta)+torch.log(s.view(-1, 1)) #(N,P)*(P,G) + (N,1) = (N,G)\n",
    "        \n",
    "        mu_log=X_beta_s.unsqueeze(dim=1).repeat(1,delta.shape[0],1)+delta #(N,1,G)+(C,G) = (N,C,G)\n",
    "        \n",
    "        mu=torch.exp(mu_log) # (N,C,G)\n",
    "        \n",
    "        #Y_extend=Y.view(Y.shape[0],1,Y.shape[1]).repeat(1,mu_log.shape[1],1) # (N,C,G)\n",
    "        Y_extend=Y.unsqueeze(dim=1).repeat(1,mu_log.shape[1],1)\n",
    "        \n",
    "        # Negative Binomial\n",
    "        phi_B=self.NB_basis_a*torch.exp(-self.NB_basis_b*(mu_log.unsqueeze(dim=3).repeat(1,1,1,B)-self.NB_basis_mean)**2) # (N,C,G,B)\n",
    "        phi=torch.sum(phi_B,axis=3)+self.LOWER_BOUND\n",
    "        Y_logprob=NB_logprob(total_count=phi,probs=(mu/(mu+phi)),value=Y_extend) # (N,C,G)\n",
    "        \n",
    "        # Normal\n",
    "        #Y_logprob+=normal_logprob(loc=mu_log,scale=torch.ones(1).to(device),value=Y_extend) # (N,C,G)\n",
    "        \n",
    "        # Poisson\n",
    "        #Y_logprob=poisson_logprob(rate=mu,value=Y_extend) # (N,C,G)\n",
    "\n",
    "        theta_log=F.log_softmax(self.theta_logit,dim=0) # (C)          \n",
    "        Y_logprob_reduce=torch.sum(Y_logprob,axis=2)+theta_log # (N,C)\n",
    "        \n",
    "        Y_logprob_reduce_reduce=torch.logsumexp(Y_logprob_reduce,dim=1).view(-1,1) # (N,1)\n",
    "        \n",
    "        gamma=torch.exp(Y_logprob_reduce-Y_logprob_reduce_reduce) # (N,C)\n",
    "\n",
    "        if mode=='E':\n",
    "            return gamma,None,None\n",
    "        elif mode=='M' or mode=='LL':\n",
    "            theta_log_prob=dirichlet_logprob(concentration=1e-2*torch.ones_like(theta_log),value=(torch.exp(theta_log)+self.THETA_LOWER_BOUND))\n",
    "            #print(1e-2*torch.ones_like(theta_log))\n",
    "            delta_log_prob=torch.sum(normal_logprob(loc=self.delta_log_mean*self.masked.mask,scale=self.delta_log_variance,value=delta_log_masked))            \n",
    "            if mode=='M':\n",
    "                Q=-torch.sum(gamma_fixed*Y_logprob_reduce) # (N,C) (N,C)\n",
    "                Q=Q-theta_log_prob\n",
    "                Q=Q-delta_log_prob\n",
    "                return gamma,Q,None\n",
    "            elif mode=='LL':      \n",
    "                LL=torch.sum(Y_logprob_reduce_reduce) # product of likelihood(y_i)-> (1) \n",
    "                print(LL,theta_log_prob,delta_log_prob)\n",
    "                LL=LL+theta_log_prob\n",
    "                LL=LL+delta_log_prob\n",
    "                return gamma,None,LL\n",
    "        else:\n",
    "            raise          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1589112618.9493372\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print(time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_dataset=Cell_Dataset(Y,X,s)\n",
    "cell_dataloader=DataLoader(dataset=cell_dataset,shuffle=False,batch_size=BATCH_SIZE,num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=device_cuda_list[1]#\n",
    "#device=device_cpu\n",
    "model=Custom_Model(Y,rho,X_col=X.shape[1],delta_min=delta_min,LOWER_BOUND=LOWER_BOUND,THETA_LOWER_BOUND=THETA_LOWER_BOUND).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=LR)#,betas=(0.92, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2020-05-10 21:10:20\n",
      "tensor(-652498.3615, device='cuda:4') tensor(-13.2101, device='cuda:4') tensor(-1093.5178, device='cuda:4')\n",
      "tensor(-653605.0894, device='cuda:4')\n",
      "tensor(654436.4986, device='cuda:4')\n",
      "M: 0, Q: 654436.498617614 Q_diff: 0.0\n",
      "M: 20, Q: 333389.01977047406 Q_diff: 0.49057086443879316\n",
      "M: 40, Q: 310419.4991277382 Q_diff: 0.06889705203413579\n",
      "M: 60, Q: 304329.90862675745 Q_diff: 0.019617293752783406\n",
      "M: 80, Q: 302917.34072357387 Q_diff: 0.004641567795809402\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-fae0d8d8d4e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgamma_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLL_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_EM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-994695fee8c8>\u001b[0m in \u001b[0;36mrun_EM\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma_fixed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'M'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mtemp_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mQ_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mtemp_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gamma_value,Q_value,LL_value=run_EM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-4a7787e0c50e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "source": [
    "np.median(np.array(time_list).T[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.000138 zero grad \n",
    "# 0.047395 front \n",
    "# 0.057923 backward\n",
    "# 0.001740 step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  2.,  0.,  1.,  1.,  0.,  1.,  0., 61., 27.]),\n",
       " array([0.00071716, 0.00083199, 0.00094681, 0.00106163, 0.00117645,\n",
       "        0.00129128, 0.0014061 , 0.00152092, 0.00163574, 0.00175056,\n",
       "        0.00186539]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.array(time_list).T[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.isnan(M_get),M_get.shape\n",
    "\n",
    "rho\n",
    "#7363 7294 7367"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8719801042160114, 0.9000473709142587)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7363/8444,7600/8444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7630,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.argmax(gamma_new.cpu().numpy(),axis=1)==exp_data_meta_clusterid_clusteridunique.values),\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 55)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ch6845/tools/miniconda3/envs/pytorch/lib/python3.6/site-packages/scipy/stats/stats.py:3399: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7478"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#M_true.shape,\n",
    "from scipy.stats import pearsonr\n",
    "#M_get=np.array([np.mean(exp_data[marker_unique_exp_data_idx,:][:,np.argmax(gamma_new.cpu().numpy(),axis=1)==i],axis=1) for i in sorted(np.unique(np.argmax(gamma_new.cpu().numpy(),axis=1)))])\n",
    "M_get=np.array([np.mean(exp_data[marker_unique_exp_data_idx,:][:,np.argmax(gamma_new.cpu().numpy(),axis=1)==i],axis=1) for i in range(rho.shape[0])])\n",
    "M_get=np.nan_to_num(M_get,1)\n",
    "type_mapping=[np.argmin([pearsonr(M_get[i],M_true[j])[1] for j in range(M_true.shape[0])]) for i in range(M_get.shape[0])]\n",
    "sum(pd.Series(type_mapping)[np.argmax(gamma_value.cpu().numpy(),axis=1)].values==exp_data_meta_clusterid_clusteridunique.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 24)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 63)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8444,), (8444,))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(type_mapping)[np.argmax(gamma_value.cpu().numpy(),axis=1)].values.shape,exp_data_meta_clusterid_clusteridunique.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), (11, 63))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(exp_data_meta_clusterid_clusteridunique.values),M_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.delta_log_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.5438, 6.8677, 6.4499,  ..., 6.0781, 8.0561, 6.3932],\n",
       "        [6.3569, 6.0225, 6.1163,  ..., 6.1911, 5.7865, 6.9426],\n",
       "        [6.3507, 5.1697, 5.0077,  ..., 5.5911, 5.8916, 5.6769],\n",
       "        ...,\n",
       "        [5.9142, 9.2594, 5.8219,  ..., 5.5893, 5.2697, 5.8865],\n",
       "        [5.0248, 9.6543, 3.7499,  ..., 6.7895, 3.3872, 4.8630],\n",
       "        [6.0585, 5.9672, 6.1915,  ..., 6.0775, 6.9236, 5.9648]],\n",
       "       device='cuda:4', grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.delta_log.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5065, device='cuda:4', grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.delta_log.exp()[model.masked.mask]).flatten().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ch6845/tools/miniconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "model=Custom_Model(Y,rho,X_col=X.shape[1],delta_min=delta_min,LOWER_BOUND=LOWER_BOUND,THETA_LOWER_BOUND=THETA_LOWER_BOUND).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8444, 63), 33776, 8444]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cell_dataset[:]['Y'].shape,cell_dataset[:]['X'].size,cell_dataset[:]['s'].size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      ~/tools/miniconda3/envs/pytorch/lib/python3.6/site-packages/torchsummary/torchsummary.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=5)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Float but got scalar type Double for argument #2 'mat2' in call to _th_mm",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-12a2a1fd1daf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcell_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tools/miniconda3/envs/pytorch/lib/python3.6/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-a667d01cdfbf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, Y, X, s, gamma_fixed, mode)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_log_masked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mX_beta_s\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(N,P)*(P,G) + (N,1) = (N,G)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mmu_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_beta_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdelta\u001b[0m \u001b[0;31m#(N,1,G)+(C,G) = (N,C,G)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Float but got scalar type Double for argument #2 'mat2' in call to _th_mm"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model,input_size=[cell_dataset[:]['Y'].shape,cell_dataset[:]['X'].shape,cell_dataset[:]['s'].shape],device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_log torch.Size([12, 63])\n",
      "delta_log_mean torch.Size([1])\n",
      "delta_log_variance torch.Size([1])\n",
      "beta torch.Size([4, 63])\n",
      "NB_basis_mean torch.Size([10])\n",
      "NB_basis_a torch.Size([10])\n",
      "NB_basis_b torch.Size([10])\n",
      "theta_logit torch.Size([12])\n",
      "masked.mask torch.Size([12, 63])\n"
     ]
    }
   ],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    print(name,param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx,batch in enumerate(cell_dataloader):\n",
    "    # It is usually just one iteration(batch).\n",
    "    # However, developer of cellAssign may have done this for extreme situation of larse sample size\n",
    "    batch_Y=batch['Y'].to(device)\n",
    "    batch_X=batch['X'].to(device)\n",
    "    batch_s=batch['s'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2020-05-03 13:35:28\n",
      "tensor(-7669628.7273, device='cuda:5') tensor(-19.1671, device='cuda:5') tensor(-1178.8088, device='cuda:5')\n",
      "tensor(-7670826.7032, device='cuda:5')\n",
      "tensor(7671773.7737, device='cuda:5')\n",
      "M: 0, Q: 7671773.7736541815 Q_diff: 0.0\n",
      "M: 20, Q: 2566880.1658998225 Q_diff: 0.6654124272127515\n",
      "M: 40, Q: 2049999.058519762 Q_diff: 0.20136549974036955\n",
      "M: 60, Q: 1881904.480737453 Q_diff: 0.08199739267377253\n",
      "M: 80, Q: 1805504.3614400132 Q_diff: 0.040597235449219705\n",
      "M: 100, Q: 1761565.7056747677 Q_diff: 0.024335945513973365\n",
      "M: 120, Q: 1733824.8860063814 Q_diff: 0.015747820009790803\n",
      "M: 140, Q: 1715936.6013248402 Q_diff: 0.010317238393517503\n",
      "M: 160, Q: 1704009.9663510057 Q_diff: 0.006950510272131375\n",
      "M: 180, Q: 1695831.3038760908 Q_diff: 0.004799656478787409\n",
      "M: 200, Q: 1690110.1844792143 Q_diff: 0.00337363709692115\n",
      "M: 220, Q: 1686048.270426146 Q_diff: 0.0024033427467451715\n",
      "M: 240, Q: 1683130.947574422 Q_diff: 0.0017302724381589873\n",
      "M: 260, Q: 1681017.1755881426 Q_diff: 0.0012558571211144227\n",
      "M: 280, Q: 1679475.5546980654 Q_diff: 0.0009170762276940043\n",
      "M: 300, Q: 1678345.9091958702 Q_diff: 0.0006726180080652109\n",
      "M: 320, Q: 1677516.7078499997 Q_diff: 0.0004940586689115783\n",
      "M: 340, Q: 1676903.7924206743 Q_diff: 0.0003653706854048898\n",
      "M: 360, Q: 1676452.6932864792 Q_diff: 0.00026900716441457357\n",
      "M: 380, Q: 1676119.988657601 Q_diff: 0.0001984575110354272\n",
      "M: 400, Q: 1675874.744560096 Q_diff: 0.0001463165520157139\n",
      "M: 420, Q: 1675694.1258121023 Q_diff: 0.00010777580399722309\n",
      "M: 440, Q: 1675561.2633887397 Q_diff: 7.928799254952696e-05\n",
      "M break\n",
      "tensor(-1479577.8404, device='cuda:5') tensor(13.7440, device='cuda:5') tensor(-1143.5067, device='cuda:5')\n",
      "EM: 0, LL: -1480707.6030814622 LL_diff: 0.8069689669207604\n",
      "M: 0, Q: 1481355.114257189 Q_diff: 0.11590513183551306\n",
      "M: 20, Q: 1409233.0277174902 Q_diff: 0.048686561274582485\n",
      "M: 40, Q: 1400687.502718328 Q_diff: 0.006063954527806673\n",
      "M: 60, Q: 1398441.1322258287 Q_diff: 0.0016037627865885633\n",
      "M: 80, Q: 1397245.5892429654 Q_diff: 0.0008549111974133487\n",
      "M: 100, Q: 1396490.8674128999 Q_diff: 0.0005401497316405966\n",
      "M: 120, Q: 1395965.1931121377 Q_diff: 0.00037642516183150973\n",
      "M: 140, Q: 1395592.407114031 Q_diff: 0.00026704533891394486\n",
      "M: 160, Q: 1395325.4684199705 Q_diff: 0.00019127267581840336\n",
      "M: 180, Q: 1395133.2536346863 Q_diff: 0.0001377562365444306\n",
      "M: 200, Q: 1394994.3305323774 Q_diff: 9.957694144775424e-05\n",
      "M break\n",
      "tensor(-1352949.1703, device='cuda:5') tensor(-11.7958, device='cuda:5') tensor(-1147.3137, device='cuda:5')\n",
      "EM: 1, LL: -1354108.2797781967 LL_diff: 0.08549920527172476\n",
      "M: 0, Q: 1355290.7746256152 Q_diff: 0.028461446070257437\n",
      "M: 20, Q: 1327510.6456089588 Q_diff: 0.020497541587952146\n",
      "M: 40, Q: 1323815.9039985354 Q_diff: 0.0027832105321675734\n",
      "M: 60, Q: 1323176.7222377334 Q_diff: 0.0004828328160065985\n",
      "M: 80, Q: 1322778.6122336686 Q_diff: 0.00030087440126031173\n",
      "M: 100, Q: 1322511.670513665 Q_diff: 0.0002018037769394625\n",
      "M: 120, Q: 1322312.7402803483 Q_diff: 0.00015041850877540016\n",
      "M: 140, Q: 1322161.2962158835 Q_diff: 0.00011452968715462693\n",
      "M: 160, Q: 1322045.7469740796 Q_diff: 8.7394209870255e-05\n",
      "M break\n",
      "tensor(-1291021.0486, device='cuda:5') tensor(-14.0217, device='cuda:5') tensor(-1143.0303, device='cuda:5')\n",
      "EM: 2, LL: -1292178.100650277 LL_diff: 0.04573502728900225\n",
      "M: 0, Q: 1293794.5589714448 Q_diff: 0.021369296839611324\n",
      "M: 20, Q: 1271878.587787463 Q_diff: 0.016939297689893478\n",
      "M: 40, Q: 1270465.6832721035 Q_diff: 0.0011108800233970554\n",
      "M: 60, Q: 1270095.3461998054 Q_diff: 0.00029149710785131543\n",
      "M: 80, Q: 1269914.6547894608 Q_diff: 0.00014226602033083442\n",
      "M: 100, Q: 1269778.9928993748 Q_diff: 0.00010682756480868456\n",
      "M: 120, Q: 1269671.7928309075 Q_diff: 8.442419434150448e-05\n",
      "M break\n",
      "tensor(-1251524.4391, device='cuda:5') tensor(-14.6062, device='cuda:5') tensor(-1140.8881, device='cuda:5')\n",
      "EM: 3, LL: -1252679.9334274777 LL_diff: 0.030567123218480617\n",
      "M: 0, Q: 1254555.799190383 Q_diff: 0.011905433928575656\n",
      "M: 20, Q: 1243737.9105696934 Q_diff: 0.00862288359566848\n",
      "M: 40, Q: 1242120.643846971 Q_diff: 0.0013003275923153751\n",
      "M: 60, Q: 1241976.3253318663 Q_diff: 0.00011618719632397265\n",
      "M: 80, Q: 1241959.5872487773 Q_diff: 1.3476974357406359e-05\n",
      "M break\n",
      "tensor(-1235530.0046, device='cuda:5') tensor(-14.4422, device='cuda:5') tensor(-1139.6051, device='cuda:5')\n",
      "EM: 4, LL: -1236684.0518790048 LL_diff: 0.012769328478589359\n",
      "M: 0, Q: 1238837.1639326278 Q_diff: 0.0025141102401458845\n",
      "M: 20, Q: 1237829.150418764 Q_diff: 0.0008136771669521891\n",
      "M: 40, Q: 1237666.3428674145 Q_diff: 0.00013152667417343873\n",
      "M: 60, Q: 1237619.3110604656 Q_diff: 3.800039260974384e-05\n",
      "M break\n",
      "tensor(-1233576.1236, device='cuda:5') tensor(-14.2968, device='cuda:5') tensor(-1138.8679, device='cuda:5')\n",
      "EM: 5, LL: -1234729.288386043 LL_diff: 0.0015806490671500394\n",
      "M: 0, Q: 1237108.1479174537 Q_diff: 0.0004130213050521806\n",
      "M: 20, Q: 1236638.168756981 Q_diff: 0.0003799014348615463\n",
      "M: 40, Q: 1236565.6777577214 Q_diff: 5.8619409533821334e-05\n",
      "M break\n",
      "tensor(-1232827.8701, device='cuda:5') tensor(-14.2797, device='cuda:5') tensor(-1138.9435, device='cuda:5')\n",
      "EM: 6, LL: -1233981.0933489853 LL_diff: 0.0006059587669096395\n",
      "M: 0, Q: 1236542.9537592542 Q_diff: 1.837670159858454e-05\n",
      "M: 20, Q: 1236287.3152920706 Q_diff: 0.00020673642303034817\n",
      "M: 40, Q: 1236145.4747273724 Q_diff: 0.00011473106853377698\n",
      "M: 60, Q: 1236115.3210351919 Q_diff: 2.4393320039596426e-05\n",
      "M break\n",
      "tensor(-1232157.6339, device='cuda:5') tensor(-14.3456, device='cuda:5') tensor(-1138.5113, device='cuda:5')\n",
      "EM: 7, LL: -1233310.4907143223 LL_diff: 0.0005434464419896426\n",
      "M: 0, Q: 1236022.8312444189 Q_diff: 7.482294669364917e-05\n",
      "M: 20, Q: 1235914.6019506562 Q_diff: 8.756253608498619e-05\n",
      "M break\n",
      "tensor(-1231975.6418, device='cuda:5') tensor(-14.3908, device='cuda:5') tensor(-1138.3839, device='cuda:5')\n",
      "EM: 8, LL: -1233128.416607766 LL_diff: 0.0001476303882333051\n",
      "M: 0, Q: 1235959.473638966 Q_diff: -3.630646343930452e-05\n",
      "M: 20, Q: 1235268.7943458532 Q_diff: 0.000558820339860579\n",
      "M: 40, Q: 1235225.5609539684 Q_diff: 3.49991775738243e-05\n",
      "M break\n",
      "tensor(-1231238.8653, device='cuda:5') tensor(-14.3455, device='cuda:5') tensor(-1137.8207, device='cuda:5')\n",
      "EM: 9, LL: -1232391.0314993681 LL_diff: 0.0005979791710795971\n",
      "M: 0, Q: 1235341.2656430758 Q_diff: -9.367089927932721e-05\n",
      "M: 20, Q: 1235317.158143034 Q_diff: 1.9514850440364333e-05\n",
      "M break\n",
      "tensor(-1231205.9805, device='cuda:5') tensor(-14.4061, device='cuda:5') tensor(-1137.7352, device='cuda:5')\n",
      "EM: 10, LL: -1232358.1218264336 LL_diff: 2.670392115278781e-05\n",
      "EM break\n",
      "End time: 2020-05-03 13:36:12\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total 8444 cells\n",
    "#NB              7640 LL: -271628 90sec 9iters\n",
    "#NB(old markers) 7362 LL: -241538 90sec 13iters\n",
    "#Poisson         7300 LL: -1116555 40sec 9iters \n",
    "#Normal          3369  LL: -783656280 doesn't_converge\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 5, 5, 5])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.delta_log_min\n",
    "exp_data_col_patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>2621</td>\n",
       "      <td>156</td>\n",
       "      <td>24</td>\n",
       "      <td>244</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1    2   3    4  5  6   7   8\n",
       "0   0     2    3   4    5  6  7   9  10\n",
       "1  41  2621  156  24  244  2  1  29   9"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.array(np.unique(exp_data_meta_clusterid_clusteridunique.values[exp_data_col_patient==3],return_counts=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>41</td>\n",
       "      <td>13</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96</td>\n",
       "      <td>629</td>\n",
       "      <td>2621</td>\n",
       "      <td>43</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>212</td>\n",
       "      <td>85</td>\n",
       "      <td>156</td>\n",
       "      <td>151</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>283</td>\n",
       "      <td>123</td>\n",
       "      <td>244</td>\n",
       "      <td>294</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>275</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>305</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>82</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>344</td>\n",
       "      <td>29</td>\n",
       "      <td>77</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1    1     1    1    1\n",
       "0                           \n",
       "0     8   23    41   13   34\n",
       "1     7    7     0   34   81\n",
       "2    96  629  2621   43  112\n",
       "3   212   85   156  151  240\n",
       "4     8    0    24    2    3\n",
       "5   283  123   244  294  248\n",
       "6   275    1     2  305  378\n",
       "7    82    5     1  192  289\n",
       "8    92    0     0  199  197\n",
       "9     4  344    29   77   57\n",
       "10    1    2     9   15   66"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.DataFrame([np.unique(exp_data_meta_clusterid_clusteridunique.values[exp_data_col_patient==i],return_counts=True) for i in range(1,6)]).T\n",
    "pd.concat([pd.DataFrame(np.array(np.unique(exp_data_meta_clusterid_clusteridunique.values[exp_data_col_patient==i],return_counts=True))).T.set_index(0) for i in range(1,6)],axis=1).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1,2 7607\n",
    "# 7639\n",
    "#1,2,4 7663\n",
    "# 4485\n",
    "\n",
    "#3 7600ê°œ\n",
    "#2 7600ê°œ\n",
    "#0  (1ì´ìƒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "full0=pd.read_csv('data/analysis/cellassign_fit.prob.tsv',sep='\\t').values#.idxmax(axis=1)#.value_counts()\n",
    "full1=pd.read_csv('data/analysis/cellassign_fit_full.prob.tsv',sep='\\t').values#.idxmax(axis=1)#.value_counts()\n",
    "full2=pd.read_csv('data/analysis/cellassign_fit_full2.prob.tsv',sep='\\t').values#.idxmax(axis=1)#.value_counts()\n",
    "full3=pd.read_csv('data/analysis/cellassign_fit_full3.prob.tsv',sep='\\t').values#.idxmax(axis=1)#.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-f921ea0a86ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mexp_data_meta_clusterid_clusteridunique\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mexp_data_meta_clusterid_clusteridunique\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mexp_data_meta_clusterid_clusteridunique\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mexp_data_meta_clusterid_clusteridunique\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test3' is not defined"
     ]
    }
   ],
   "source": [
    "sum(np.argmax(full0,axis=1)==exp_data_meta_clusterid_clusteridunique.values),\\\n",
    "sum(np.argmax(full1,axis=1)==exp_data_meta_clusterid_clusteridunique.values),\\\n",
    "sum(np.argmax(full2,axis=1)==exp_data_meta_clusterid_clusteridunique.values),\\\n",
    "sum(np.argmax(full3,axis=1)==exp_data_meta_clusterid_clusteridunique.values),\\\n",
    "sum(np.argmax(test3,axis=1)==exp_data_meta_clusterid_clusteridunique.values),\\\n",
    "sum(np.argmax(gamma_new.cpu().numpy(),axis=1)==exp_data_meta_clusterid_clusteridunique.values),\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(exp_data_meta_clusterid_clusteridunique.values,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),\n",
       " array([ 125,  115, 3558,  741,   50, 1177,  602,  750,  664,  539,  121,\n",
       "           2]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.argmax(gamma_value.cpu().numpy(),axis=1),return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'delta_log': Parameter containing:\n",
       " tensor([[0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
       "          0.9312, 0.6931, 0.6931, 1.4206, 1.5949, 0.6931, 0.6931, 0.6931, 0.6931,\n",
       "          1.6822, 1.9177, 1.8566, 0.6931, 0.6931, 0.6931, 1.5274, 0.6931, 1.6235,\n",
       "          1.5616, 1.5839, 1.7507, 0.6931, 1.9307, 0.6931, 1.2182, 1.7909, 0.6931,\n",
       "          0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 1.5505, 1.8670, 1.9099,\n",
       "          1.8641, 1.5157, 1.7668, 1.7909, 0.6931, 1.8040, 0.6931, 0.6931, 0.6931,\n",
       "          0.6931, 1.9931, 1.5128, 1.6299, 1.7888, 1.3055, 2.1360, 0.6931, 0.6931],\n",
       "         [0.6931, 0.6931, 0.6931, 1.1388, 1.8762, 1.5920, 0.8309, 1.0070, 0.6931,\n",
       "          1.3236, 1.3432, 0.9883, 0.6931, 0.6931, 0.6931, 0.8321, 0.6931, 1.9308,\n",
       "          0.6931, 1.8264, 0.6931, 1.1021, 1.2913, 0.6931, 0.6931, 0.6931, 0.6931,\n",
       "          0.6931, 1.2296, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
       "          1.7357, 1.8895, 1.8848, 0.6931, 0.6931, 1.1758, 0.6931, 1.6272, 0.6931,\n",
       "          1.5863, 0.6931, 0.6931, 0.6931, 1.8581, 0.6931, 1.0678, 0.6931, 1.2012,\n",
       "          1.2998, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 1.6015],\n",
       "         [0.9204, 1.3557, 1.9014, 0.6931, 1.1535, 1.2134, 1.3272, 0.6931, 0.6931,\n",
       "          0.6931, 0.6931, 0.6931, 1.3512, 0.6931, 0.6931, 0.6931, 1.5998, 0.7324,\n",
       "          0.8960, 0.6931, 0.6931, 1.9192, 0.6931, 1.8955, 0.6931, 1.2691, 0.6931,\n",
       "          0.6931, 1.3509, 1.0100, 0.6931, 1.0546, 0.6931, 0.6931, 1.2925, 0.6931,\n",
       "          0.6931, 0.6931, 0.6931, 1.5435, 1.3311, 0.6931, 0.6931, 0.6931, 0.6931,\n",
       "          0.6931, 0.6931, 1.7315, 0.6931, 1.9607, 0.6931, 0.6931, 1.2611, 0.6931,\n",
       "          0.6931, 1.9912, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931],\n",
       "         [0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.9748, 1.4575,\n",
       "          1.2316, 0.7971, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 1.4680,\n",
       "          1.9487, 0.9037, 0.6931, 1.6308, 1.6924, 1.9933, 1.9109, 0.6931, 0.6931,\n",
       "          1.3360, 1.4751, 0.6931, 0.6931, 1.5754, 0.8963, 0.8531, 0.6931, 0.6931,\n",
       "          0.6931, 0.6931, 0.6931, 1.4382, 0.6931, 1.5573, 0.6931, 0.6931, 0.6931,\n",
       "          0.6931, 0.6931, 1.1934, 0.6931, 0.6931, 0.6931, 0.6931, 0.7698, 0.9362,\n",
       "          1.3447, 0.6931, 1.7895, 0.6931, 1.9973, 0.6931, 0.6931, 0.6931, 1.6052],\n",
       "         [1.9047, 0.6931, 0.6931, 0.6931, 1.2965, 0.6931, 0.9603, 0.6931, 1.0368,\n",
       "          0.8543, 1.9932, 1.6089, 0.6931, 1.4715, 0.6931, 0.9983, 0.6931, 1.3263,\n",
       "          0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 2.0816, 1.9793, 1.8647, 0.6931,\n",
       "          1.9088, 0.6931, 0.8176, 0.6931, 0.6931, 1.5219, 1.7192, 0.6931, 1.4395,\n",
       "          0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 1.6861, 0.6931, 0.6931,\n",
       "          0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 1.7080, 0.6931, 0.6931, 0.6931,\n",
       "          0.6931, 1.1305, 0.6931, 0.6931, 1.4388, 1.4801, 0.6931, 0.6931, 0.6931],\n",
       "         [1.8659, 0.6931, 1.4598, 0.6931, 0.6931, 0.6931, 1.1837, 1.3338, 0.7522,\n",
       "          0.6931, 0.6931, 0.6931, 0.6931, 0.7122, 1.2478, 0.6931, 1.2592, 0.6931,\n",
       "          0.6931, 0.6931, 1.7254, 0.6931, 0.6931, 0.6931, 1.5548, 1.7897, 0.6931,\n",
       "          0.6931, 0.6931, 0.6931, 1.4246, 0.6931, 1.1846, 0.6931, 0.6931, 0.6931,\n",
       "          1.1075, 0.8672, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
       "          0.6931, 1.4710, 1.5315, 1.8574, 0.6931, 1.8838, 0.6931, 1.4627, 1.5112,\n",
       "          1.3509, 0.6931, 0.6931, 1.5075, 0.8503, 0.6931, 1.3900, 0.6931, 0.6931],\n",
       "         [0.6931, 1.0663, 1.2598, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 1.6332,\n",
       "          0.6931, 0.6931, 0.9682, 0.6931, 0.6931, 1.0850, 1.0159, 0.7219, 0.6931,\n",
       "          1.7015, 0.7790, 0.6931, 0.6931, 0.6931, 0.6931, 1.5452, 0.6931, 0.6931,\n",
       "          0.6931, 0.6931, 0.8143, 0.9782, 1.6602, 0.6931, 0.7933, 0.6931, 0.6931,\n",
       "          0.6931, 0.6931, 0.6931, 0.6931, 1.3238, 0.6931, 0.7799, 0.6931, 0.6931,\n",
       "          0.6931, 0.9685, 1.5991, 1.0545, 1.0482, 1.5536, 0.6931, 0.6931, 0.6931,\n",
       "          1.4645, 0.6931, 1.8899, 0.6931, 1.7023, 0.6931, 0.6931, 1.1360, 1.8187],\n",
       "         [0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 1.9897, 0.6931, 0.6931, 0.6931,\n",
       "          0.8962, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 1.4259,\n",
       "          0.6931, 0.6931, 0.6931, 1.8467, 1.0624, 1.2188, 0.6931, 0.6931, 0.7677,\n",
       "          0.6931, 1.4085, 0.6931, 1.3065, 0.6931, 1.7018, 0.6931, 0.6931, 0.6931,\n",
       "          1.1511, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 1.5447, 0.6931,\n",
       "          1.1907, 1.7796, 0.6931, 0.6931, 0.6931, 0.6931, 1.3294, 0.6931, 0.9938,\n",
       "          1.4577, 0.6931, 1.6327, 1.6895, 0.6931, 0.6931, 0.6931, 1.6973, 0.6931],\n",
       "         [1.2472, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 1.6422, 0.6931, 1.6433,\n",
       "          1.0125, 0.8796, 0.6931, 0.6931, 0.7247, 1.6191, 1.8604, 0.6931, 1.4600,\n",
       "          0.6931, 0.6931, 0.6931, 0.6931, 1.2734, 0.6931, 0.6931, 0.6931, 0.8342,\n",
       "          1.8944, 0.9349, 0.6931, 0.6931, 1.6089, 0.6931, 1.0664, 0.6931, 0.6931,\n",
       "          0.9989, 0.6931, 0.9517, 0.6931, 0.6931, 0.6931, 0.6931, 1.5572, 0.6931,\n",
       "          0.6931, 0.8858, 0.6931, 1.7405, 0.7427, 1.5482, 1.1927, 0.6931, 0.6931,\n",
       "          1.4705, 0.6931, 0.9347, 0.6931, 0.6931, 0.6931, 0.6931, 1.4986, 0.6931],\n",
       "         [0.6931, 0.6931, 1.9763, 0.6931, 1.2076, 0.6931, 1.6625, 0.6931, 0.6931,\n",
       "          0.6931, 0.6931, 1.6012, 0.6931, 1.1893, 1.8398, 0.6931, 1.6767, 1.7663,\n",
       "          1.6316, 0.6931, 1.9276, 0.6931, 1.7779, 1.0933, 0.6931, 0.6931, 0.6931,\n",
       "          1.8342, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
       "          0.6931, 1.8857, 0.7371, 1.9114, 1.8849, 1.8767, 0.6931, 0.6931, 0.6931,\n",
       "          0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 1.5450, 0.6931, 0.6931, 0.6931,\n",
       "          0.8608, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 1.7568, 1.4854, 0.6931],\n",
       "         [0.6931, 0.9699, 0.6931, 2.2059, 0.6931, 0.6931, 0.6931, 2.1278, 0.6931,\n",
       "          0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
       "          0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
       "          0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.9862, 1.5512, 0.6931, 2.1978,\n",
       "          0.6931, 1.4493, 0.6931, 0.6931, 0.6931, 0.6931, 1.2991, 1.5834, 1.9732,\n",
       "          0.6931, 0.6931, 0.7402, 0.6931, 0.6931, 1.6483, 0.6931, 1.3965, 0.6931,\n",
       "          0.6931, 0.6931, 1.6901, 1.5056, 0.6931, 0.6931, 1.0145, 0.6931, 0.6931],\n",
       "         [0.6931, 0.8170, 0.9811, 0.6931, 0.6931, 0.6931, 1.1958, 0.6931, 0.9132,\n",
       "          0.6931, 0.6931, 0.7175, 0.6931, 0.6931, 0.6931, 1.9460, 1.0314, 0.6931,\n",
       "          0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 1.3305, 0.6931, 0.6931, 0.6931,\n",
       "          1.9175, 1.0036, 0.6931, 0.6931, 1.5938, 0.6931, 0.6931, 1.2734, 1.0164,\n",
       "          0.6931, 0.6931, 1.5127, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
       "          0.6931, 1.2038, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.7581,\n",
       "          0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 1.9151, 0.6931, 0.6931, 1.7684]],\n",
       "        device='cuda:5', requires_grad=True),\n",
       " 'delta_log_mean': Parameter containing:\n",
       " tensor([1.5176], device='cuda:5', requires_grad=True),\n",
       " 'delta_log_variance': Parameter containing:\n",
       " tensor([0.9314], device='cuda:5', requires_grad=True),\n",
       " 'beta': Parameter containing:\n",
       " tensor([[-4.0408e+00, -4.8387e+00, -2.5109e+00, -5.8827e+00,  2.1676e+00,\n",
       "          -2.3355e+00, -3.4034e+00, -4.2724e+00, -4.1704e+00, -7.6474e-01,\n",
       "          -4.8199e+00, -1.7592e+00, -3.0138e+00, -2.4212e+00, -1.8417e+00,\n",
       "          -2.2117e+00, -2.8006e+00, -5.3511e+00, -7.0438e+00, -6.6713e+00,\n",
       "          -6.4981e+00, -4.4835e+00, -1.3036e+00, -5.3039e+00, -4.9446e+00,\n",
       "          -4.3446e+00, -4.6708e+00, -4.9336e+00, -5.0765e+00, -5.9711e+00,\n",
       "          -3.4755e+00, -5.2458e+00, -2.6551e+00, -1.6778e+00, -1.3489e+00,\n",
       "           1.1163e-01, -2.5610e+00, -1.6952e+00, -6.0323e+00, -2.1878e+00,\n",
       "          -2.1584e+00, -3.5023e-01, -3.6256e-01, -3.8326e+00, -6.1462e+00,\n",
       "          -5.6770e+00, -2.1519e+00, -3.5968e+00, -7.1162e+00, -5.2851e+00,\n",
       "          -3.7675e+00, -9.1203e-01, -2.1165e+00, -2.2844e+00, -3.9639e+00,\n",
       "          -1.8819e+00, -1.7505e+00, -5.6820e+00, -2.3910e+00, -1.2074e+00,\n",
       "          -6.4834e+00, -2.1486e+00, -4.0507e+00],\n",
       "         [ 4.7411e-01, -1.5755e+00,  1.3618e-01, -8.8593e-01, -1.2347e+00,\n",
       "          -1.4594e-01, -1.6769e+00, -1.3401e+00,  1.6117e-01, -7.4671e-01,\n",
       "          -1.0250e+00,  8.7855e-01,  1.3040e-01, -1.8184e-01,  1.2146e+00,\n",
       "           1.8282e+00, -2.7035e-01, -2.7171e-01, -1.3485e+00,  5.7748e-02,\n",
       "          -1.3707e-01, -6.8414e-01, -7.1086e-02, -2.5188e+00, -1.0510e+00,\n",
       "          -7.8239e-01,  4.7190e-01,  8.0947e-01,  5.5980e-01,  2.2379e-01,\n",
       "           2.7430e-01,  8.2907e-01, -6.5963e-01, -3.2666e-01, -1.2541e+00,\n",
       "          -2.8849e+00,  4.1456e-01, -1.5593e+00,  2.0537e-01,  4.3206e-01,\n",
       "          -1.5965e+00, -6.0245e-01,  3.5886e-02, -2.4106e-01,  1.6794e-01,\n",
       "           4.7016e-01, -6.4326e-03,  2.5737e-01,  4.9646e-01,  3.8953e-01,\n",
       "          -3.6755e-02, -1.8307e-01, -9.1645e-01,  1.0868e-01,  2.9129e-01,\n",
       "           2.2732e-01, -1.0048e-01,  2.0174e-01,  2.0714e+00, -4.3886e-01,\n",
       "          -2.7770e+00,  1.5525e+00, -1.2035e+00],\n",
       "         [ 4.5711e-01, -1.8205e+00,  6.2178e-01, -1.2056e+00,  1.7280e-01,\n",
       "           1.0581e-01, -3.1893e+00,  8.5640e-02, -5.6543e-02, -3.5311e-01,\n",
       "           1.7505e+00, -1.9579e+00,  9.4450e-01, -9.7197e-01, -1.3551e+00,\n",
       "          -1.4482e+00,  2.4069e-01, -7.8292e-01,  4.7895e-01, -7.1734e-01,\n",
       "          -6.9649e-02,  7.2005e-01, -2.6953e-01,  2.0871e-01, -1.8991e+00,\n",
       "          -5.1406e-01,  1.8007e-01,  9.3477e-02,  6.3340e-01, -3.4544e-01,\n",
       "           8.3633e-01, -5.3574e-01, -3.7511e-01, -2.1273e+00, -6.9647e-01,\n",
       "          -5.0134e-01, -2.3428e+00,  6.5867e-01, -3.3772e+00,  2.3567e-01,\n",
       "           1.8550e+00,  1.0228e+00, -6.6194e-02, -1.5222e+00,  5.4673e-01,\n",
       "           8.4695e-01,  6.1681e-01,  7.9817e-01, -5.7445e-01, -7.2413e-02,\n",
       "           5.1631e-01, -1.5610e+00, -9.2892e-01,  3.7386e-01,  4.7857e-01,\n",
       "          -5.0472e-01, -5.7391e-01,  6.5779e-01,  1.2575e+00, -1.2525e+00,\n",
       "          -7.3921e-01, -1.4107e+00,  2.8981e-01]], device='cuda:5',\n",
       "        requires_grad=True),\n",
       " 'NB_basis_mean': Parameter containing:\n",
       " tensor([    0.0000,  2415.5556,  4831.1111,  7246.6667,  9662.2222, 12077.7778,\n",
       "         14493.3333, 16908.8889, 19324.4444, 21740.0000], device='cuda:5'),\n",
       " 'NB_basis_a': Parameter containing:\n",
       " tensor([0.2762, 0.2866, 0.2970, 0.3073, 0.3177, 0.3280, 0.3383, 0.3522, 1.0030,\n",
       "         1.0000], device='cuda:5', requires_grad=True),\n",
       " 'NB_basis_b': Parameter containing:\n",
       " tensor([8.5691e-08, 8.5691e-08, 8.5691e-08, 8.5691e-08, 8.5691e-08, 8.5691e-08,\n",
       "         8.5691e-08, 8.5691e-08, 8.5691e-08, 8.5691e-08], device='cuda:5'),\n",
       " 'theta_logit': Parameter containing:\n",
       " tensor([-0.8651, -0.9597,  2.4936,  0.9123, -1.7971,  1.3895,  0.7054,  0.9687,\n",
       "          0.7995,  0.6088, -0.9202, -2.0556], device='cuda:5',\n",
       "        requires_grad=True),\n",
       " 'masked.mask': Parameter containing:\n",
       " tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False,  True, False, False, False, False, False,  True,  True,\n",
       "           True, False, False, False, False, False,  True, False, False,  True,\n",
       "          False,  True, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False,  True,  True, False, False,  True, False,\n",
       "          False, False, False, False, False, False, False,  True, False, False,\n",
       "           True, False, False],\n",
       "         [False, False, False, False, False, False, False, False, False, False,\n",
       "           True, False, False, False, False, False, False,  True, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False,  True, False,\n",
       "          False,  True, False, False, False, False, False, False, False,  True,\n",
       "          False, False, False, False,  True, False, False, False, False, False,\n",
       "          False, False, False],\n",
       "         [False,  True, False, False,  True,  True,  True, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False,  True, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False,  True, False, False, False, False, False, False, False,\n",
       "          False, False, False],\n",
       "         [False, False, False, False, False, False, False, False,  True, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False,  True, False, False, False, False, False, False,  True, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False,  True, False, False, False, False, False, False,\n",
       "          False, False,  True],\n",
       "         [ True, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False,  True,  True,  True, False,  True, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "           True, False, False, False, False,  True, False, False,  True,  True,\n",
       "          False, False, False],\n",
       "         [False, False,  True, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False,  True, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "           True, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False,  True,  True, False, False,\n",
       "          False, False, False, False,  True, False, False, False, False, False,\n",
       "          False, False, False],\n",
       "         [False, False, False, False, False, False, False, False, False, False,\n",
       "          False,  True, False, False,  True,  True, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False,  True, False, False, False, False, False, False, False,\n",
       "          False, False, False, False,  True, False, False, False, False, False,\n",
       "          False,  True, False],\n",
       "         [False, False, False, False, False, False, False, False, False,  True,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False,  True, False, False, False, False, False, False, False,\n",
       "           True, False,  True, False, False, False,  True, False, False, False,\n",
       "          False, False, False,  True, False, False, False, False, False, False,\n",
       "          False,  True, False, False,  True, False, False, False, False, False,\n",
       "          False, False, False],\n",
       "         [False, False, False, False, False, False, False, False, False,  True,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False,  True, False, False, False, False, False, False, False,\n",
       "          False, False, False,  True, False, False, False, False, False, False,\n",
       "          False, False, False,  True, False, False, False, False, False, False,\n",
       "          False,  True, False, False,  True, False, False, False, False, False,\n",
       "          False, False, False],\n",
       "         [False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False,  True, False, False, False,  True, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False,  True, False,  True,\n",
       "           True,  True, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False,  True, False, False, False, False, False,\n",
       "          False, False, False],\n",
       "         [False, False, False,  True, False, False, False,  True, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False,  True, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False,  True, False, False, False,\n",
       "          False, False, False],\n",
       "         [False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False, False, False, False, False, False, False, False,\n",
       "          False, False, False]], device='cuda:5')}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "with torch.no_grad():\n",
    "    for batch_idx,batch in enumerate(cell_dataloader):\n",
    "        batch_Y=batch['Y'].to(device)\n",
    "        batch_X=batch['X'].to(device)\n",
    "        batch_s=batch['s'].to(device)    \n",
    "    gamma_fixed,_,LL_old=model(batch_Y,batch_X,batch_s,gamma_fixed=None,mode='LL')\n",
    "    _,Q_old,_=model(batch_Y,batch_X,batch_s,gamma_fixed=gamma_fixed,mode='M')\n",
    "    \n",
    "print(LL_old)\n",
    "print(Q_old)\n",
    "\n",
    "\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\n",
    "make_dot(model(batch_Y,batch_X,batch_s,gamma_fixed=gamma_fixed,mode='M')[1],params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
